{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afda71d-6df5-46d7-a249-9a95f28e1eba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ProteinMPNN single conformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b383c6b5-eb26-43d6-b184-e9d0c3ed7bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615bded-d038-40e7-90d6-435a9f5aad30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6086342c-da46-436e-9ddf-f9c2964a1fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def randomize_list(lst, ratio):\n",
    "\n",
    "    new_lst = lst.copy()\n",
    "    for i in range(len(new_lst)):\n",
    "        if random.random() < ratio:\n",
    "            new_lst[i] = random.randint(0, 20)\n",
    "    return new_lst\n",
    "\n",
    "def featurize(batch,lst_chain ,device = \"cpu\",is_train = True):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    Clabel = [0,1,3,3,0,1,2,0,2,0,0,1,0,1,2,1,1,0,0,1,0]\n",
    "    B = len(batch)\n",
    "     \n",
    "    lengths = np.array([len(b['seq']) for b in batch], dtype=np.int32) #sum of chain seq lengths\n",
    "    L_max = max([len(b['seq']) for b in batch])\n",
    "    X = np.zeros([B, L_max, 4, 3])\n",
    "    residue_idx = -100*np.ones([B, L_max], dtype=np.int32) #residue idx with jumps across chains\n",
    "    chain_M = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted, 0.0 for the bits that are given\n",
    "    mask_self = np.ones([B, L_max, L_max], dtype=np.int32) #for interface loss calculation - 0.0 for self interaction, 1.0 for other\n",
    "    chain_encoding_all = np.zeros([B, L_max], dtype=np.int32) #integer encoding for chains 0, 0, 0,...0, 1, 1,..., 1, 2, 2, 2...\n",
    "    S = np.zeros([B, L_max], dtype=np.int32) #sequence AAs integers\n",
    "    S_noise = np.zeros([B, L_max], dtype=np.int32) #sequence AAs integers\n",
    "    S_s = np.zeros([B, L_max], dtype=np.int32)\n",
    "    mask_rp = np.zeros([B, L_max], dtype=np.int32)\n",
    "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
    "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
    "    chain_letters = init_alphabet + extra_alphabet\n",
    "    ids = 0\n",
    "    for i, b in enumerate(batch):\n",
    "        masked_chains = b['masked_list']\n",
    "        visible_chains = b['visible_list']\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        visible_temp_dict = {}\n",
    "        masked_temp_dict = {}\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            chain_seq = b[f'seq_chain_{letter}']\n",
    "            if letter in visible_chains:\n",
    "                visible_temp_dict[letter] = chain_seq\n",
    "            elif letter in masked_chains:\n",
    "                masked_temp_dict[letter] = chain_seq\n",
    "        for km, vm in masked_temp_dict.items():\n",
    "            for kv, vv in visible_temp_dict.items():\n",
    "                if vm == vv:\n",
    "                    if kv not in masked_chains:\n",
    "                        masked_chains.append(kv)\n",
    "                    if kv in visible_chains:\n",
    "                        visible_chains.remove(kv)\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        index_of_a = all_chains.index(lst_chain[ids])\n",
    "        all_chains.insert(0, all_chains.pop(index_of_a))\n",
    "        \n",
    "        \n",
    "        #random.shuffle(all_chains) #randomly shuffle chain order\n",
    "        num_chains = b['num_of_chains']\n",
    "        mask_dict = {}\n",
    "        x_chain_list = []\n",
    "        chain_mask_list = []\n",
    "        chain_seq_list = []\n",
    "        chain_encoding_list = []\n",
    "        c = 1\n",
    "        l0 = 0\n",
    "        l1 = 0\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            if letter != lst_chain[ids]:\n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.zeros(chain_length) #0.0 for visible chains\n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_length,4,3]\n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "            else: \n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.ones(chain_length) #0.0 for visible chains\n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]\n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "        x = np.concatenate(x_chain_list,0) #[L, 4, 3]\n",
    "        all_sequence = \"\".join(chain_seq_list)\n",
    "        m = np.concatenate(chain_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "        chain_encoding = np.concatenate(chain_encoding_list,0)\n",
    "\n",
    "        l = len(all_sequence)\n",
    "        \n",
    "        \n",
    "        all_sequence = list(all_sequence)\n",
    "        for aas in range(len(all_sequence)):\n",
    "            if all_sequence[aas] not in alphabet:\n",
    "                all_sequence[aas] = \"X\"\n",
    "        all_sequence = \"\".join(all_sequence)\n",
    "        \n",
    "        x_pad = np.pad(x, [[0,L_max-l], [0,0], [0,0]], 'constant', constant_values=(np.nan, ))\n",
    "        X[i,:,:,:] = x_pad\n",
    "\n",
    "        m_pad = np.pad(m, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_M[i,:] = m_pad\n",
    "\n",
    "        chain_encoding_pad = np.pad(chain_encoding, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_encoding_all[i,:] = chain_encoding_pad\n",
    "\n",
    "        # Convert to labels\n",
    "        indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "        \n",
    "        S_s_s = []\n",
    "        for ids_aa in indices:\n",
    "            S_s_s.append(Clabel[ids_aa])\n",
    "        S[i, :l] = indices\n",
    "        \n",
    "        S_noise[i, :l] = randomize_list(indices, 0.1)\n",
    "        S_s[i, :l] = S_s_s\n",
    "        \n",
    "        mask_rp[i,:l] = np.ones([l], dtype=np.int32)\n",
    "        \n",
    "        ids+=1\n",
    "\n",
    "    isnan = np.isnan(X)\n",
    "    mask = np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n",
    "    X[isnan] = 0.\n",
    "    \n",
    "    \n",
    "    #if is_train:\n",
    "        #mask = torch.from_numpy(mask_rp).to(dtype=torch.float32, device=device)\n",
    "    #else:\n",
    "        #mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
    "        \n",
    "\n",
    "    # Conversion\n",
    "    mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
    "    mask_rp = torch.from_numpy(mask_rp).to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    residue_idx = torch.from_numpy(residue_idx).to(dtype=torch.long,device=device)\n",
    "    S = torch.from_numpy(S).to(dtype=torch.long,device=device)\n",
    "    S_noise = torch.from_numpy(S_noise).to(dtype=torch.long,device=device)\n",
    "    S_s = torch.from_numpy(S_s).to(dtype=torch.long,device=device)\n",
    "    X = torch.from_numpy(X).to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    mask_self = torch.from_numpy(mask_self).to(dtype=torch.float32, device=device)\n",
    "    chain_M = torch.from_numpy(chain_M).to(dtype=torch.float32, device=device)\n",
    "    chain_encoding_all = torch.from_numpy(chain_encoding_all).to(dtype=torch.long, device=device)\n",
    "    return X, S, mask,mask_rp, lengths, chain_M, residue_idx, mask_self, chain_encoding_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf0ce8-77fa-48c9-9ec4-46480dec8b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13685af7-100e-47ff-83d4-8aeb1ea44c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class StructureLoader():\n",
    "    def __init__(self, dataset, filtered_dict, batch_size=2000, shuffle=True,\n",
    "        collate_fn=lambda x:x, drop_last=False):\n",
    "    \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.filtered_dict = filtered_dict\n",
    "        self.size = list(filtered_dict.keys())\n",
    "        #print(len(self.size))\n",
    "        self.lengths = [len(dataset[i]['seq']) for i in self.size]\n",
    "        #print(len(self.lengths))\n",
    "        self.batch_size = batch_size\n",
    "        sorted_ix = np.argsort(self.lengths)\n",
    "        #print(len(sorted_ix))\n",
    "         \n",
    "\n",
    "        # Cluster into batches of similar sizes\n",
    "        clusters, batch = [], []\n",
    "        batch_max = 0\n",
    "        for ix in sorted_ix:\n",
    " \n",
    "                    batch.append(self.size[ix])\n",
    "                    clusters.append(batch)\n",
    "                    #batch_max = size\n",
    "                    batch, batch_max = [], 0\n",
    "               \n",
    "                \n",
    "                \n",
    "        if len(batch) > 0:\n",
    "            clusters.append(batch)\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clusters)\n",
    "\n",
    "    def __iter__(self):\n",
    "        #np.random.shuffle(self.clusters)\n",
    "        for b_idx in self.clusters:\n",
    "            #print(b_idx)\n",
    "            batch = []\n",
    "            lst_chain = []\n",
    "            length_batch = 0\n",
    "            max_length_batch = 0\n",
    "            for idx in b_idx:\n",
    "                #print(self.filtered_dict.keys())\n",
    "                bb_idx = self.filtered_dict[idx]#.tolist()\n",
    "                bb_l = len(bb_idx)\n",
    "                bb_lst = list(bb_idx.keys())\n",
    "                bb_rand = random.randint(0, bb_l-1)\n",
    "                bb_rand = 0\n",
    "                max_length_batch1 = max([max_length_batch,len(self.dataset[idx][\"seq\"]),len(self.dataset[bb_lst[bb_rand]][\"seq\"])])\n",
    "                if (length_batch+1)*max_length_batch1 < self.batch_size*5:\n",
    "                    length_batch = length_batch+1\n",
    "                    max_length_batch = max([max_length_batch,max_length_batch1])\n",
    "                    batch.append(self.dataset[idx])\n",
    "                    batch.append(self.dataset[bb_lst[bb_rand]])\n",
    "                    lst_chain.append(bb_idx[bb_lst[bb_rand]][0][0])\n",
    "                    lst_chain.append(bb_idx[bb_lst[bb_rand]][1][0])\n",
    "                   \n",
    "            yield batch,lst_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99f83f-ebc4-4e86-b9ba-b3cbc180364b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd610e5-ffdd-4229-a15f-3058de7f2102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json, time, os, sys, glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split, Subset\n",
    "import torch.utils\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import itertools\n",
    "from transformer import TransformerBlock\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# The following gather functions\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "def loss_nll(S, log_probs, mask):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    criterion = torch.nn.NLLLoss(reduction='none')\n",
    "    loss = criterion(\n",
    "        log_probs.contiguous().view(-1, log_probs.size(-1)), S.contiguous().view(-1)\n",
    "    ).view(S.size())\n",
    "    S_argmaxed = torch.argmax(log_probs,-1) #[B, L]\n",
    "    true_false = (S == S_argmaxed).float()\n",
    "    loss_av = torch.sum(loss * mask) / torch.sum(mask)\n",
    "    return loss, loss_av, true_false\n",
    "\n",
    "\n",
    "def loss_smoothed(S, log_probs, mask, weight=0.1):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    S_onehot = torch.nn.functional.one_hot(S, 21).float()\n",
    "\n",
    "    # Label smoothing\n",
    "    S_onehot = S_onehot + weight / float(S_onehot.size(-1))\n",
    "    S_onehot = S_onehot / S_onehot.sum(-1, keepdim=True)\n",
    "\n",
    "    loss = -(S_onehot * log_probs).sum(-1)\n",
    "    loss_av = torch.sum(loss * mask) / 2000.0 #fixed \n",
    "    return loss, loss_av\n",
    "def loss_smoothed_4(S, log_probs, mask, weight=0.1):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    S_onehot = torch.nn.functional.one_hot(S, 4).float()\n",
    "\n",
    "    # Label smoothing\n",
    "    S_onehot = S_onehot + weight / float(S_onehot.size(-1))\n",
    "    S_onehot = S_onehot / S_onehot.sum(-1, keepdim=True)\n",
    "\n",
    "    loss = -(S_onehot * log_probs).sum(-1)\n",
    "    loss_av = torch.sum(loss * mask) / 2000.0 #fixed \n",
    "    return loss, loss_av\n",
    "\n",
    "\n",
    "\n",
    "def gather_edges(edges, neighbor_idx):\n",
    "    # Features [B,N,N,C] at Neighbor indices [B,N,K] => Neighbor features [B,N,K,C]\n",
    "    neighbors = neighbor_idx.unsqueeze(-1).expand(-1, -1, -1, edges.size(-1))\n",
    "    edge_features = torch.gather(edges, 2, neighbors)\n",
    "    return edge_features\n",
    "\n",
    "def gather_nodes(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n",
    "    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n",
    "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n",
    "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    # Gather and re-pack\n",
    "    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n",
    "    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n",
    "    return neighbor_features\n",
    "\n",
    "def gather_nodes_t(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor index [B,K] => Neighbor features[B,K,C]\n",
    "    idx_flat = neighbor_idx.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    neighbor_features = torch.gather(nodes, 1, idx_flat)\n",
    "    return neighbor_features\n",
    "\n",
    "def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n",
    "    h_nodes = gather_nodes(h_nodes, E_idx)\n",
    "    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n",
    "    return h_nn\n",
    "\n",
    "\n",
    "class EncLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(EncLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.norm3 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W11 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W13 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, h_V, h_E, E_idx, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / self.scale\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
    "        h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))\n",
    "        h_E = self.norm3(h_E + self.dropout3(h_message))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return h_V, h_E\n",
    "\n",
    "\n",
    "\n",
    "class DecLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(DecLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "\n",
    "        # Concatenate h_V_i to h_E_ij \n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_E.size(-2),-1)\n",
    "        \n",
    "        \n",
    "        h_EV = torch.cat([h_V_expand, h_E], -1)\n",
    "        \n",
    "        \n",
    "\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        \n",
    "        #print(h_message[0,0].sum())\n",
    "        \n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / self.scale\n",
    "        \n",
    "        #print(dh[0,0].sum())\n",
    "\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "\n",
    "        # Position-wise feedforward\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "            \n",
    "        #print(h_V[0,0].sum())\n",
    "        return h_V\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, num_hidden, num_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.W_in = nn.Linear(num_hidden, num_ff, bias=True)\n",
    "        self.W_out = nn.Linear(num_ff, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "    def forward(self, h_V):\n",
    "        h = self.act(self.W_in(h_V))\n",
    "        h = self.W_out(h)\n",
    "        return h\n",
    "\n",
    "class PositionalEncodings(nn.Module):\n",
    "    def __init__(self, num_embeddings, max_relative_feature=32):\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.max_relative_feature = max_relative_feature\n",
    "        self.linear = nn.Linear(2*max_relative_feature+1+1, num_embeddings)\n",
    "\n",
    "    def forward(self, offset, mask):\n",
    "        d = torch.clip(offset + self.max_relative_feature, 0, 2*self.max_relative_feature)*mask + (1-mask)*(2*self.max_relative_feature+1)\n",
    "        d_onehot = torch.nn.functional.one_hot(d, 2*self.max_relative_feature+1+1)\n",
    "        E = self.linear(d_onehot.float())\n",
    "        return E\n",
    "\n",
    "\n",
    "class ProteinFeatures(nn.Module):\n",
    "    def __init__(self, edge_features, node_features, num_positional_embeddings=16,\n",
    "        num_rbf=16, top_k=30, augment_eps=0., num_chain_embeddings=16):\n",
    "        \"\"\" Extract protein features \"\"\"\n",
    "        super(ProteinFeatures, self).__init__()\n",
    "        self.edge_features = edge_features\n",
    "        self.node_features = node_features\n",
    "        self.top_k = top_k\n",
    "        self.augment_eps = augment_eps \n",
    "        self.num_rbf = num_rbf\n",
    "        self.num_positional_embeddings = num_positional_embeddings\n",
    "\n",
    "        self.embeddings = PositionalEncodings(num_positional_embeddings)\n",
    "        node_in, edge_in = 6, num_positional_embeddings + num_rbf*25\n",
    "        self.edge_embedding = nn.Linear(edge_in, edge_features, bias=False)\n",
    "        self.norm_edges = nn.LayerNorm(edge_features)\n",
    "\n",
    "    def _dist(self, X, mask, eps=1E-6):\n",
    "        mask_2D = torch.unsqueeze(mask,1) * torch.unsqueeze(mask,2)\n",
    "        dX = torch.unsqueeze(X,1) - torch.unsqueeze(X,2)\n",
    "        D = mask_2D * torch.sqrt(torch.sum(dX**2, 3) + eps)\n",
    "        D_max, _ = torch.max(D, -1, keepdim=True)\n",
    "        D_adjust = D + (1. - mask_2D) * D_max\n",
    "        sampled_top_k = self.top_k\n",
    "        D_neighbors, E_idx = torch.topk(D_adjust, np.minimum(self.top_k, X.shape[1]), dim=-1, largest=False)\n",
    "        return D_neighbors, E_idx\n",
    "\n",
    "    def _rbf(self, D):\n",
    "        device = D.device\n",
    "        D_min, D_max, D_count = 2., 22., self.num_rbf\n",
    "        D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "        D_mu = D_mu.view([1,1,1,-1])\n",
    "        D_sigma = (D_max - D_min) / D_count\n",
    "        D_expand = torch.unsqueeze(D, -1)\n",
    "        RBF = torch.exp(-((D_expand - D_mu) / D_sigma)**2)\n",
    "        return RBF\n",
    "\n",
    "    def _get_rbf(self, A, B, E_idx):\n",
    "        D_A_B = torch.sqrt(torch.sum((A[:,:,None,:] - B[:,None,:,:])**2,-1) + 1e-6) #[B, L, L]\n",
    "        D_A_B_neighbors = gather_edges(D_A_B[:,:,:,None], E_idx)[:,:,:,0] #[B,L,K]\n",
    "        RBF_A_B = self._rbf(D_A_B_neighbors)\n",
    "        return RBF_A_B\n",
    "\n",
    "    def forward(self, X, mask, residue_idx, chain_labels):\n",
    "        if self.training and self.augment_eps > 0:\n",
    "            X = X + self.augment_eps * torch.randn_like(X)\n",
    "        \n",
    "        b = X[:,:,1,:] - X[:,:,0,:]\n",
    "        c = X[:,:,2,:] - X[:,:,1,:]\n",
    "        a = torch.cross(b, c, dim=-1)\n",
    "        Cb = -0.58273431*a + 0.56802827*b - 0.54067466*c + X[:,:,1,:]\n",
    "        Ca = X[:,:,1,:]\n",
    "        N = X[:,:,0,:]\n",
    "        C = X[:,:,2,:]\n",
    "        O = X[:,:,3,:]\n",
    " \n",
    "        D_neighbors, E_idx = self._dist(Ca, mask)\n",
    "\n",
    "        RBF_all = []\n",
    "        RBF_all.append(self._rbf(D_neighbors)) #Ca-Ca\n",
    "        RBF_all.append(self._get_rbf(N, N, E_idx)) #N-N\n",
    "        RBF_all.append(self._get_rbf(C, C, E_idx)) #C-C\n",
    "        RBF_all.append(self._get_rbf(O, O, E_idx)) #O-O\n",
    "        RBF_all.append(self._get_rbf(Cb, Cb, E_idx)) #Cb-Cb\n",
    "        RBF_all.append(self._get_rbf(Ca, N, E_idx)) #Ca-N\n",
    "        RBF_all.append(self._get_rbf(Ca, C, E_idx)) #Ca-C\n",
    "        RBF_all.append(self._get_rbf(Ca, O, E_idx)) #Ca-O\n",
    "        RBF_all.append(self._get_rbf(Ca, Cb, E_idx)) #Ca-Cb\n",
    "        RBF_all.append(self._get_rbf(N, C, E_idx)) #N-C\n",
    "        RBF_all.append(self._get_rbf(N, O, E_idx)) #N-O\n",
    "        RBF_all.append(self._get_rbf(N, Cb, E_idx)) #N-Cb\n",
    "        RBF_all.append(self._get_rbf(Cb, C, E_idx)) #Cb-C\n",
    "        RBF_all.append(self._get_rbf(Cb, O, E_idx)) #Cb-O\n",
    "        RBF_all.append(self._get_rbf(O, C, E_idx)) #O-C\n",
    "        RBF_all.append(self._get_rbf(N, Ca, E_idx)) #N-Ca\n",
    "        RBF_all.append(self._get_rbf(C, Ca, E_idx)) #C-Ca\n",
    "        RBF_all.append(self._get_rbf(O, Ca, E_idx)) #O-Ca\n",
    "        RBF_all.append(self._get_rbf(Cb, Ca, E_idx)) #Cb-Ca\n",
    "        RBF_all.append(self._get_rbf(C, N, E_idx)) #C-N\n",
    "        RBF_all.append(self._get_rbf(O, N, E_idx)) #O-N\n",
    "        RBF_all.append(self._get_rbf(Cb, N, E_idx)) #Cb-N\n",
    "        RBF_all.append(self._get_rbf(C, Cb, E_idx)) #C-Cb\n",
    "        RBF_all.append(self._get_rbf(O, Cb, E_idx)) #O-Cb\n",
    "        RBF_all.append(self._get_rbf(C, O, E_idx)) #C-O\n",
    "        RBF_all = torch.cat(tuple(RBF_all), dim=-1)\n",
    "\n",
    "        offset = residue_idx[:,:,None]-residue_idx[:,None,:]\n",
    "        offset = gather_edges(offset[:,:,:,None], E_idx)[:,:,:,0] #[B, L, K]\n",
    "\n",
    "        d_chains = ((chain_labels[:, :, None] - chain_labels[:,None,:])==0).long() #find self vs non-self interaction\n",
    "        E_chains = gather_edges(d_chains[:,:,:,None], E_idx)[:,:,:,0]\n",
    "        E_positional = self.embeddings(offset.long(), E_chains)\n",
    "        E = torch.cat((E_positional, RBF_all), -1)\n",
    "        E = self.edge_embedding(E)\n",
    "        E = self.norm_edges(E)\n",
    "        return E, E_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e33936c-8d81-4031-a26c-887dbc8d72a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test = torch.load(\"./x_test_multi.pt\")\n",
    "data_set = \"test1\"\n",
    "if data_set == \"test1\":\n",
    "    dict_x_test = torch.load(\"./dict_x_test_30_159.pt\")\n",
    "    StructureLoader_test = StructureLoader(x_test,dict_x_test)\n",
    "elif data_set == \"test2\":\n",
    "    dict_x_test = torch.load(\"./dict_x_test_sim_50_rmsd_2.pt\")\n",
    "    StructureLoader_test = StructureLoader(x_test,dict_x_test)\n",
    "elif data_set == \"test3\":\n",
    "    StructureLoader_test = torch.load(\"./lst_diff_inter_38_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332f5ac-aaf1-4857-a517-a85c84b7ba91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53183d43-f1af-4620-875f-6e3b6116c9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2551d957-f5a0-490d-8895-9da8fac1678f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n"
     ]
    }
   ],
   "source": [
    "def _scores(S, log_probs, mask):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    criterion = torch.nn.NLLLoss(reduction='none')\n",
    "    loss = criterion(\n",
    "        log_probs.contiguous().view(-1,log_probs.size(-1)),\n",
    "        S.contiguous().view(-1)\n",
    "    ).view(S.size())\n",
    "    scores = torch.sum(loss * mask, dim=-1) / torch.sum(mask, dim=-1)\n",
    "    return scores\n",
    "#single simple\n",
    "class ProteinMPNN(nn.Module):\n",
    "    def __init__(self, num_letters, node_features, edge_features,\n",
    "        hidden_dim, num_encoder_layers=3, num_decoder_layers=3,\n",
    "        vocab=21, k_neighbors=48, augment_eps=0.05, dropout=0.1, ca_only=False):\n",
    "        super(ProteinMPNN, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.node_features = node_features\n",
    "        self.edge_features = edge_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Featurization layers\n",
    "        if ca_only:\n",
    "            self.features = CA_ProteinFeatures(node_features, edge_features, top_k=k_neighbors, augment_eps=augment_eps)\n",
    "            self.W_v = nn.Linear(node_features, hidden_dim, bias=True)\n",
    "        else:\n",
    "            self.features = ProteinFeatures(node_features, edge_features, top_k=k_neighbors, augment_eps=augment_eps)\n",
    "\n",
    "        self.W_e = nn.Linear(edge_features, hidden_dim, bias=True)\n",
    "        self.W_s = nn.Embedding(vocab, hidden_dim)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncLayer(hidden_dim, hidden_dim*2, dropout=dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecLayer(hidden_dim, hidden_dim*3, dropout=dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.W_out = nn.Linear(hidden_dim, num_letters, bias=True)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, X, S, mask, chain_M, residue_idx, chain_encoding_all,is_eval = False):\n",
    "        \"\"\" Graph-conditioned sequence model \"\"\"\n",
    "        device=X.device\n",
    "        # Prepare node and edge embeddings\n",
    "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
    "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device)\n",
    "        h_E = self.W_e(E)\n",
    "\n",
    "        # Encoder is unmasked self-attention\n",
    "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
    "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
    "        for layer in self.encoder_layers:\n",
    "            h_V, h_E = torch.utils.checkpoint.checkpoint(layer, h_V, h_E, E_idx, mask, mask_attend)\n",
    "\n",
    "        # Concatenate sequence embeddings for autoregressive decoder\n",
    "        h_S = self.W_s(S)\n",
    "        h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)\n",
    "\n",
    "        # Build encoder embeddings\n",
    "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
    "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
    "\n",
    "\n",
    "        \n",
    "        if is_eval:\n",
    "\n",
    "            \n",
    "            chain_M = chain_M*mask #update chain_M to include missing regions\n",
    "            decoding_order = torch.argsort((chain_M+0.0001)*(torch.abs(torch.randn(chain_M.shape, device=device)))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
    "            mask_size = E_idx.shape[1]\n",
    "            permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
    "            order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
    "            chain_M_d = - (chain_M - torch.ones_like(chain_M))\n",
    "            chain_M_d = chain_M_d.unsqueeze(-2).repeat(1,chain_M.shape[-1],1)\n",
    "            #order_mask_backward = order_mask_backward * chain_M_d\n",
    "            \n",
    "            order_mask_backward = torch.ones_like(order_mask_backward) * chain_M_d\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            chain_M = mask*chain_M #update chain_M to include missing regions\n",
    "            decoding_order = torch.argsort((chain_M+0.0001)*(torch.abs(torch.randn(chain_M.shape, device=device)))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
    "            mask_size = E_idx.shape[1]\n",
    "            permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
    "            order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
    "        \n",
    "        \n",
    "\n",
    "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
    "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
    "        mask_bw = mask_1D * mask_attend\n",
    "        mask_fw = mask_1D * (1. - mask_attend)\n",
    "\n",
    "        \n",
    "            \n",
    "       \n",
    "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            h_ESV = cat_neighbors_nodes(h_V, h_ES, E_idx)\n",
    "            \n",
    "            h_ESV = mask_bw * h_ESV + h_EXV_encoder_fw\n",
    "\n",
    "            h_V = torch.utils.checkpoint.checkpoint(layer, h_V, h_ESV, mask)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        logits = self.W_out(h_V)\n",
    "\n",
    "        \n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return log_probs \n",
    "\n",
    "\n",
    "\n",
    "    def sample(self, X, randn, S_true, chain_mask, chain_encoding_all, residue_idx, mask=None, temperature=1.0,chain_M_pos=None):\n",
    "        device = X.device\n",
    "        # Prepare node and edge embeddings\n",
    "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
    "       \n",
    "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=device)\n",
    "  \n",
    "        h_E = self.W_e(E)\n",
    "  \n",
    "        # Encoder is unmasked self-attention\n",
    "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
    "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
    "        for layer in self.encoder_layers:\n",
    "            h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)\n",
    "    \n",
    "        # Decoder uses masked self-attention\n",
    "        chain_mask = chain_mask*chain_M_pos*mask #update chain_M to include missing regions\n",
    "\n",
    "        \n",
    "        \n",
    "        decoding_order = torch.argsort((chain_mask+0.0001)*(torch.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
    "\n",
    "        mask_size = E_idx.shape[1]\n",
    "        permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
    "        order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
    "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
    "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
    "        mask_bw = mask_1D * mask_attend\n",
    "        mask_fw = mask_1D * (1. - mask_attend)\n",
    "\n",
    "        N_batch, N_nodes = X.size(0), X.size(1)\n",
    "        log_probs = torch.zeros((N_batch, N_nodes, 21), device=device)\n",
    "        all_probs = torch.zeros((N_batch, N_nodes, 21), device=device, dtype=torch.float32)\n",
    "        h_S = torch.zeros_like(h_V, device=device)\n",
    "        S = torch.zeros((N_batch, N_nodes), dtype=torch.int64, device=device)\n",
    "        h_V_stack = [h_V] + [torch.zeros_like(h_V, device=device) for _ in range(len(self.decoder_layers))]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
    "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
    "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder#b n k c\n",
    "        #print(chain_mask.shape,N_nodes)\n",
    "        for t_ in range(N_nodes):\n",
    "            t = decoding_order[:,t_] #[B]\n",
    "            #print(t)\n",
    "            chain_mask_gathered = torch.gather(chain_mask, 1, t[:,None]) #[B]\n",
    "            mask_gathered = torch.gather(mask, 1, t[:,None]) #[B]\n",
    "            #bias_by_res_gathered = torch.gather(bias_by_res, 1, t[:,None,None].repeat(1,1,21))[:,0,:] #[B, 21]\n",
    "            if (mask_gathered==0).all(): #for padded or missing regions only\n",
    "                S_t = torch.gather(S_true, 1, t[:,None])\n",
    "            else:\n",
    "                # Hidden layers\n",
    "                E_idx_t = torch.gather(E_idx, 1, t[:,None,None].repeat(1,1,E_idx.shape[-1]))\n",
    "                h_E_t = torch.gather(h_E, 1, t[:,None,None,None].repeat(1,1,h_E.shape[-2], h_E.shape[-1]))\n",
    "                h_ES_t = cat_neighbors_nodes(h_S, h_E_t, E_idx_t)\n",
    "                h_EXV_encoder_t = torch.gather(h_EXV_encoder_fw, 1, t[:,None,None,None].repeat(1,1,h_EXV_encoder_fw.shape[-2], h_EXV_encoder_fw.shape[-1]))\n",
    "                mask_t = torch.gather(mask, 1, t[:,None])\n",
    "                for l, layer in enumerate(self.decoder_layers):\n",
    "                    # Updated relational features for future states\n",
    "                    h_ESV_decoder_t = cat_neighbors_nodes(h_V_stack[l], h_ES_t, E_idx_t)\n",
    "                    h_V_t = torch.gather(h_V_stack[l], 1, t[:,None,None].repeat(1,1,h_V_stack[l].shape[-1]))\n",
    "                    h_ESV_t = torch.gather(mask_bw, 1, t[:,None,None,None].repeat(1,1,mask_bw.shape[-2], mask_bw.shape[-1])) * h_ESV_decoder_t + h_EXV_encoder_t\n",
    "                    h_V_stack[l+1].scatter_(1, t[:,None,None].repeat(1,1,h_V.shape[-1]), layer(h_V_t, h_ESV_t, mask_V=mask_t))\n",
    "                # Sampling step\n",
    "                h_V_t = torch.gather(h_V_stack[-1], 1, t[:,None,None].repeat(1,1,h_V_stack[-1].shape[-1]))[:,0]\n",
    "\n",
    "                logits = self.W_out(h_V_t) / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                S_t =  torch.multinomial(probs, 1)\n",
    "\n",
    "                \n",
    "                all_probs.scatter_(1, t[:,None,None].repeat(1,1,21), (chain_mask_gathered[:,:,None,]*probs[:,None,:]).float())\n",
    "            S_true_gathered = torch.gather(S_true, 1, t[:,None])\n",
    "            S_t = (S_t*chain_mask_gathered+S_true_gathered*(1.0-chain_mask_gathered)).long()\n",
    "            temp1 = self.W_s(S_t)\n",
    "            h_S.scatter_(1, t[:,None,None].repeat(1,1,temp1.shape[-1]), temp1)\n",
    "            S.scatter_(1, t[:,None], S_t)\n",
    "        output_dict = {\"S\": S, \"probs\": all_probs, \"decoding_order\": decoding_order}\n",
    "        return output_dict[\"S\"],all_probs\n",
    "\n",
    "checkpoint_path = \"./vanilla_model_weights/v_48_020.pt\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device) \n",
    "noise_level_print = checkpoint['noise_level']\n",
    "model = ProteinMPNN(num_letters=21, node_features=128, edge_features=128, hidden_dim=128, num_encoder_layers=3, num_decoder_layers=3, augment_eps=0., k_neighbors=checkpoint['num_edges'])\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "print(\"load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10144fc-8956-4a7d-98cb-c26ae4638caf",
   "metadata": {},
   "source": [
    "# autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eba7809-6d5a-4f6a-921e-271e4f48b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "lst_score = []\n",
    "lst_acc = []\n",
    "with torch.no_grad():\n",
    "    lst_mask_chain = []\n",
    "    lst_mask_chain = []\n",
    "    lst_name = []\n",
    "    lst_x = []\n",
    "    lst_seq_pre = []\n",
    "    ns = 0\n",
    "    #if True:\n",
    "    for batch in StructureLoader_test:\n",
    "        print(ns)\n",
    "        X, S, mask, mask_train, lengths, chain_M, residue_idx, mask_self, chain_encoding_all =  featurize(batch[0][:],\n",
    "                                                                                                         batch[1][:],\n",
    "                                                                                                         device,is_train=False)\n",
    "        \n",
    "        masked_list = batch[0][0]['masked_list']\n",
    "        masked_seq_name = \"seq_chain_\"+masked_list[0]\n",
    "        masked_seq = batch[0][0][masked_seq_name]\n",
    "        length_seq = len(masked_seq)\n",
    "        B =2\n",
    "        for sdp in range(1):\n",
    "            randn = torch.randn(chain_M.shape).to(device)\n",
    "            S_pre,log_probs = model.sample( X, randn, S, chain_M, chain_encoding_all, residue_idx, mask=mask, \\\n",
    "                                           temperature=0.1,chain_M_pos=mask)\n",
    "            \n",
    "            for i in batch[0]:\n",
    "                lst_name.append(i[\"name\"]+\"_\"+str(ns)+\"_\"+str(sdp))\n",
    "                lst_x.append(i)\n",
    "            for ss in range(len(mask)) :\n",
    "                lst_seq_pre.append(S_pre[ss].cpu())\n",
    "                lst_mask_chain.append((mask[ss] *chain_M[ss]).cpu())\n",
    "                \n",
    "                \n",
    "            mask_for_loss = mask * chain_M\n",
    "\n",
    "            \n",
    "            score_design = _scores(S, log_probs, mask_for_loss)\n",
    "            lst_score.append(score_design)\n",
    "        \n",
    "            loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
    " \n",
    "            acc_design = torch.sum(torch.sum(torch.nn.functional.one_hot(S[0], 21) * \n",
    "                                                  torch.nn.functional.one_hot(S_pre[0], 21), axis=-1) * mask_for_loss[0]) / \\\n",
    "                                       torch.sum(mask_for_loss[0])\n",
    "            lst_acc.append(acc_design.cpu())\n",
    "                \n",
    "        \n",
    "\n",
    "\n",
    "        ns+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c819ce9f-52ad-4b0a-907a-ebad39785ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f15aae54-5c91-4a00-a0f3-441f8d573ccb",
   "metadata": {},
   "source": [
    "# save desgin sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa26214b-deb2-4507-845b-9c0be29ddb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_fasta = \"design_sequence.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b278001-1fbf-4223-9478-bf92ed3a5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_last_one(lst):\n",
    "    first_one = -1\n",
    "    last_one = -1\n",
    "    \n",
    "    for i, value in enumerate(lst):\n",
    "        if value == 1:\n",
    "            if first_one == -1:\n",
    "                first_one = i\n",
    "            last_one = i\n",
    "            \n",
    "    return first_one, last_one\n",
    "def indices_to_chars(indices):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    return ''.join([alphabet[int(i)] for i in indices])\n",
    "def seqs_get_signle(lst_x,lst_seq_pre):\n",
    "    length_lst = len(lst_x)\n",
    "    lst_seq_all = []\n",
    "\n",
    "    for i in range(length_lst):\n",
    "        lst_seq_1 = []\n",
    "        masked_list = lst_x[i]['masked_list']\n",
    "        masked_seq_name = \"seq_chain_\"+masked_list[0]\n",
    "        masked_seq = lst_x[i][masked_seq_name]\n",
    "        length_seq = len(masked_seq)\n",
    "        #a,b = find_first_last_one(lst_mask_chain[i])\n",
    "        seq_pre_AA = lst_seq_pre[i][:length_seq]\n",
    "        ######\n",
    "        #seq_pre_str = indices_to_chars(seq_pre_AA[a:b+1])\n",
    "        seq_pre_str = indices_to_chars(seq_pre_AA)\n",
    "        ######\n",
    "        lst_seq_1.append(seq_pre_str)\n",
    "\n",
    "\n",
    "        lst_seq_keys_1 = []\n",
    "        for keys_1 in lst_x[i]:\n",
    "            if \"seq_chain_\" in keys_1 and keys_1 != masked_seq_name:\n",
    "                lst_seq_keys_1.append(keys_1)\n",
    "\n",
    "        for keys_seq in lst_seq_keys_1:\n",
    "            lst_seq_1.append(lst_x[i][keys_seq])\n",
    "\n",
    "        lst_seq_all.append(lst_seq_1)\n",
    "\n",
    "    return lst_seq_all\n",
    "\n",
    "def seq_single_complex_cross(lst_seq_all):\n",
    "    lengths_all = len(lst_seq_all)\n",
    "    lengths_all = int(lengths_all//2)\n",
    "    ns = 0\n",
    "    lst_cross = []\n",
    "    for i in range(lengths_all):\n",
    "        lst1 = lst_seq_all[i*2]\n",
    "        lst2 = lst_seq_all[i * 2 + 1]\n",
    "\n",
    "        lst_3 = []\n",
    "        lst_4 = []\n",
    "        lst_3.append(lst2[0])\n",
    "        lst_4.append(lst1[0])\n",
    "\n",
    "        for l in lst1[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_3.append(l)\n",
    "        for l in lst2[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_4.append(l)\n",
    "        lst_cross.append(lst_3)\n",
    "        lst_cross.append(lst_4)\n",
    "\n",
    "    return lst_cross\n",
    "\n",
    "#def get_fasta_lst(lst):\n",
    "lst_pre_mpnn =seqs_get_signle(lst_x,lst_seq_pre)\n",
    "with open(path_fasta, \"w\") as file:\n",
    "     \n",
    "    for i in range(len(lst_name)):\n",
    "        \n",
    "        file.write(f\">{lst_name[i]} \\n\")\n",
    "        seqs = \"\"\n",
    "        for s in lst_pre_mpnn[i]:\n",
    "            \n",
    "            all_sequence = list(s)\n",
    "            for aas in range(len(all_sequence)):\n",
    "                if all_sequence[aas] == \"X\":\n",
    "                    all_sequence[aas] = \"A\"\n",
    "            all_sequence = \"\".join(all_sequence)\n",
    "            \n",
    "            seqs = seqs+all_sequence+\":\"\n",
    "        file.write(seqs[:-1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a569ce7-4c5b-4d0a-9832-4006b410aac5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_first_last_one(lst):\n",
    "    first_one = -1\n",
    "    last_one = -1\n",
    "    \n",
    "    for i, value in enumerate(lst):\n",
    "        if value == 1:\n",
    "            if first_one == -1:\n",
    "                first_one = i\n",
    "            last_one = i\n",
    "            \n",
    "    return first_one, last_one\n",
    "def indices_to_chars(indices):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    return ''.join([alphabet[int(i)] for i in indices])\n",
    "def seqs_get_signle(lst_x,lst_seq_pre,lst_mask_chain):\n",
    "    length_lst = len(lst_x)\n",
    "    lst_seq_all = []\n",
    "\n",
    "    for i in range(length_lst):\n",
    "        lst_seq_1 = []\n",
    "        masked_list = lst_x[i]['masked_list']\n",
    "        masked_seq_name = \"seq_chain_\"+masked_list[0]\n",
    "        masked_seq = lst_x[i][masked_seq_name]\n",
    "        length_seq = len(masked_seq)\n",
    "        a,b = find_first_last_one(lst_mask_chain[i])\n",
    "        seq_pre_AA = lst_seq_pre[i][:length_seq]\n",
    "        ######\n",
    "        #seq_pre_str = indices_to_chars(seq_pre_AA[a:b+1])\n",
    "        seq_pre_str = indices_to_chars(seq_pre_AA)\n",
    "        ######\n",
    "        lst_seq_1.append(seq_pre_str)\n",
    "\n",
    "\n",
    "        lst_seq_keys_1 = []\n",
    "        for keys_1 in lst_x[i]:\n",
    "            if \"seq_chain_\" in keys_1 and keys_1 != masked_seq_name:\n",
    "                lst_seq_keys_1.append(keys_1)\n",
    "\n",
    "        for keys_seq in lst_seq_keys_1:\n",
    "            lst_seq_1.append(lst_x[i][keys_seq])\n",
    "\n",
    "        lst_seq_all.append(lst_seq_1)\n",
    "\n",
    "    return lst_seq_all\n",
    "\n",
    "def seq_single_complex_cross(lst_seq_all):\n",
    "    lengths_all = len(lst_seq_all)\n",
    "    lengths_all = int(lengths_all//2)\n",
    "    ns = 0\n",
    "    lst_cross = []\n",
    "    for i in range(lengths_all):\n",
    "        lst1 = lst_seq_all[i*2]\n",
    "        lst2 = lst_seq_all[i * 2 + 1]\n",
    "\n",
    "        lst_3 = []\n",
    "        lst_4 = []\n",
    "        lst_3.append(lst2[0])\n",
    "        lst_4.append(lst1[0])\n",
    "\n",
    "        for l in lst1[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_3.append(l)\n",
    "        for l in lst2[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_4.append(l)\n",
    "        lst_cross.append(lst_3)\n",
    "        lst_cross.append(lst_4)\n",
    "\n",
    "    return lst_cross\n",
    "\n",
    "#def get_fasta_lst(lst):\n",
    "lst_pre_mpnn =seqs_get_signle(lst_x,lst_seq_pre,lst_mask_chain)\n",
    "with open(\"test_111.fasta\", \"w\") as file:\n",
    "     \n",
    "    for i in range(len(lst_name)):\n",
    "        \n",
    "        file.write(f\">{lst_name[i]} \\n\")\n",
    "        seqs = \"\"\n",
    "        for s in lst_pre_mpnn[i]:\n",
    "            \n",
    "            all_sequence = list(s)\n",
    "            for aas in range(len(all_sequence)):\n",
    "                if all_sequence[aas] == \"X\":\n",
    "                    all_sequence[aas] = \"A\"\n",
    "            all_sequence = \"\".join(all_sequence)\n",
    "            \n",
    "            seqs = seqs+all_sequence+\":\"\n",
    "        file.write(seqs[:-1] + \"\\n\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f7c905-7412-402f-bb79-d51ae8027a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "lst_pre_mpnn1 = seq_single_complex_cross(lst_pre_mpnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff0a77-e7c1-4bb6-8179-b211f49ed989",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fasta = \"design_sequence1.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d64a79b1-5d8c-4706-b216-1e75fec121ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(path_fasta, \"w\") as file:\n",
    "     \n",
    "    for i in range(len(lst_name)):\n",
    "        \n",
    "        file.write(f\">{lst_name[i]} \\n\")\n",
    "        seqs = \"\"\n",
    "        for s in lst_pre_mpnn1[i]:\n",
    "            \n",
    "            all_sequence = list(s)\n",
    "            for aas in range(len(all_sequence)):\n",
    "                if all_sequence[aas] == \"X\":\n",
    "                    all_sequence[aas] = \"A\"\n",
    "            all_sequence = \"\".join(all_sequence)\n",
    "            \n",
    "            seqs = seqs+all_sequence+\":\"\n",
    "        file.write(seqs[:-1] + \"\\n\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2114ac5-0c79-4fd0-b415-c692ba312207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9a09a-8373-4b66-9722-747cb291faff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51d39cd8-dc31-4b60-8c08-6b0c34dc663f",
   "metadata": {},
   "source": [
    "# non-autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5fdb8d-6dc1-47a1-aae4-a81db94059bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dssg/home/acct-clsyzs/clsyzs/.conda/envs/esm_dds/lib/python3.7/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.484 5.140\n"
     ]
    }
   ],
   "source": [
    "#single\n",
    "lst_seq_pre = []\n",
    "lst_mask_chain = []\n",
    "model.eval()\n",
    "lst_acc = []\n",
    "lst_name = []\n",
    "lst_x = []\n",
    "with torch.no_grad():\n",
    "    validation_sum, validation_weights = 0., 0.\n",
    "    validation_acc = 0.\n",
    "    for _, batch in enumerate(StructureLoader_test):\n",
    "        X, S, mask, mask_train, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch[0],\n",
    "                                                                                                         batch[1],\n",
    "                                                                                                         device,\n",
    "                                                                                                         is_train=False)\n",
    "\n",
    "\n",
    "        for i in batch[0]:\n",
    "            lst_name.append(i[\"name\"])\n",
    "            lst_x.append(i)\n",
    "\n",
    "        B = S.shape[0]\n",
    "\n",
    "        log_probs = model(X, S, mask, chain_M, residue_idx, chain_encoding_all, is_eval=True)\n",
    "        for i in range(len(log_probs)):\n",
    "            seq1 = torch.argmax(log_probs[i], -1).cpu() * (mask[i] *chain_M[i]).cpu() + \\\n",
    "            (torch.ones_like((mask[i] *chain_M[i]).cpu()) - (mask[i] *chain_M[i]).cpu() ) *S[i].cpu()\n",
    "            \n",
    "            lst_seq_pre.append(seq1)\n",
    "            lst_mask_chain.append((mask[i] *chain_M[i]).cpu())\n",
    "        mask_for_loss = mask * chain_M\n",
    "\n",
    "\n",
    "        loss, loss_av, true_false = loss_nll(S, log_probs, mask_for_loss)\n",
    "        lst_acc = lst_acc + list(torch.sum(true_false * mask_for_loss, -1).cpu().data.numpy() / torch.sum(mask_for_loss,\n",
    "                                                                                                          -1).cpu().data.numpy())\n",
    "        validation_sum += torch.sum(loss * mask_for_loss).cpu().data.numpy()\n",
    "        validation_acc += torch.sum(true_false * mask_for_loss).cpu().data.numpy()\n",
    "        validation_weights += torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "\n",
    "validation_loss = validation_sum / validation_weights\n",
    "validation_accuracy = validation_acc / validation_weights\n",
    "validation_perplexity = np.exp(validation_loss)\n",
    "\n",
    "# train_perplexity_ = np.format_float_positional(np.float32(train_perplexity), unique=False, precision=3)\n",
    "validation_perplexity_ = np.format_float_positional(np.float32(validation_perplexity), unique=False, precision=3)\n",
    "# train_accuracy_ = np.format_float_positional(np.float32(train_accuracy), unique=False, precision=3)\n",
    "validation_accuracy_ = np.format_float_positional(np.float32(validation_accuracy), unique=False, precision=3)\n",
    "\n",
    "\n",
    "print(validation_accuracy_,validation_perplexity_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac332389-670e-4474-be32-4df23ed333f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 0.484 5.140\n"
     ]
    }
   ],
   "source": [
    "print(len(lst_acc),validation_accuracy_,validation_perplexity_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8053203-b6a4-4a9b-ad9a-2ec9f5c6059a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esm_dds)",
   "language": "python",
   "name": "esm_dds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
