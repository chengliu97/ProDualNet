{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3faca6-1171-445f-86e7-29c39c9271bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is based on ProteinMPNN/Pifold/esm, under the MIT License.\n",
    "# Source: https://github.com/dauparas/ProteinMPNN, https://github.com/A4Bio/PiFold,https://github.com/facebookresearch/esm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6483da9-8760-4dd2-a112-3eedd3db0f47",
   "metadata": {},
   "source": [
    "# NoiseMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b383c6b5-eb26-43d6-b184-e9d0c3ed7bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dssg/home/acct-clsyzs/clsyzs/.conda/envs/esm_dds/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os.path\n",
    "import json, time, os, sys, glob\n",
    "import shutil\n",
    "import warnings\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import queue\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os.path\n",
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor    \n",
    "from utils import worker_init_fn, get_pdbs, loader_pdb, build_training_clusters, PDB_dataset, StructureDataset, StructureLoader\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9f14c-a69e-4a1f-a751-093987bdaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdbs(data_loader, repeat=1, max_length=2000, num_units=1000000):\n",
    "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
    "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
    "    chain_alphabet = init_alphabet + extra_alphabet\n",
    "    c = 0\n",
    "    c1 = 0\n",
    "    pdb_dict_list = []\n",
    "    t0 = time.time()\n",
    "    for _ in range(repeat):\n",
    "        for step,t in enumerate(data_loader):\n",
    "            t = {k:v[0] for k,v in t.items()}\n",
    "            c1 += 1\n",
    "            if 'label' in list(t):\n",
    "                my_dict = {}\n",
    "                s = 0\n",
    "                concat_seq = ''\n",
    "                concat_N = []\n",
    "                concat_CA = []\n",
    "                concat_C = []\n",
    "                concat_O = []\n",
    "                concat_mask = []\n",
    "                coords_dict = {}\n",
    "                mask_list = []\n",
    "                visible_list = []\n",
    "                if len(list(np.unique(t['idx']))) < 352:\n",
    "                    for idx in list(np.unique(t['idx'])):\n",
    "                        letter = chain_alphabet[idx]\n",
    "                        res = np.argwhere(t['idx']==idx)\n",
    "                        initial_sequence= \"\".join(list(np.array(list(t['seq']))[res][0,]))\n",
    "                        if initial_sequence[-6:] == \"HHHHHH\":\n",
    "                            res = res[:,:-6]\n",
    "                        if initial_sequence[0:6] == \"HHHHHH\":\n",
    "                            res = res[:,6:]\n",
    "                        if initial_sequence[-7:-1] == \"HHHHHH\":\n",
    "                           res = res[:,:-7]\n",
    "                        if initial_sequence[-8:-2] == \"HHHHHH\":\n",
    "                           res = res[:,:-8]\n",
    "                        if initial_sequence[-9:-3] == \"HHHHHH\":\n",
    "                           res = res[:,:-9]\n",
    "                        if initial_sequence[-10:-4] == \"HHHHHH\":\n",
    "                           res = res[:,:-10]\n",
    "                        if initial_sequence[1:7] == \"HHHHHH\":\n",
    "                            res = res[:,7:]\n",
    "                        if initial_sequence[2:8] == \"HHHHHH\":\n",
    "                            res = res[:,8:]\n",
    "                        if initial_sequence[3:9] == \"HHHHHH\":\n",
    "                            res = res[:,9:]\n",
    "                        if initial_sequence[4:10] == \"HHHHHH\":\n",
    "                            res = res[:,10:]\n",
    "                        if res.shape[1] < 4:\n",
    "                            pass\n",
    "                        else:\n",
    "                            my_dict['seq_chain_'+letter]= \"\".join(list(np.array(list(t['seq']))[res][0,]))\n",
    "                            concat_seq += my_dict['seq_chain_'+letter]\n",
    "                            if idx in t['masked']:\n",
    "                                mask_list.append(letter)\n",
    "                            else:\n",
    "                                visible_list.append(letter)\n",
    "                            coords_dict_chain = {}\n",
    "                            all_atoms = np.array(t['xyz'][res,])[0,] #[L, 14, 3]\n",
    "                            coords_dict_chain['N_chain_'+letter]=all_atoms[:,0,:].tolist()\n",
    "                            coords_dict_chain['CA_chain_'+letter]=all_atoms[:,1,:].tolist()\n",
    "                            coords_dict_chain['C_chain_'+letter]=all_atoms[:,2,:].tolist()\n",
    "                            coords_dict_chain['O_chain_'+letter]=all_atoms[:,3,:].tolist()\n",
    "                            \n",
    "                            p1 = np.expand_dims(all_atoms[:,1,:], axis=1)\n",
    "\n",
    "                            coords_dict_chain[\"all_atoms\"] = np.concatenate((p1, all_atoms[:,4:,:]), axis=1).tolist()\n",
    "                            \n",
    "                            my_dict['coords_chain_'+letter]=coords_dict_chain\n",
    "                    my_dict['name']= t['label']\n",
    "                    my_dict['masked_list']= mask_list\n",
    "                    my_dict['visible_list']= visible_list\n",
    "                    my_dict['num_of_chains'] = len(mask_list) + len(visible_list)\n",
    "                    my_dict['seq'] = concat_seq\n",
    "                    if len(concat_seq) <= max_length:\n",
    "                        pdb_dict_list.append(my_dict)\n",
    "                    if len(pdb_dict_list) >= num_units:\n",
    "                        break\n",
    "    return pdb_dict_list\n",
    "data_path = \"./pdb_2021aug02\"\n",
    "params = {\n",
    "        \"LIST\"    : f\"{data_path}/list.csv\", \n",
    "        \"VAL\"     : f\"{data_path}/valid_clusters.txt\",\n",
    "        \"TEST\"    : f\"{data_path}/test_clusters.txt\",\n",
    "        \"DIR\"     : f\"{data_path}\",\n",
    "        \"DATCUT\"  : \"2030-Jan-01\",\n",
    "        \"RESCUT\"  : 3.5, #resolution cutoff for PDBs\n",
    "        \"HOMO\"    : 0.70 #min seq.id. to detect homo chains\n",
    "    }\n",
    "LOAD_PARAM = {'batch_size': 1,\n",
    "                  'shuffle': True,\n",
    "                  'pin_memory':False,\n",
    "                  'num_workers': 4}\n",
    "train, _, _ = build_training_clusters(params, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d87a58-3a22-498f-9a46-67173b13a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取csv文件\n",
    "data_all = pd.read_csv('./pdb_2021aug02/list.csv')\n",
    "cluster = data_all.CLUSTER.tolist()\n",
    "chainid = data_all.CHAINID.tolist()\n",
    "lst_chain_cluster = {}\n",
    "for i in range(len(chainid)):\n",
    "    lst_chain_cluster[chainid[i]] = cluster[i]\n",
    "lst_name = []\n",
    "lst_cluster_s = []\n",
    "dict_chainid_cluster = {}\n",
    "for i in dict_x:\n",
    "    for j in dict_x[i]:\n",
    "\n",
    "        if lst_chain_cluster[x[j][\"name\"]] in dict_chainid_cluster.keys():\n",
    "            #if list(set([i,j])) not in dict_chainid_cluster[lst_chain_cluster[x_test[j][\"name\"]]]:\n",
    "            dict_chainid_cluster[lst_chain_cluster[x[j][\"name\"]]].append([i,j])\n",
    "                \n",
    "        else:\n",
    "            dict_chainid_cluster[lst_chain_cluster[x[j][\"name\"]]] = [[i,j]]\n",
    "\n",
    "            \n",
    "lst_dict_x = {}\n",
    "for i in dict_chainid_cluster:\n",
    "    lst_pair = dict_chainid_cluster[i]\n",
    "    len1 = len(lst_pair)\n",
    "    idx = random.randint(0, len1-1)\n",
    "    keys1,keys2 = lst_pair[idx]\n",
    "    if keys1 in dict_x.keys():\n",
    "        lst_dict_x[keys1] = {keys2: dict_x[keys1][keys2]}\n",
    "    else:\n",
    "        lst_dict_x[keys2] = {keys1: dict_x[keys2][keys1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808fc7d-c1c4-438c-a5df-406b7ad8db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#NoiseMix\n",
    "def random_numbers(n):\n",
    "    return random.sample(range(0, n), 4000)\n",
    "def get_sample_key(keys_lst,sample_lst):\n",
    "    lst_sample_keys = []\n",
    "    for i in sample_lst:\n",
    "        lst_sample_keys.append(keys_lst[i])\n",
    "    return lst_sample_keys\n",
    "train_keys_all = list(train.keys())\n",
    "result = random_numbers(len(train.keys()))\n",
    "train_sample = get_sample_key(train_keys_all,result)\n",
    "train_set = PDB_dataset(train_sample, loader_pdb, train, params)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
    "x_2 = get_pdbs(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef58f4-ffa2-46f5-bf39-4611975423c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class StructureLoader_train_func():\n",
    "    def __init__(self, dataset, filtered_dict, batch_size = 5000, shuffle=True,\n",
    "        collate_fn=lambda x:x, drop_last=False):\n",
    "    \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.filtered_dict = filtered_dict\n",
    "        self.size = list(filtered_dict.keys())\n",
    "        self.lengths = [len(dataset[i]['seq']) for i in self.size]\n",
    "        self.batch_size = batch_size\n",
    "        sorted_ix = np.argsort(self.lengths)\n",
    "        \n",
    "         \n",
    "\n",
    "        # Cluster into batches of similar sizes\n",
    "        clusters, batch = [], []\n",
    "        batch_max = 0\n",
    "        for ix in sorted_ix:\n",
    "            size = self.lengths[ix]\n",
    "            if size * (len(batch) + 1) <= self.batch_size:\n",
    "                batch.append(self.size[ix])\n",
    "                batch_max = size\n",
    "            else:\n",
    "                clusters.append(batch)\n",
    "                batch, batch_max = [], 0\n",
    "                batch.append(self.size[ix])\n",
    "                batch_max = size\n",
    "        if len(batch) > 0:\n",
    "            clusters.append(batch)\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clusters)\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.clusters)\n",
    "        for b_idx in self.clusters:\n",
    "            #print(b_idx)\n",
    "            batch = []\n",
    "            lst_chain = []\n",
    "            length_batch = 0\n",
    "            max_length_batch = 0\n",
    "            for idx in b_idx:\n",
    "                #print(self.filtered_dict.keys())\n",
    "                bb_idx = self.filtered_dict[idx]#.tolist()\n",
    "                bb_l = len(bb_idx)\n",
    "                bb_lst = list(bb_idx.keys())\n",
    "                bb_rand = random.randint(0, bb_l-1)\n",
    "                max_length_batch1 = max([max_length_batch,len(self.dataset[idx][\"seq\"]),len(self.dataset[bb_lst[bb_rand]][\"seq\"])])\n",
    "                if (length_batch+1)*max_length_batch1 < self.batch_size:\n",
    "                    length_batch = length_batch+1\n",
    "                    max_length_batch = max([max_length_batch,max_length_batch1])\n",
    "                    batch.append(self.dataset[idx])\n",
    "                    batch.append(self.dataset[bb_lst[bb_rand]])\n",
    "                    lst_chain.append(bb_idx[bb_lst[bb_rand]][0][0])\n",
    "                    lst_chain.append(bb_idx[bb_lst[bb_rand]][1][0])\n",
    "            #batch = [self.dataset[i] for i in b_idx]\n",
    "            yield batch,lst_chain\n",
    "\n",
    "def combine_dataset(data1,data2,dict_x_1):\n",
    "    len_data1 = len(data1)\n",
    "    len_data2 = len(data2)\n",
    "    data_all = data1+data2\n",
    "    len_all = len_data1+len_data2\n",
    "    for i in range(len_data1,len_all):\n",
    "        if len(data2[i-len_data1][\"masked_list\"]) == 1:\n",
    "            dict_x_1[i] = {i:[[data2[i-len_data1][\"masked_list\"][0]],[data2[i-len_data1][\"masked_list\"][0]]]}\n",
    "    \n",
    "    return data_all,dict_x_1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fcf7a-b6ee-4e75-87b5-494677a0b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com_x, dict_x_com= combine_dataset(x,x_2,copy.deepcopy(lst_dict_x))\n",
    "StructureLoader_train = StructureLoader_train_func(data_com_x, dict_x_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05bfbf-f03f-471f-9943-b42fbbf0830f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d44acb-1afc-49bb-9e40-15d2033d5284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138fc486-3cab-47d8-b3c2-9f35bb9ee418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import esm \n",
    "from  esm.model.esm2 import ESM2, ESM2_decoder\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "def esm_model():\n",
    "    regression_data = torch.load('./esm2_t33_650M_UR50D-contact-regression.pt')\n",
    "    model_data = torch.load('./esm2_t33_650M_UR50D.pt')\n",
    "    model_data[\"model\"].update(regression_data[\"model\"])\n",
    "    alphabet = esm.data.Alphabet.from_architecture(\"ESM-1b\")\n",
    "    model = ESM2(\n",
    "        num_layers=33,\n",
    "        embed_dim=1280,\n",
    "        attention_heads=20,\n",
    "        alphabet=alphabet,\n",
    "        token_dropout=True,\n",
    "    )\n",
    "    import re\n",
    "    def upgrade_state_dict(state_dict):\n",
    "        \"\"\"Removes prefixes 'model.encoder.sentence_encoder.' and 'model.encoder.'.\"\"\"\n",
    "        prefixes = [\"encoder.sentence_encoder.\", \"encoder.\"]\n",
    "        pattern = re.compile(\"^\" + \"|\".join(prefixes))\n",
    "        state_dict = {pattern.sub(\"\", name): param for name, param in state_dict.items()}\n",
    "        return state_dict\n",
    "\n",
    "    model_data = upgrade_state_dict(model_data[\"model\"])\n",
    "    model.load_state_dict(model_data, strict=True)\n",
    "    \n",
    "    \n",
    "    decoder = ESM2_decoder(\n",
    "        num_layers=33,\n",
    "        embed_dim=1280,\n",
    "        attention_heads=20,\n",
    "        alphabet=alphabet,\n",
    "        token_dropout=True,\n",
    "    )\n",
    "    decoder_keys = ['embed_tokens.weight','lm_head.weight', 'lm_head.bias', 'lm_head.dense.weight','lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
    "    decoder_data = {}\n",
    "    for i in decoder_keys:\n",
    "        decoder_data[i] = model_data[i]\n",
    "    \n",
    "    decoder.load_state_dict(decoder_data, strict=True)\n",
    "    return model, decoder\n",
    "\n",
    "esm_encoder, esm_decoder = esm_model()\n",
    "alphabet = esm.data.Alphabet.from_architecture(\"ESM-1b\")\n",
    "esm_encoder.to(device)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfef95-8b9c-401c-8b83-e778ab8349a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615bded-d038-40e7-90d6-435a9f5aad30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6086342c-da46-436e-9ddf-f9c2964a1fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def randomize_list(lst, ratio):\n",
    "    \n",
    "    new_lst = lst.copy()\n",
    "    for i in range(len(new_lst)):\n",
    "        if random.random() < ratio:\n",
    "            new_lst[i] = random.randint(0, 20)\n",
    "    return new_lst\n",
    "\n",
    "def set_nan(arr):\n",
    "    \n",
    "    N = arr.shape[0]\n",
    "    \n",
    "    \n",
    "    num_nan = int(0.1 * N)\n",
    "    \n",
    "    \n",
    "    indices = np.random.choice(N, num_nan, replace=False)\n",
    "    \n",
    "    arr[indices, :, :] = np.nan\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def featurize(batch,lst_chain ,device = \"cpu\",is_train = True):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    Clabel = [0,1,3,3,0,1,2,0,2,0,0,1,0,1,2,1,1,0,0,1,0]\n",
    "    B = len(batch)\n",
    "     \n",
    "    lengths = np.array([len(b['seq']) for b in batch], dtype=np.int32) #sum of chain seq lengths\n",
    "    L_max = max([len(b['seq']) for b in batch])\n",
    "    X = np.zeros([B, L_max, 4, 3])\n",
    "    residue_idx = -100*np.ones([B, L_max], dtype=np.int32) #residue idx with jumps across chains\n",
    "    chain_M = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted, 0.0 for the bits that are given\n",
    "    mask_self = np.ones([B, L_max, L_max], dtype=np.int32) #for interface loss calculation - 0.0 for self interaction, 1.0 for other\n",
    "    chain_encoding_all = np.zeros([B, L_max], dtype=np.int32) #integer encoding for chains 0, 0, 0,...0, 1, 1,..., 1, 2, 2, 2...\n",
    "    S = np.zeros([B, L_max], dtype=np.int32) #sequence AAs integers\n",
    "    S_noise = np.zeros([B, L_max], dtype=np.int32) #sequence AAs integers\n",
    "    S_s = np.zeros([B, L_max], dtype=np.int32)\n",
    "    mask_rp = np.zeros([B, L_max], dtype=np.int32)\n",
    "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
    "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
    "    chain_letters = init_alphabet + extra_alphabet\n",
    "    ids = 0\n",
    "    lst_seq_str = []\n",
    "    for i, b in enumerate(batch):\n",
    "        #print(b)\n",
    "        lst_seq_str_1 = []\n",
    "        masked_chains = b['masked_list']\n",
    "        visible_chains = b['visible_list']\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        visible_temp_dict = {}\n",
    "        masked_temp_dict = {}\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            chain_seq = b[f'seq_chain_{letter}']\n",
    "            if letter in visible_chains:\n",
    "                visible_temp_dict[letter] = chain_seq\n",
    "            elif letter in masked_chains:\n",
    "                masked_temp_dict[letter] = chain_seq\n",
    "        for km, vm in masked_temp_dict.items():\n",
    "            for kv, vv in visible_temp_dict.items():\n",
    "                if vm == vv:\n",
    "                    if kv not in masked_chains:\n",
    "                        masked_chains.append(kv)\n",
    "                    if kv in visible_chains:\n",
    "                        visible_chains.remove(kv)\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        index_of_a = all_chains.index(lst_chain[ids])\n",
    "        all_chains.insert(0, all_chains.pop(index_of_a))\n",
    "        \n",
    "        \n",
    "        #random.shuffle(all_chains) #randomly shuffle chain order\n",
    "        num_chains = b['num_of_chains']\n",
    "        mask_dict = {}\n",
    "        x_chain_list = []\n",
    "        chain_mask_list = []\n",
    "        chain_seq_list = []\n",
    "        chain_encoding_list = []\n",
    "        c = 1\n",
    "        l0 = 0\n",
    "        l1 = 0\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            if letter != lst_chain[ids]:\n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                \n",
    "                lst_seq_str_1.append(chain_seq)\n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.zeros(chain_length) #0.0 for visible chains\n",
    "                \n",
    "                \n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_length,4,3]\n",
    "                #if is_train:\n",
    "                    #x_chain = set_nan(x_chain)\n",
    "                \n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "            else: \n",
    "                \n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                \n",
    "                lst_seq_str_1.append(chain_seq)\n",
    "                \n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.ones(chain_length) #0.0 for visible chains\n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]\n",
    "                #if is_train:\n",
    "                    #x_chain = set_nan(x_chain)\n",
    "                \n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "        x = np.concatenate(x_chain_list,0) #[L, 4, 3]\n",
    "        all_sequence = \"\".join(chain_seq_list)\n",
    "        m = np.concatenate(chain_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "        chain_encoding = np.concatenate(chain_encoding_list,0)\n",
    "\n",
    "        l = len(all_sequence)\n",
    "        \n",
    "        \n",
    "        all_sequence = list(all_sequence)\n",
    "        for aas in range(len(all_sequence)):\n",
    "            if all_sequence[aas] not in alphabet:\n",
    "                all_sequence[aas] = \"X\"\n",
    "        all_sequence = \"\".join(all_sequence)\n",
    "        \n",
    "        x_pad = np.pad(x, [[0,L_max-l], [0,0], [0,0]], 'constant', constant_values=(np.nan, ))\n",
    "        X[i,:,:,:] = x_pad\n",
    "\n",
    "        m_pad = np.pad(m, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_M[i,:] = m_pad\n",
    "\n",
    "        chain_encoding_pad = np.pad(chain_encoding, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_encoding_all[i,:] = chain_encoding_pad\n",
    "\n",
    "        # Convert to labels\n",
    "        indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "        \n",
    "        S_s_s = []\n",
    "        for ids_aa in indices:\n",
    "            S_s_s.append(Clabel[ids_aa])\n",
    "        S[i, :l] = indices\n",
    "        \n",
    "        S_noise[i, :l] = randomize_list(indices, 0.1)\n",
    "        S_s[i, :l] = S_s_s\n",
    "        \n",
    "        mask_rp[i,:l] = np.ones([l], dtype=np.int32)\n",
    "        \n",
    "        lst_seq_str.append(lst_seq_str_1)\n",
    "        \n",
    "        ids+=1\n",
    "\n",
    "    isnan = np.isnan(X)\n",
    "    mask = np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n",
    "    X[isnan] = 0.\n",
    "\n",
    "        \n",
    "\n",
    "    # Conversion\n",
    "    mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
    "    mask_rp = torch.from_numpy(mask_rp).to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    residue_idx = torch.from_numpy(residue_idx).to(dtype=torch.long,device=device)\n",
    "    S = torch.from_numpy(S).to(dtype=torch.long,device=device)\n",
    "    S_noise = torch.from_numpy(S_noise).to(dtype=torch.long,device=device)\n",
    "    S_s = torch.from_numpy(S_s).to(dtype=torch.long,device=device)\n",
    "    X = torch.from_numpy(X).to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    mask_self = torch.from_numpy(mask_self).to(dtype=torch.float32, device=device)\n",
    "    chain_M = torch.from_numpy(chain_M).to(dtype=torch.float32, device=device)\n",
    "    chain_encoding_all = torch.from_numpy(chain_encoding_all).to(dtype=torch.long, device=device)\n",
    "    return X, S, mask,mask_rp, lengths, chain_M, residue_idx, mask_self, chain_encoding_all,lst_seq_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd298fa-30f0-4496-ab36-e7fac6f73314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from produalnet_main import ProDualNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193df756-9e32-446c-93e1-102126dcf54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821d5f5-57c2-494f-bb59-72d345b42893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fcde16-c087-411c-92ba-58895828e4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok----------\n"
     ]
    }
   ],
   "source": [
    "    import argparse\n",
    "    import os.path\n",
    "\n",
    "    import json, time, os, sys, glob\n",
    "    import shutil\n",
    "    import warnings\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch import optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    import queue\n",
    "    import copy\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import random\n",
    "    import os.path\n",
    "    import subprocess\n",
    "    from concurrent.futures import ProcessPoolExecutor    \n",
    "\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    PATH = \"./esm_pretain.pt\" \n",
    "    \n",
    "    model = ProDualNet(node_features=128, \n",
    "                        edge_features=128, \n",
    "                        hidden_dim=128, \n",
    "                        num_encoder_layers=4, \n",
    "                        num_decoder_layers=4, \n",
    "                        k_neighbors=32, \n",
    "                        dropout=0.1, \n",
    "                        augment_eps=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    if PATH:\n",
    "        \n",
    "        checkpoint = torch.load(PATH, map_location=device)\n",
    "        total_step = checkpoint['step'] #write total_step from the checkpoint\n",
    "        epoch = checkpoint['epoch'] #write epoch from the checkpoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        print(\"load ok----------\")\n",
    "    else:\n",
    "        total_step = 0\n",
    "        epoch = 0\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, betas=(0.9, 0.98), eps=1e-9)#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95348cf-52e4-4b83-8cd0-08aba8461ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839bae85-be09-48ab-813d-b2c8e85d15f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class StructureLoader():\n",
    "    def __init__(self, dataset, filtered_dict, batch_size=5000, shuffle=True,\n",
    "        collate_fn=lambda x:x, drop_last=False):\n",
    "    \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.filtered_dict = filtered_dict\n",
    "        self.size = list(filtered_dict.keys())\n",
    "        #print(len(self.size))\n",
    "        self.lengths = [len(dataset[i]['seq']) for i in self.size]\n",
    "        #print(len(self.lengths))\n",
    "        self.batch_size = batch_size\n",
    "        sorted_ix = np.argsort(self.lengths)\n",
    "        #print(len(sorted_ix))\n",
    "         \n",
    "\n",
    "        # Cluster into batches of similar sizes\n",
    "        clusters, batch = [], []\n",
    "        batch_max = 0\n",
    "        for ix in sorted_ix:\n",
    " \n",
    "                    batch.append(self.size[ix])\n",
    "                    clusters.append(batch)\n",
    "                    #batch_max = size\n",
    "                    batch, batch_max = [], 0\n",
    "               \n",
    "                \n",
    "                \n",
    "        if len(batch) > 0:\n",
    "            clusters.append(batch)\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clusters)\n",
    "\n",
    "    def __iter__(self):\n",
    "        #np.random.shuffle(self.clusters)\n",
    "        for b_idx in self.clusters:\n",
    "            #print(b_idx)\n",
    "            batch = []\n",
    "            lst_chain = []\n",
    "            length_batch = 0\n",
    "            max_length_batch = 0\n",
    "            for idx in b_idx:\n",
    "                #print(self.filtered_dict.keys())\n",
    "                bb_idx = self.filtered_dict[idx]#.tolist()\n",
    "                bb_l = len(bb_idx)\n",
    "                bb_lst = list(bb_idx.keys())\n",
    "                bb_rand = random.randint(0, bb_l-1)\n",
    "                bb_rand = 0\n",
    "                max_length_batch1 = max([max_length_batch,len(self.dataset[idx][\"seq\"]),len(self.dataset[bb_lst[bb_rand]][\"seq\"])])\n",
    "                if (length_batch+1)*max_length_batch1 < self.batch_size*15:\n",
    "                    length_batch = length_batch+1\n",
    "                    max_length_batch = max([max_length_batch,max_length_batch1])\n",
    "                    batch.append(self.dataset[idx])\n",
    "                    batch.append(self.dataset[bb_lst[bb_rand]])\n",
    "                    lst_chain.append(bb_idx[bb_lst[bb_rand]][0][0])\n",
    "                    lst_chain.append(bb_idx[bb_lst[bb_rand]][1][0])\n",
    "                    #if self.dataset[idx][\"name\"] == \"2xpx_A\":\n",
    "                        #print(self.dataset[idx][\"seq_chain_\"+bb_idx[bb_lst[bb_rand]][0][0]])\n",
    "                        #print(self.dataset[bb_lst[bb_rand]][\"seq_chain_\"+bb_idx[bb_lst[bb_rand]][1][0]])\n",
    "                        #print(self.dataset[bb_lst[bb_rand]][\"name\"])\n",
    "                    #if self.dataset[idx][\"seq_chain_\"+bb_idx[bb_lst[bb_rand]][0][0]] \\\n",
    "                    #!= self.dataset[bb_lst[bb_rand]][\"seq_chain_\"+bb_idx[bb_lst[bb_rand]][1][0]]:\n",
    "                        #print(11111111)\n",
    "                    #if self.dataset[idx][\"masked_list\"] != bb_idx[bb_lst[bb_rand]][0] or \\\n",
    "                    #self.dataset[bb_lst[bb_rand]][\"masked_list\"]!= bb_idx[bb_lst[bb_rand]][1]:\n",
    "                        #print(111111111111)\n",
    "                    #print(self.dataset[idx][\"masked_list\"] ,bb_idx[bb_lst[bb_rand]][0],self.dataset[bb_lst[bb_rand]][\"masked_list\"],bb_idx[bb_lst[bb_rand]][1])\n",
    "            #batch = [self.dataset[i] for i in b_idx]\n",
    "            yield batch,lst_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfdb72ea-6300-497f-bc63-47c24a1b61c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test = torch.load(\"./x_test_multi.pt\")\n",
    "data_set = \"test1\"\n",
    "if data_set == \"test1\":\n",
    "    dict_x_test = torch.load(\"./dict_x_test_30_159.pt\")\n",
    "    StructureLoader_test = StructureLoader(x_test,dict_x_test)\n",
    "elif data_set == \"test2\":\n",
    "    dict_x_test = torch.load(\"./dict_x_test_sim_50_rmsd_2.pt\")\n",
    "    StructureLoader_test = StructureLoader(x_test,dict_x_test)\n",
    "elif data_set == \"test3\":\n",
    "    StructureLoader_test = torch.load(\"./lst_diff_inter_38_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991495a9-4b6e-476f-9cd3-b44fbce508a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a30820-3fdf-4f2c-b939-af8488f8e632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def seq_esm_embed_func(lst_seq,L,esm_encoder,alphabet):\n",
    "    lst_embed = []\n",
    "    esm_encoder.eval()\n",
    "    for i in range(len(lst_seq)):\n",
    "        L1 = 0\n",
    "        lst_embed_1 = []\n",
    "        seq_lst = lst_seq[i]\n",
    "        L1 = L1+len(seq_lst[0])\n",
    "        lst_embed_1.append(torch.zeros(len(seq_lst[0]),1280))# designing sequences embedding is zeros\n",
    "        seq_l = \"\"\n",
    "        for j in range(len(seq_lst)-1):\n",
    "            seq1 = seq_lst[j+1]\n",
    "            L1 = L1+len(seq_lst[j+1])\n",
    "            seq_l = seq_l+seq1\n",
    "        S1 = alphabet.get_batch_converter()([[1,seq_l]])[-1].to(device)\n",
    "        with torch.no_grad():\n",
    "            S_embedding = esm_encoder(S1,[33])\n",
    "            S_embedding = S_embedding[\"representations\"][33][0,1:-1]\n",
    "            lst_embed_1.append(S_embedding.cpu())\n",
    "        lst_embed_1.append(torch.zeros(int(L-L1),1280))\n",
    "        #print(torch.cat(lst_embed_1,0).shape)\n",
    "        lst_embed.append(torch.cat(lst_embed_1,0))\n",
    "    \n",
    "    return torch.stack(lst_embed).to(device)\n",
    "\n",
    "def seq_esm_embed_func_re(lst_seq,L,esm_encoder,alphabet):\n",
    "    lst_embed = []\n",
    "    esm_encoder.eval()\n",
    "    #for i in range(len(lst_seq)):\n",
    "        #lst_seq[i][0] = lst_seq[i][lst_pre_re]\n",
    "    for i in range(len(lst_seq)):\n",
    "        L1 = 0\n",
    "        lst_embed_1 = []\n",
    "        seq_lst = lst_seq[i]\n",
    "        #L1 = L1+len(seq_lst[0])\n",
    "        #lst_embed_1.append(torch.zeros(len(seq_lst[0]),1280))\n",
    "        seq_l = \"\"\n",
    "        for j in range(len(seq_lst)):\n",
    "            seq1 = seq_lst[j]\n",
    "            L1 = L1+len(seq_lst[j])\n",
    "            seq_l = seq_l+seq1\n",
    "        S1 = alphabet.get_batch_converter()([[1,seq_l]])[-1].to(device)\n",
    "        with torch.no_grad():\n",
    "            S_embedding = esm_encoder(S1,[33])\n",
    "            S_embedding = S_embedding[\"representations\"][33][0,1:-1]\n",
    "            lst_embed_1.append(S_embedding.cpu())\n",
    "        lst_embed_1.append(torch.zeros(int(L-L1),1280))\n",
    "        #print(torch.cat(lst_embed_1,0).shape)\n",
    "        lst_embed.append(torch.cat(lst_embed_1,0))\n",
    "    \n",
    "    return torch.stack(lst_embed).to(device)\n",
    "\n",
    "def find_first_last_one(lst):\n",
    "    first_one = -1\n",
    "    last_one = -1\n",
    "    \n",
    "    for i, value in enumerate(lst):\n",
    "        if value == 1:\n",
    "            if first_one == -1:\n",
    "                first_one = i\n",
    "            last_one = i\n",
    "            \n",
    "    return first_one, last_one\n",
    "def indices_to_chars(indices):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    return ''.join([alphabet[int(i)] for i in indices])\n",
    "def seqs_get_signle(lst_x,lst_seq_pre):\n",
    "    length_lst = len(lst_x)\n",
    "    lst_seq_all = []\n",
    "\n",
    "    for i in range(length_lst):\n",
    "        lst_seq_1 = []\n",
    "        masked_list = lst_x[i]['masked_list']\n",
    "        masked_seq_name = \"seq_chain_\"+masked_list[0]\n",
    "        masked_seq = lst_x[i][masked_seq_name]\n",
    "        length_seq = len(masked_seq)\n",
    "        #a,b = find_first_last_one(lst_mask_chain[i])\n",
    "        seq_pre_AA = lst_seq_pre[i][:length_seq]\n",
    "        ######\n",
    "        #seq_pre_str = indices_to_chars(seq_pre_AA[a:b+1])\n",
    "        seq_pre_str = indices_to_chars(seq_pre_AA)\n",
    "        ######\n",
    "        lst_seq_1.append(seq_pre_str)\n",
    "\n",
    "\n",
    "        lst_seq_keys_1 = []\n",
    "        for keys_1 in lst_x[i]:\n",
    "            if \"seq_chain_\" in keys_1 and keys_1 != masked_seq_name:\n",
    "                lst_seq_keys_1.append(keys_1)\n",
    "\n",
    "        for keys_seq in lst_seq_keys_1:\n",
    "            lst_seq_1.append(lst_x[i][keys_seq])\n",
    "\n",
    "        lst_seq_all.append(lst_seq_1)\n",
    "\n",
    "    return lst_seq_all\n",
    "\n",
    "def seq_single_complex_cross(lst_seq_all):\n",
    "    lengths_all = len(lst_seq_all)\n",
    "    lengths_all = int(lengths_all//2)\n",
    "    ns = 0\n",
    "    lst_cross = []\n",
    "    for i in range(lengths_all):\n",
    "        lst1 = lst_seq_all[i*2]\n",
    "        lst2 = lst_seq_all[i * 2 + 1]\n",
    "\n",
    "        lst_3 = []\n",
    "        lst_4 = []\n",
    "        lst_3.append(lst2[0])\n",
    "        lst_4.append(lst1[0])\n",
    "\n",
    "        for l in lst1[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_3.append(l)\n",
    "        for l in lst2[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_4.append(l)\n",
    "        lst_cross.append(lst_3)\n",
    "        lst_cross.append(lst_4)\n",
    "\n",
    "    return lst_cross\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f8a22-bfe4-404a-b018-ad3411709077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be07e0e-a06c-47de-abe8-4643687b107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85f988-9ae2-4f43-928d-fc2a783b58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"./esm_test128/\"\n",
    "logfile = base_folder + 'log.txt'\n",
    "\n",
    "with open(logfile, 'w') as f:\n",
    "    f.write('Epoch\\tTrain\\tValidation\\n')\n",
    "logfile = base_folder + 'log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088973a-8094-4a62-b3e8-22ddc6f2185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_numbers(n):\n",
    "    return random.sample(range(0, n), 4000)\n",
    "def get_sample_key(keys_lst,sample_lst):\n",
    "    lst_sample_keys = []\n",
    "    for i in sample_lst:\n",
    "        lst_sample_keys.append(keys_lst[i])\n",
    "    return lst_sample_keys\n",
    "\n",
    "\n",
    "def combine_dataset(data1, data2, dict_x_1):\n",
    "    len_data1 = len(data1)\n",
    "    len_data2 = len(data2)\n",
    "    data_all = data1 + data2\n",
    "    len_all = len_data1 + len_data2\n",
    "    for i in range(len_data1, len_all):\n",
    "        if len(data2[i - len_data1][\"masked_list\"]) == 1:\n",
    "            dict_x_1[i] = {i: [[data2[i - len_data1][\"masked_list\"][0]], [data2[i - len_data1][\"masked_list\"][0]]]}\n",
    "\n",
    "    return data_all, dict_x_1\n",
    "\n",
    "\n",
    "\n",
    "def seq_esm_embed_func(lst_seq,L,esm_encoder,alphabet):\n",
    "    lst_embed = []\n",
    "    esm_encoder.eval()\n",
    "    for i in range(len(lst_seq)):\n",
    "        L1 = 0\n",
    "        lst_embed_1 = []\n",
    "        seq_lst = lst_seq[i]\n",
    "        L1 = L1+len(seq_lst[0])\n",
    "        lst_embed_1.append(torch.zeros(len(seq_lst[0]),1280))\n",
    "        seq_l = \"\"\n",
    "        for j in range(len(seq_lst)-1):\n",
    "            seq1 = seq_lst[j+1]\n",
    "            L1 = L1+len(seq_lst[j+1])\n",
    "            seq_l = seq_l+seq1\n",
    "        S1 = alphabet.get_batch_converter()([[1,seq_l]])[-1].cuda()\n",
    "        with torch.no_grad():\n",
    "            S_embedding = esm_encoder(S1,[33])\n",
    "            S_embedding = S_embedding[\"representations\"][33][0,1:-1]\n",
    "            lst_embed_1.append(S_embedding.cpu())\n",
    "        lst_embed_1.append(torch.zeros(int(L-L1),1280))\n",
    "        #print(torch.cat(lst_embed_1,0).shape)\n",
    "        lst_embed.append(torch.cat(lst_embed_1,0))\n",
    "    \n",
    "    return torch.stack(lst_embed).cuda()\n",
    "\n",
    "def seq_esm_embed_func_re(lst_seq,L,esm_encoder,alphabet):\n",
    "    lst_embed = []\n",
    "    esm_encoder.eval()\n",
    "    \n",
    "    for i in range(len(lst_seq)):\n",
    "        L1 = 0\n",
    "        lst_embed_1 = []\n",
    "        seq_lst = lst_seq[i]\n",
    "        \n",
    "        seq_l = \"\"\n",
    "        for j in range(len(seq_lst)):\n",
    "            seq1 = seq_lst[j]\n",
    "            L1 = L1+len(seq_lst[j])\n",
    "            seq_l = seq_l+seq1\n",
    "        S1 = alphabet.get_batch_converter()([[1,seq_l]])[-1].cuda()\n",
    "        with torch.no_grad():\n",
    "            S_embedding = esm_encoder(S1,[33])\n",
    "            S_embedding = S_embedding[\"representations\"][33][0,1:-1]\n",
    "            lst_embed_1.append(S_embedding.cpu())\n",
    "        lst_embed_1.append(torch.zeros(int(L-L1),1280))\n",
    "        #print(torch.cat(lst_embed_1,0).shape)\n",
    "        lst_embed.append(torch.cat(lst_embed_1,0))\n",
    "    \n",
    "    return torch.stack(lst_embed).cuda()\n",
    "                \n",
    "            \n",
    "\n",
    "max_acc = 0.\n",
    "total_step = 0\n",
    "ec = 0\n",
    "for epoch in range(50):\n",
    "    ec+=1\n",
    "\n",
    "    t0 = time.time()\n",
    "    # e = epoch + e\n",
    "    model.train()\n",
    "    train_sum, train_weights = 0., 0.\n",
    "    train_acc = 0.\n",
    "\n",
    "    if ec%2 ==0:\n",
    "        sample_lst_train = random_numbers(len(train.keys()))\n",
    "        train_keys_all = list(train.keys())\n",
    "        train_sample = get_sample_key(train_keys_all, sample_lst_train)\n",
    "        train_set = PDB_dataset(train_sample, loader_pdb, train, params)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
    "        x_2 = get_pdbs(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "        lst_dict_x = {}\n",
    "        for i in dict_chainid_cluster:\n",
    "            lst_pair = dict_chainid_cluster[i]\n",
    "            len1 = len(lst_pair)\n",
    "            idx = random.randint(0, len1 - 1)\n",
    "            keys1, keys2 = lst_pair[idx]\n",
    "            if keys1 in dict_x.keys():\n",
    "                lst_dict_x[keys1] = {keys2: dict_x[keys1][keys2]}\n",
    "            else:\n",
    "                lst_dict_x[keys2] = {keys1: dict_x[keys2][keys1]}\n",
    "\n",
    "        data_com_x, dict_x_com = combine_dataset(x, x_2, copy.deepcopy(lst_dict_x))\n",
    "        StructureLoader_train = StructureLoader_train_func(data_com_x, dict_x_com)\n",
    "\n",
    "\n",
    "\n",
    "    for _, batch in enumerate(StructureLoader_train):\n",
    "        model.train()\n",
    "        start_batch = time.time()\n",
    "        lst_x = []\n",
    "        X, S, mask_train, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all,S_lst = featurize(batch[0],\n",
    "                                                                                                         batch[1],\n",
    "                                                                                                         device)\n",
    "        B = S.shape[0]\n",
    "        for idf in batch[0]:\n",
    "            lst_x.append(idf)\n",
    "        elapsed_featurize = time.time() - start_batch\n",
    "        optimizer.zero_grad()\n",
    "        mask_for_loss = mask * chain_M\n",
    "        if random.random()<0.5:\n",
    "            lst_seq_pre = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                S_embed = seq_esm_embed_func(S_lst,S.shape[-1],esm_encoder,alphabet)\n",
    "\n",
    "                log_probs = model(X,S, S_embed, mask_train, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "                \n",
    "                for i in range(len(mask)):\n",
    "                    if i %2 ==0:\n",
    "                        ks=i\n",
    "                    else:\n",
    "                        ks = i-1\n",
    "                    seq1 = torch.argmax(log_probs[int(i//2)], -1).cpu() * (mask[ks] *chain_M[ks]).cpu() + \\\n",
    "                    (torch.ones_like((mask[ks] *chain_M[ks]).cpu()) - (mask[ks] *chain_M[ks]).cpu() ) *S[ks].cpu()\n",
    "                    lst_seq_pre.append(seq1)\n",
    "            lst_pre_mpnn =seqs_get_signle(lst_x,lst_seq_pre)\n",
    "                    \n",
    "            S_embed = seq_esm_embed_func_re(lst_pre_mpnn,S.shape[-1],esm_encoder,alphabet)\n",
    "                \n",
    "            log_probs = model(X,S, S_embed, mask_train, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "                \n",
    "        else:\n",
    "            S_embed = seq_esm_embed_func(S_lst,S.shape[-1],esm_encoder,alphabet)\n",
    "            log_probs = model(X,S, S_embed, mask_train, mask, chain_M, residue_idx, chain_encoding_all)\n",
    "        \n",
    "        S1 = S.reshape(B // 2, 2, -1)[:, 1]\n",
    "        mask_for_loss = mask_for_loss.reshape(B // 2, 2, -1)[:, 1]\n",
    "\n",
    "        mask = mask * (torch.ones_like(chain_M) - chain_M)\n",
    "\n",
    "        _, loss_av_smoothed = loss_smoothed(S1, log_probs, mask_for_loss)\n",
    "\n",
    "        # _, loss_av_smoothed1 = loss_smoothed(S, log_probs_all, mask)\n",
    "\n",
    "        # loss_av_total = 0.4*loss_av_smoothed   + 0.6*loss_av_smoothed1\n",
    "        loss_av_total = loss_av_smoothed\n",
    "\n",
    "        loss_av_total.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, loss_av, true_false = loss_nll(S1, log_probs, mask_for_loss)\n",
    "\n",
    "        train_sum += torch.sum(loss * mask_for_loss).cpu().data.numpy()\n",
    "        train_acc += torch.sum(true_false * mask_for_loss).cpu().data.numpy()\n",
    "        train_weights += torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "\n",
    "        total_step += 1\n",
    "        # break\n",
    "        if total_step % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                validation_sum, validation_weights = 0., 0.\n",
    "                validation_acc = 0.\n",
    "                # if True:\n",
    "                for _, batch in enumerate(StructureLoader_test):\n",
    "                    X, S, mask, mask_train, lengths, chain_M, residue_idx, mask_self, chain_encoding_all,S_lst = featurize(\n",
    "                        batch[0], batch[1], device, is_train=False)\n",
    "                    # X, S, mask,mask_train, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(data,[\"P\",\"P\"], device,is_train = False)\n",
    "                    B = S.shape[0]\n",
    "                    \n",
    "                    \n",
    "                    S_embed = seq_esm_embed_func(S_lst,S.shape[-1],esm_encoder,alphabet)\n",
    "                    \n",
    "                    log_probs = model(X,S, S_embed, mask, mask_train, chain_M, residue_idx, chain_encoding_all, is_eval=True)\n",
    "                    mask_for_loss = mask * chain_M\n",
    "                    \n",
    "                    #S1 =S\n",
    "\n",
    "                    S1 = S.reshape(B // 2, 2, -1)[:, 1]\n",
    "                    mask_for_loss = mask_for_loss.reshape(B // 2, 2, -1)[:, 1]\n",
    "\n",
    "                    loss, loss_av, true_false = loss_nll(S1, log_probs, mask_for_loss)\n",
    "\n",
    "                    validation_sum += torch.sum(loss * mask_for_loss).cpu().data.numpy()\n",
    "                    validation_acc += torch.sum(true_false * mask_for_loss).cpu().data.numpy()\n",
    "                    validation_weights += torch.sum(mask_for_loss).cpu().data.numpy()\n",
    "\n",
    "            train_loss = train_sum / train_weights\n",
    "            train_accuracy = train_acc / train_weights\n",
    "            train_perplexity = np.exp(train_loss)\n",
    "            validation_loss = validation_sum / validation_weights\n",
    "            validation_accuracy = validation_acc / validation_weights\n",
    "            validation_perplexity = np.exp(validation_loss)\n",
    "\n",
    "            train_perplexity_ = np.format_float_positional(np.float32(train_perplexity), unique=False, precision=3)\n",
    "            validation_perplexity_ = np.format_float_positional(np.float32(validation_perplexity), unique=False,\n",
    "                                                                precision=3)\n",
    "            train_accuracy_ = np.format_float_positional(np.float32(train_accuracy), unique=False, precision=3)\n",
    "            validation_accuracy_ = np.format_float_positional(np.float32(validation_accuracy), unique=False,\n",
    "                                                              precision=3)\n",
    "       \n",
    "\n",
    "            t1 = time.time()\n",
    "            dt = np.format_float_positional(np.float32(t1 - t0), unique=False, precision=1)\n",
    "            with open(logfile, 'a') as f:\n",
    "                f.write(\n",
    "                    f'epoch: {epoch + 1}, step: {total_step}, time: {dt}, train: {train_perplexity_}, valid: {validation_perplexity_}, train_acc: {train_accuracy_}, valid_acc: {validation_accuracy_}\\n')\n",
    "            print(\n",
    "                f'epoch: {epoch + 1}, step: {total_step}, time: {dt}, train: {train_perplexity_}, valid: {validation_perplexity_}, train_acc: {train_accuracy_}, valid_acc: {validation_accuracy_}')\n",
    "            if float(validation_accuracy_)>max_acc:\n",
    "                max_acc = float(validation_accuracy_)\n",
    "                checkpoint_filename = base_folder + 'model_weights/epoch{}_step{}.pt'.format(epoch + 1, total_step)\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'step': total_step,\n",
    "\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "\n",
    "                }, checkpoint_filename)\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            train_sum, train_weights = 0., 0.\n",
    "            train_acc = 0.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esm_dds)",
   "language": "python",
   "name": "esm_dds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
