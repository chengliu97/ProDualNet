{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3faca6-1171-445f-86e7-29c39c9271bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is based on ProteinMPNN/Pifold/esm, under the MIT License.\n",
    "# Source: https://github.com/dauparas/ProteinMPNN, https://github.com/A4Bio/PiFold,https://github.com/facebookresearch/esm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a111a-bc3d-4374-8096-f83e481832cc",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This is a real-world design case for SAR425899, the GLP-1R/GCGR dual agonist SAR425899, with PDB (8JIU, 8JIR). \n",
    "It utilizes the PDB processing script (from ProteinMPNN), where the design chain length must be equal. \n",
    "Currently, only dual agonist designs are supported. The design process involves setting conserved sites (e.g., 1-13) \n",
    "to complete the sequence design. \n",
    "\n",
    "Note: Since non-natural amino acids are not supported, non-natural amino acids in the design need to be specially handled, \n",
    "such as being set as conserved sites and processed as natural amino acids.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b383c6b5-eb26-43d6-b184-e9d0c3ed7bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os.path\n",
    "import json, time, os, sys, glob\n",
    "import shutil\n",
    "import warnings\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import queue\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os.path\n",
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor    \n",
    "from utils import worker_init_fn, get_pdbs, loader_pdb, build_training_clusters, PDB_dataset, StructureDataset, StructureLoader\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9f14c-a69e-4a1f-a751-093987bdaa1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "138fc486-3cab-47d8-b3c2-9f35bb9ee418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import esm \n",
    "from  esm.model.esm2 import ESM2, ESM2_decoder\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "def esm_model():\n",
    "    regression_data = torch.load('./esm2_t33_650M_UR50D-contact-regression.pt')\n",
    "    model_data = torch.load('./esm2_t33_650M_UR50D.pt')\n",
    "    model_data[\"model\"].update(regression_data[\"model\"])\n",
    "    alphabet = esm.data.Alphabet.from_architecture(\"ESM-1b\")\n",
    "    model = ESM2(\n",
    "        num_layers=33,\n",
    "        embed_dim=1280,\n",
    "        attention_heads=20,\n",
    "        alphabet=alphabet,\n",
    "        token_dropout=True,\n",
    "    )\n",
    "    import re\n",
    "    def upgrade_state_dict(state_dict):\n",
    "        \"\"\"Removes prefixes 'model.encoder.sentence_encoder.' and 'model.encoder.'.\"\"\"\n",
    "        prefixes = [\"encoder.sentence_encoder.\", \"encoder.\"]\n",
    "        pattern = re.compile(\"^\" + \"|\".join(prefixes))\n",
    "        state_dict = {pattern.sub(\"\", name): param for name, param in state_dict.items()}\n",
    "        return state_dict\n",
    "\n",
    "    model_data = upgrade_state_dict(model_data[\"model\"])\n",
    "    model.load_state_dict(model_data, strict=True)\n",
    "    \n",
    "    \n",
    "    decoder = ESM2_decoder(\n",
    "        num_layers=33,\n",
    "        embed_dim=1280,\n",
    "        attention_heads=20,\n",
    "        alphabet=alphabet,\n",
    "        token_dropout=True,\n",
    "    )\n",
    "    decoder_keys = ['embed_tokens.weight','lm_head.weight', 'lm_head.bias', 'lm_head.dense.weight','lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
    "    decoder_data = {}\n",
    "    for i in decoder_keys:\n",
    "        decoder_data[i] = model_data[i]\n",
    "    \n",
    "    decoder.load_state_dict(decoder_data, strict=True)\n",
    "    return model, decoder\n",
    "\n",
    "esm_encoder, esm_decoder = esm_model()\n",
    "alphabet = esm.data.Alphabet.from_architecture(\"ESM-1b\")\n",
    "esm_encoder.to(device)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfef95-8b9c-401c-8b83-e778ab8349a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615bded-d038-40e7-90d6-435a9f5aad30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6086342c-da46-436e-9ddf-f9c2964a1fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def randomize_list(lst, ratio):\n",
    "    \n",
    "    new_lst = lst.copy()\n",
    "    for i in range(len(new_lst)):\n",
    "        if random.random() < ratio:\n",
    "            new_lst[i] = random.randint(0, 20)\n",
    "    return new_lst\n",
    "\n",
    "def set_nan(arr):\n",
    "    \n",
    "    N = arr.shape[0]\n",
    "    \n",
    "    \n",
    "    num_nan = int(0.1 * N)\n",
    "    \n",
    "    \n",
    "    indices = np.random.choice(N, num_nan, replace=False)\n",
    "    \n",
    "    arr[indices, :, :] = np.nan\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def featurize(batch,lst_chain ,device = \"cpu\",is_train = True):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    Clabel = [0,1,3,3,0,1,2,0,2,0,0,1,0,1,2,1,1,0,0,1,0]\n",
    "    B = len(batch)\n",
    "     \n",
    "    lengths = np.array([len(b['seq']) for b in batch], dtype=np.int32) #sum of chain seq lengths\n",
    "    L_max = max([len(b['seq']) for b in batch])\n",
    "    X = np.zeros([B, L_max, 4, 3])\n",
    "    residue_idx = -100*np.ones([B, L_max], dtype=np.int32) #residue idx with jumps across chains\n",
    "    chain_M = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted, 0.0 for the bits that are given\n",
    "    mask_self = np.ones([B, L_max, L_max], dtype=np.int32) #for interface loss calculation - 0.0 for self interaction, 1.0 for other\n",
    "    chain_encoding_all = np.zeros([B, L_max], dtype=np.int32) #integer encoding for chains 0, 0, 0,...0, 1, 1,..., 1, 2, 2, 2...\n",
    "    S = np.zeros([B, L_max], dtype=np.int32) #sequence AAs integers\n",
    "    S_noise = np.zeros([B, L_max], dtype=np.int32) #sequence AAs integers\n",
    "    S_s = np.zeros([B, L_max], dtype=np.int32)\n",
    "    mask_rp = np.zeros([B, L_max], dtype=np.int32)\n",
    "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
    "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
    "    chain_letters = init_alphabet + extra_alphabet\n",
    "    ids = 0\n",
    "    lst_seq_str = []\n",
    "    for i, b in enumerate(batch):\n",
    "        #print(b)\n",
    "        lst_seq_str_1 = []\n",
    "        masked_chains = b['masked_list']\n",
    "        visible_chains = b['visible_list']\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        visible_temp_dict = {}\n",
    "        masked_temp_dict = {}\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            chain_seq = b[f'seq_chain_{letter}']\n",
    "            if letter in visible_chains:\n",
    "                visible_temp_dict[letter] = chain_seq\n",
    "            elif letter in masked_chains:\n",
    "                masked_temp_dict[letter] = chain_seq\n",
    "        for km, vm in masked_temp_dict.items():\n",
    "            for kv, vv in visible_temp_dict.items():\n",
    "                if vm == vv:\n",
    "                    if kv not in masked_chains:\n",
    "                        masked_chains.append(kv)\n",
    "                    if kv in visible_chains:\n",
    "                        visible_chains.remove(kv)\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        index_of_a = all_chains.index(lst_chain[ids])\n",
    "        all_chains.insert(0, all_chains.pop(index_of_a))\n",
    "        \n",
    "        \n",
    "        #random.shuffle(all_chains) #randomly shuffle chain order\n",
    "        num_chains = b['num_of_chains']\n",
    "        mask_dict = {}\n",
    "        x_chain_list = []\n",
    "        chain_mask_list = []\n",
    "        chain_seq_list = []\n",
    "        chain_encoding_list = []\n",
    "        c = 1\n",
    "        l0 = 0\n",
    "        l1 = 0\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            if letter != lst_chain[ids]:\n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                \n",
    "                lst_seq_str_1.append(chain_seq)\n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.zeros(chain_length) #0.0 for visible chains\n",
    "                \n",
    "                \n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_length,4,3]\n",
    "                #if is_train:\n",
    "                    #x_chain = set_nan(x_chain)\n",
    "                \n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "            else: \n",
    "                \n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                \n",
    "                lst_seq_str_1.append(chain_seq)\n",
    "                \n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.ones(chain_length) #0.0 for visible chains\n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]\n",
    "                #if is_train:\n",
    "                    #x_chain = set_nan(x_chain)\n",
    "                \n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "        x = np.concatenate(x_chain_list,0) #[L, 4, 3]\n",
    "        all_sequence = \"\".join(chain_seq_list)\n",
    "        m = np.concatenate(chain_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "        chain_encoding = np.concatenate(chain_encoding_list,0)\n",
    "\n",
    "        l = len(all_sequence)\n",
    "        \n",
    "        \n",
    "        all_sequence = list(all_sequence)\n",
    "        for aas in range(len(all_sequence)):\n",
    "            if all_sequence[aas] not in alphabet:\n",
    "                all_sequence[aas] = \"X\"\n",
    "        all_sequence = \"\".join(all_sequence)\n",
    "        \n",
    "        x_pad = np.pad(x, [[0,L_max-l], [0,0], [0,0]], 'constant', constant_values=(np.nan, ))\n",
    "        X[i,:,:,:] = x_pad\n",
    "\n",
    "        m_pad = np.pad(m, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_M[i,:] = m_pad\n",
    "\n",
    "        chain_encoding_pad = np.pad(chain_encoding, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_encoding_all[i,:] = chain_encoding_pad\n",
    "\n",
    "        # Convert to labels\n",
    "        indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "        \n",
    "        S_s_s = []\n",
    "        for ids_aa in indices:\n",
    "            S_s_s.append(Clabel[ids_aa])\n",
    "        S[i, :l] = indices\n",
    "        \n",
    "        S_noise[i, :l] = randomize_list(indices, 0.1)\n",
    "        S_s[i, :l] = S_s_s\n",
    "        \n",
    "        mask_rp[i,:l] = np.ones([l], dtype=np.int32)\n",
    "        \n",
    "        lst_seq_str.append(lst_seq_str_1)\n",
    "        \n",
    "        ids+=1\n",
    "\n",
    "    isnan = np.isnan(X)\n",
    "    mask = np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n",
    "    X[isnan] = 0.\n",
    "\n",
    "        \n",
    "\n",
    "    # Conversion\n",
    "    mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
    "    mask_rp = torch.from_numpy(mask_rp).to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    residue_idx = torch.from_numpy(residue_idx).to(dtype=torch.long,device=device)\n",
    "    S = torch.from_numpy(S).to(dtype=torch.long,device=device)\n",
    "    S_noise = torch.from_numpy(S_noise).to(dtype=torch.long,device=device)\n",
    "    S_s = torch.from_numpy(S_s).to(dtype=torch.long,device=device)\n",
    "    X = torch.from_numpy(X).to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    mask_self = torch.from_numpy(mask_self).to(dtype=torch.float32, device=device)\n",
    "    chain_M = torch.from_numpy(chain_M).to(dtype=torch.float32, device=device)\n",
    "    chain_encoding_all = torch.from_numpy(chain_encoding_all).to(dtype=torch.long, device=device)\n",
    "    return X, S, mask,mask_rp, lengths, chain_M, residue_idx, mask_self, chain_encoding_all,lst_seq_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd298fa-30f0-4496-ab36-e7fac6f73314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from produalnet_main import ProDualNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821d5f5-57c2-494f-bb59-72d345b42893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fcde16-c087-411c-92ba-58895828e4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok----------\n"
     ]
    }
   ],
   "source": [
    "    import argparse\n",
    "    import os.path\n",
    "\n",
    "    import json, time, os, sys, glob\n",
    "    import shutil\n",
    "    import warnings\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch import optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    import queue\n",
    "    import copy\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import random\n",
    "    import os.path\n",
    "    import subprocess\n",
    "    from concurrent.futures import ProcessPoolExecutor    \n",
    "\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    PATH = \"./esm_test128/model_weights/best_esm.pt\"#\"./produalnet_esm.pt\"\n",
    "    \n",
    "    model = ProDualNet(node_features=128, \n",
    "                        edge_features=128, \n",
    "                        hidden_dim=128, \n",
    "                        num_encoder_layers=4, \n",
    "                        num_decoder_layers=4, \n",
    "                        k_neighbors=32, \n",
    "                        dropout=0.1, \n",
    "                        augment_eps=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    if PATH:\n",
    "        \n",
    "        checkpoint = torch.load(PATH, map_location=device)\n",
    "        total_step = checkpoint['step'] #write total_step from the checkpoint\n",
    "        epoch = checkpoint['epoch'] #write epoch from the checkpoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        print(\"load ok----------\")\n",
    "    else:\n",
    "        total_step = 0\n",
    "        epoch = 0\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, betas=(0.9, 0.98), eps=1e-9)#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95348cf-52e4-4b83-8cd0-08aba8461ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a30820-3fdf-4f2c-b939-af8488f8e632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def seq_esm_embed_func(lst_seq,L,esm_encoder,alphabet):\n",
    "    lst_embed = []\n",
    "    esm_encoder.eval()\n",
    "    for i in range(len(lst_seq)):\n",
    "        L1 = 0\n",
    "        lst_embed_1 = []\n",
    "        seq_lst = lst_seq[i]\n",
    "        L1 = L1+len(seq_lst[0])\n",
    "        lst_embed_1.append(torch.zeros(len(seq_lst[0]),1280))# designing sequences embedding is zeros\n",
    "        seq_l = \"\"\n",
    "        for j in range(len(seq_lst)-1):\n",
    "            seq1 = seq_lst[j+1]\n",
    "            L1 = L1+len(seq_lst[j+1])\n",
    "            seq_l = seq_l+seq1\n",
    "        S1 = alphabet.get_batch_converter()([[1,seq_l]])[-1].to(device)\n",
    "        with torch.no_grad():\n",
    "            S_embedding = esm_encoder(S1,[33])\n",
    "            S_embedding = S_embedding[\"representations\"][33][0,1:-1]\n",
    "            lst_embed_1.append(S_embedding.cpu())\n",
    "        lst_embed_1.append(torch.zeros(int(L-L1),1280))\n",
    "        #print(torch.cat(lst_embed_1,0).shape)\n",
    "        lst_embed.append(torch.cat(lst_embed_1,0))\n",
    "    \n",
    "    return torch.stack(lst_embed).to(device)\n",
    "\n",
    "def seq_esm_embed_func_re(lst_seq,L,esm_encoder,alphabet):\n",
    "    lst_embed = []\n",
    "    esm_encoder.eval()\n",
    "    #for i in range(len(lst_seq)):\n",
    "        #lst_seq[i][0] = lst_seq[i][lst_pre_re]\n",
    "    for i in range(len(lst_seq)):\n",
    "        L1 = 0\n",
    "        lst_embed_1 = []\n",
    "        seq_lst = lst_seq[i]\n",
    "        #L1 = L1+len(seq_lst[0])\n",
    "        #lst_embed_1.append(torch.zeros(len(seq_lst[0]),1280))\n",
    "        seq_l = \"\"\n",
    "        for j in range(len(seq_lst)):\n",
    "            seq1 = seq_lst[j]\n",
    "            L1 = L1+len(seq_lst[j])\n",
    "            seq_l = seq_l+seq1\n",
    "        S1 = alphabet.get_batch_converter()([[1,seq_l]])[-1].to(device)\n",
    "        with torch.no_grad():\n",
    "            S_embedding = esm_encoder(S1,[33])\n",
    "            S_embedding = S_embedding[\"representations\"][33][0,1:-1]\n",
    "            lst_embed_1.append(S_embedding.cpu())\n",
    "        lst_embed_1.append(torch.zeros(int(L-L1),1280))\n",
    "        #print(torch.cat(lst_embed_1,0).shape)\n",
    "        lst_embed.append(torch.cat(lst_embed_1,0))\n",
    "    \n",
    "    return torch.stack(lst_embed).to(device)\n",
    "\n",
    "def find_first_last_one(lst):\n",
    "    first_one = -1\n",
    "    last_one = -1\n",
    "    \n",
    "    for i, value in enumerate(lst):\n",
    "        if value == 1:\n",
    "            if first_one == -1:\n",
    "                first_one = i\n",
    "            last_one = i\n",
    "            \n",
    "    return first_one, last_one\n",
    "def indices_to_chars(indices):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    return ''.join([alphabet[int(i)] for i in indices])\n",
    "def seqs_get_signle(lst_x,lst_seq_pre):\n",
    "    length_lst = len(lst_x)\n",
    "    lst_seq_all = []\n",
    "\n",
    "    for i in range(length_lst):\n",
    "        lst_seq_1 = []\n",
    "        masked_list = lst_x[i]['masked_list']\n",
    "        masked_seq_name = \"seq_chain_\"+masked_list[0]\n",
    "        masked_seq = lst_x[i][masked_seq_name]\n",
    "        length_seq = len(masked_seq)\n",
    "        #a,b = find_first_last_one(lst_mask_chain[i])\n",
    "        seq_pre_AA = lst_seq_pre[i][:length_seq]\n",
    "        ######\n",
    "        #seq_pre_str = indices_to_chars(seq_pre_AA[a:b+1])\n",
    "        seq_pre_str = indices_to_chars(seq_pre_AA)\n",
    "        ######\n",
    "        lst_seq_1.append(seq_pre_str)\n",
    "\n",
    "\n",
    "        lst_seq_keys_1 = []\n",
    "        for keys_1 in lst_x[i]:\n",
    "            if \"seq_chain_\" in keys_1 and keys_1 != masked_seq_name:\n",
    "                lst_seq_keys_1.append(keys_1)\n",
    "\n",
    "        for keys_seq in lst_seq_keys_1:\n",
    "            lst_seq_1.append(lst_x[i][keys_seq])\n",
    "\n",
    "        lst_seq_all.append(lst_seq_1)\n",
    "\n",
    "    return lst_seq_all\n",
    "\n",
    "def seq_single_complex_cross(lst_seq_all):\n",
    "    lengths_all = len(lst_seq_all)\n",
    "    lengths_all = int(lengths_all//2)\n",
    "    ns = 0\n",
    "    lst_cross = []\n",
    "    for i in range(lengths_all):\n",
    "        lst1 = lst_seq_all[i*2]\n",
    "        lst2 = lst_seq_all[i * 2 + 1]\n",
    "\n",
    "        lst_3 = []\n",
    "        lst_4 = []\n",
    "        lst_3.append(lst2[0])\n",
    "        lst_4.append(lst1[0])\n",
    "\n",
    "        for l in lst1[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_3.append(l)\n",
    "        for l in lst2[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_4.append(l)\n",
    "        lst_cross.append(lst_3)\n",
    "        lst_cross.append(lst_4)\n",
    "\n",
    "    return lst_cross\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083f968-f77c-434d-bc08-8794941eb816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3691e-934f-4c15-9502-7b8a5b48ee1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea26284a-507b-4961-a177-b5213127902f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data_dual = []\n",
    "with open('./dual_SAR.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的JSON数据\n",
    "        data1 = json.loads(line)\n",
    "        data1[\"masked_list\"] = [\"P\"]\n",
    "        data1[\"visible_list\"] = [\"R\"]\n",
    "        data1[\"name\"] = \"dual_design\"\n",
    "        data_dual.append( data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f8a22-bfe4-404a-b018-ad3411709077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49578496-3006-4fd3-adf3-0fe4668f693d",
   "metadata": {},
   "source": [
    "dual target protein design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbab4b4-e31f-4812-a01b-cb2856f17556",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dssg/home/acct-clsyzs/clsyzs/.conda/envs/esm_dds/lib/python3.7/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "def recycle_esm(seq1, mask, chain_M, S, lst_x, esm_encoder, alphabet):\n",
    "    \"\"\" Function to refine sequences using the ESM encoder \"\"\"\n",
    "    lst_seq_pre = []\n",
    "    for i in range(len(mask)):\n",
    "        ks = i if i % 2 == 0 else i - 1\n",
    "        \n",
    "        # Append sequence to the list (only the first item in seq1 is used for now)\n",
    "        lst_seq_pre.append(seq1.cpu()[0])\n",
    "                    \n",
    "    # Generate MPNN predictions using the previously obtained sequences\n",
    "    lst_pre_mpnn = seqs_get_signle(lst_x, lst_seq_pre)\n",
    "    \n",
    "    # Compute the ESM embeddings for sequences\n",
    "    S_embed = seq_esm_embed_func_re(lst_pre_mpnn, S.shape[-1], esm_encoder, alphabet)\n",
    "    \n",
    "    return S_embed\n",
    "\n",
    "\n",
    "def _scores(S, log_probs, mask):\n",
    "    \"\"\" Calculate the negative log probabilities (NLLLoss) \"\"\"\n",
    "    criterion = torch.nn.NLLLoss(reduction='none')\n",
    "    \n",
    "    # Flatten log_probs and S for loss calculation\n",
    "    loss = criterion(log_probs.contiguous().view(-1, log_probs.size(-1)),\n",
    "                     S.contiguous().view(-1)).view(S.size())\n",
    "    \n",
    "    # Calculate scores by summing the loss weighted by the mask\n",
    "    scores = torch.sum(loss * mask, dim=-1) / torch.sum(mask, dim=-1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def run_design_sequence(model, data_dual, esm_encoder, alphabet, device, temp_a=0.1, design_num=10, recycle_num=1, conserved_sites=[]):\n",
    "    \"\"\"Main validation loop for structure prediction and optimization\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    lst_x = []  # List to store input sequences\n",
    "    lst_seq_pre = []  # List to store predicted sequences\n",
    "    lst_acc = []  # List to store accuracy\n",
    "    lst_score = []  # List to store scores\n",
    "    lst_name = []  # List to store names of sequences\n",
    "    ns = 0  # Batch counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Feature extraction for the entire batch\n",
    "        X, S, mask, mask_train, lengths, chain_M, residue_idx, mask_self, chain_encoding_all, S_lst = \\\n",
    "            featurize(data_dual, [\"P\", \"P\"], device, is_train=False)\n",
    "\n",
    "        # Generate initial ESM embeddings\n",
    "        S_embed = seq_esm_embed_func(S_lst, S.shape[-1], esm_encoder, alphabet)\n",
    "\n",
    "        # Modified chain mask\n",
    "        chain_M_pos = mask.clone()\n",
    "        \n",
    "        # Apply the conservation sites (set them to 0)\n",
    "        for site in conserved_sites:\n",
    "            chain_M_pos[0][site] = 0\n",
    "            chain_M_pos[1][site] = 0\n",
    "\n",
    "        B = 2  # Batch size\n",
    "        for sdp in range(design_num):\n",
    "            lst_x_r = []\n",
    "            randn = torch.randn(chain_M.shape).to(device)  # Random noise tensor for sampling\n",
    "\n",
    "            # Generate initial sequences using model's sampling method\n",
    "            S_pre, log_probs = model.sample(\n",
    "                X, randn, S, S_embed, chain_M, chain_encoding_all, residue_idx, mask=mask,\n",
    "                temperature=temp_a, chain_M_pos=chain_M_pos, mask_train=mask_train\n",
    "            )\n",
    "\n",
    "            # Store input data and names\n",
    "            for i in data_dual:\n",
    "                lst_name.append(f\"{i['name']}_{ns}_{sdp}\")\n",
    "                lst_x.append(i)\n",
    "                lst_x_r.append(i)\n",
    "\n",
    "            # Loop for recycling and refinement\n",
    "            for _ in range(recycle_num):\n",
    "                S_embed_r = recycle_esm(S_pre, mask, chain_M, S, lst_x_r, esm_encoder, alphabet)\n",
    "                S_pre, log_probs = model.sample(\n",
    "                    X, randn, S, S_embed_r, chain_M, chain_encoding_all, residue_idx, mask=mask,\n",
    "                    temperature=temp_a, chain_M_pos=chain_M_pos, mask_train=mask_train\n",
    "                )\n",
    "\n",
    "            # Store predicted sequences\n",
    "            for ss in range(len(mask)):\n",
    "                lst_seq_pre.append(S_pre[ss].cpu())\n",
    "\n",
    "            # Calculate the scores and accuracies\n",
    "            mask_for_loss = mask * chain_M\n",
    "            S1 = S.reshape(B // 2, 2, -1)[:, 1]\n",
    "            mask_for_loss1 = mask_for_loss.reshape(B // 2, 2, -1)[:, 1]\n",
    "\n",
    "            # Design score\n",
    "            score_design = _scores(S1, log_probs[0][None, :, :], mask_for_loss1)\n",
    "            lst_score.append(score_design[0])\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc_design = torch.sum(\n",
    "                torch.sum(torch.nn.functional.one_hot(S[0], 21) * torch.nn.functional.one_hot(S_pre[0], 21), axis=-1) * mask_for_loss[0]\n",
    "            ) / torch.sum(mask_for_loss[0])\n",
    "            lst_acc.append(acc_design.cpu())\n",
    "\n",
    "        ns += 1  # Increment batch counter\n",
    "\n",
    "    # Return the final results: lst_x, lst_seq_pre, lst_acc, lst_score, lst_name\n",
    "    return lst_x, lst_seq_pre, lst_acc, lst_score, lst_name\n",
    "\n",
    "\n",
    "# Assuming these are predefined functions and variables from your environment:\n",
    "# - model: your trained model\n",
    "# - StructureLoader_test: your test data loader\n",
    "# - esm_encoder: the ESM encoder model\n",
    "# - alphabet: the sequence alphabet (e.g., amino acid alphabet)\n",
    "# - device: the computing device (CPU or GPU)\n",
    "# - temp_a: temperature for sampling, default is 0.1\n",
    "# - design_num: number of designs to generate per case\n",
    "# - recycle_num: number of recycling iterations\n",
    "# - conserved_sites: conserved_sites in design\n",
    "# Example usage:\n",
    "lst_x,lst_seq_pre, lst_acc, lst_score,lst_name = run_design_sequence(model, data_dual, esm_encoder, alphabet, device, temp_a=0.1, design_num=10, recycle_num=1,conserved_sites = list(range(13)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0d28a-40f4-4b7d-a058-a99aae234e22",
   "metadata": {},
   "source": [
    "Save design sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37eb53b2-4344-4a17-880b-3929f59622fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_fasta = \"design_sequence.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a098e70-001c-4ba0-9313-7a3885053e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_first_last_one(lst):\n",
    "    first_one = -1\n",
    "    last_one = -1\n",
    "    \n",
    "    for i, value in enumerate(lst):\n",
    "        if value == 1:\n",
    "            if first_one == -1:\n",
    "                first_one = i\n",
    "            last_one = i\n",
    "            \n",
    "    return first_one, last_one\n",
    "def indices_to_chars(indices):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    return ''.join([alphabet[int(i)] for i in indices])\n",
    "def seqs_get_signle(lst_x,lst_seq_pre):\n",
    "    length_lst = len(lst_x)\n",
    "    lst_seq_all = []\n",
    "\n",
    "    for i in range(length_lst):\n",
    "        lst_seq_1 = []\n",
    "        masked_list = lst_x[i]['masked_list']\n",
    "        masked_seq_name = \"seq_chain_\"+masked_list[0]\n",
    "        masked_seq = lst_x[i][masked_seq_name]\n",
    "        length_seq = len(masked_seq)\n",
    "        #a,b = find_first_last_one(lst_mask_chain[i])\n",
    "        seq_pre_AA = lst_seq_pre[i][:length_seq]\n",
    "        ######\n",
    "        #seq_pre_str = indices_to_chars(seq_pre_AA[a:b+1])\n",
    "        seq_pre_str = indices_to_chars(seq_pre_AA)\n",
    "        ######\n",
    "        lst_seq_1.append(seq_pre_str)\n",
    "\n",
    "\n",
    "        lst_seq_keys_1 = []\n",
    "        for keys_1 in lst_x[i]:\n",
    "            if \"seq_chain_\" in keys_1 and keys_1 != masked_seq_name:\n",
    "                lst_seq_keys_1.append(keys_1)\n",
    "\n",
    "        for keys_seq in lst_seq_keys_1:\n",
    "            lst_seq_1.append(lst_x[i][keys_seq])\n",
    "\n",
    "        lst_seq_all.append(lst_seq_1)\n",
    "\n",
    "    return lst_seq_all\n",
    "\n",
    "def seq_single_complex_cross(lst_seq_all):\n",
    "    lengths_all = len(lst_seq_all)\n",
    "    lengths_all = int(lengths_all//2)\n",
    "    ns = 0\n",
    "    lst_cross = []\n",
    "    for i in range(lengths_all):\n",
    "        lst1 = lst_seq_all[i*2]\n",
    "        lst2 = lst_seq_all[i * 2 + 1]\n",
    "\n",
    "        lst_3 = []\n",
    "        lst_4 = []\n",
    "        lst_3.append(lst2[0])\n",
    "        lst_4.append(lst1[0])\n",
    "\n",
    "        for l in lst1[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_3.append(l)\n",
    "        for l in lst2[1:]:\n",
    "            if \"X\" in l:\n",
    "                print(\"a\")\n",
    "            lst_4.append(l)\n",
    "        lst_cross.append(lst_3)\n",
    "        lst_cross.append(lst_4)\n",
    "\n",
    "    return lst_cross\n",
    "\n",
    "#def get_fasta_lst(lst):\n",
    "lst_pre_mpnn =seqs_get_signle(lst_x,lst_seq_pre)\n",
    "with open(path_fasta, \"w\") as file:\n",
    "     \n",
    "    for i in range(len(lst_name)):\n",
    "        \n",
    "        file.write(f\">{lst_name[i]} \\n\")\n",
    "        seqs = \"\"\n",
    "        for s in lst_pre_mpnn[i]:\n",
    "            \n",
    "            all_sequence = list(s)\n",
    "            for aas in range(len(all_sequence)):\n",
    "                if all_sequence[aas] == \"X\":\n",
    "                    all_sequence[aas] = \"A\"\n",
    "            all_sequence = \"\".join(all_sequence)\n",
    "            \n",
    "            seqs = seqs+all_sequence+\":\"\n",
    "        file.write(seqs[:-1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be07e0e-a06c-47de-abe8-4643687b107f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014d00c-bd43-4d38-bb57-790c1318a361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esm_dds)",
   "language": "python",
   "name": "esm_dds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
