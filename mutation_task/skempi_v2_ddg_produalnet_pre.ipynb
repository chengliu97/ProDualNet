{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1696926-3200-47c1-af1b-60abf1761162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv(\"./skempi_v2.csv\", sep=';', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e622f1e-dc07-44a1-81a5-ef01559f3bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio.PDB import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from Bio import BiopythonWarning\n",
    "warnings.simplefilter('ignore', BiopythonWarning)\n",
    "import csv\n",
    "import numpy as np\n",
    "#from Bio.PDB.Polypeptide import three_to_one, one_to_three\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cec705e-06dc-4b8e-b15e-7fa39bcfb33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pd(fName):\n",
    "    df = pd.read_csv(fName, sep = ';')\n",
    "    df = df.dropna(subset = ['Affinity_mut_parsed', 'Affinity_wt_parsed', 'Temperature'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ede3fc-b838-44cf-8805-2082ec91354b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skepiIn = \"./mut_complex_ddg/skempi_v2.csv\"\n",
    "ppi = load_pd(skepiIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64eb6b29-107e-4d52-a11d-2bd806ca9da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Pdb</th>\n",
       "      <th>Mutation(s)_PDB</th>\n",
       "      <th>Mutation(s)_cleaned</th>\n",
       "      <th>iMutation_Location(s)</th>\n",
       "      <th>Hold_out_type</th>\n",
       "      <th>Hold_out_proteins</th>\n",
       "      <th>Affinity_mut (M)</th>\n",
       "      <th>Affinity_mut_parsed</th>\n",
       "      <th>Affinity_wt (M)</th>\n",
       "      <th>Affinity_wt_parsed</th>\n",
       "      <th>...</th>\n",
       "      <th>koff_mut_parsed</th>\n",
       "      <th>koff_wt (s^(-1))</th>\n",
       "      <th>koff_wt_parsed</th>\n",
       "      <th>dH_mut (kcal mol^(-1))</th>\n",
       "      <th>dH_wt (kcal mol^(-1))</th>\n",
       "      <th>dS_mut (cal mol^(-1) K^(-1))</th>\n",
       "      <th>dS_wt (cal mol^(-1) K^(-1))</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Method</th>\n",
       "      <th>SKEMPI version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1CSE_E_I</td>\n",
       "      <td>LI45G</td>\n",
       "      <td>LI38G</td>\n",
       "      <td>COR</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>5.26E-11</td>\n",
       "      <td>5.260000e-11</td>\n",
       "      <td>1.12E-12</td>\n",
       "      <td>1.120000e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IASP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1CSE_E_I</td>\n",
       "      <td>LI45S</td>\n",
       "      <td>LI38S</td>\n",
       "      <td>COR</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>8.33E-12</td>\n",
       "      <td>8.330000e-12</td>\n",
       "      <td>1.12E-12</td>\n",
       "      <td>1.120000e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IASP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1CSE_E_I</td>\n",
       "      <td>LI45P</td>\n",
       "      <td>LI38P</td>\n",
       "      <td>COR</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>1.02E-07</td>\n",
       "      <td>1.020000e-07</td>\n",
       "      <td>1.12E-12</td>\n",
       "      <td>1.120000e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IASP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1CSE_E_I</td>\n",
       "      <td>LI45I</td>\n",
       "      <td>LI38I</td>\n",
       "      <td>COR</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>1.72E-10</td>\n",
       "      <td>1.720000e-10</td>\n",
       "      <td>1.12E-12</td>\n",
       "      <td>1.120000e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IASP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1CSE_E_I</td>\n",
       "      <td>LI45D</td>\n",
       "      <td>LI38D</td>\n",
       "      <td>COR</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>Pr/PI</td>\n",
       "      <td>1.92E-09</td>\n",
       "      <td>1.920000e-09</td>\n",
       "      <td>1.12E-12</td>\n",
       "      <td>1.120000e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IASP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7080</th>\n",
       "      <td>3QIB_ABP_CD</td>\n",
       "      <td>KP9R</td>\n",
       "      <td>KP8R</td>\n",
       "      <td>COR</td>\n",
       "      <td>TCR/pMHC</td>\n",
       "      <td>TCR/pMHC,1JCK_A_B</td>\n",
       "      <td>2.4E-04</td>\n",
       "      <td>2.400000e-04</td>\n",
       "      <td>5.5E-06</td>\n",
       "      <td>5.500000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.2E-02</td>\n",
       "      <td>0.022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7081</th>\n",
       "      <td>3QIB_ABP_CD</td>\n",
       "      <td>TP12A</td>\n",
       "      <td>TP11A</td>\n",
       "      <td>COR</td>\n",
       "      <td>TCR/pMHC</td>\n",
       "      <td>TCR/pMHC,1JCK_A_B</td>\n",
       "      <td>&gt;1.1E-03</td>\n",
       "      <td>1.100000e-03</td>\n",
       "      <td>5.5E-06</td>\n",
       "      <td>5.500000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7082</th>\n",
       "      <td>3QIB_ABP_CD</td>\n",
       "      <td>TP12S</td>\n",
       "      <td>TP11S</td>\n",
       "      <td>COR</td>\n",
       "      <td>TCR/pMHC</td>\n",
       "      <td>TCR/pMHC,1JCK_A_B</td>\n",
       "      <td>3.38E-05</td>\n",
       "      <td>3.380000e-05</td>\n",
       "      <td>5.5E-06</td>\n",
       "      <td>5.500000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.2E-02</td>\n",
       "      <td>0.022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7083</th>\n",
       "      <td>3QIB_ABP_CD</td>\n",
       "      <td>TP12N</td>\n",
       "      <td>TP11N</td>\n",
       "      <td>COR</td>\n",
       "      <td>TCR/pMHC</td>\n",
       "      <td>TCR/pMHC,1JCK_A_B</td>\n",
       "      <td>4.34E-05</td>\n",
       "      <td>4.340000e-05</td>\n",
       "      <td>5.5E-06</td>\n",
       "      <td>5.500000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175</td>\n",
       "      <td>2.2E-02</td>\n",
       "      <td>0.022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7084</th>\n",
       "      <td>3QIB_ABP_CD</td>\n",
       "      <td>YP7F,TP12S</td>\n",
       "      <td>YP6F,TP11S</td>\n",
       "      <td>COR,COR</td>\n",
       "      <td>TCR/pMHC</td>\n",
       "      <td>TCR/pMHC,1JCK_A_B</td>\n",
       "      <td>4.29E-05</td>\n",
       "      <td>4.290000e-05</td>\n",
       "      <td>5.5E-06</td>\n",
       "      <td>5.500000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180</td>\n",
       "      <td>2.2E-02</td>\n",
       "      <td>0.022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6794 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             #Pdb Mutation(s)_PDB Mutation(s)_cleaned iMutation_Location(s)  \\\n",
       "0        1CSE_E_I           LI45G               LI38G                   COR   \n",
       "1        1CSE_E_I           LI45S               LI38S                   COR   \n",
       "2        1CSE_E_I           LI45P               LI38P                   COR   \n",
       "3        1CSE_E_I           LI45I               LI38I                   COR   \n",
       "4        1CSE_E_I           LI45D               LI38D                   COR   \n",
       "...           ...             ...                 ...                   ...   \n",
       "7080  3QIB_ABP_CD            KP9R                KP8R                   COR   \n",
       "7081  3QIB_ABP_CD           TP12A               TP11A                   COR   \n",
       "7082  3QIB_ABP_CD           TP12S               TP11S                   COR   \n",
       "7083  3QIB_ABP_CD           TP12N               TP11N                   COR   \n",
       "7084  3QIB_ABP_CD      YP7F,TP12S          YP6F,TP11S               COR,COR   \n",
       "\n",
       "     Hold_out_type  Hold_out_proteins Affinity_mut (M)  Affinity_mut_parsed  \\\n",
       "0            Pr/PI              Pr/PI         5.26E-11         5.260000e-11   \n",
       "1            Pr/PI              Pr/PI         8.33E-12         8.330000e-12   \n",
       "2            Pr/PI              Pr/PI         1.02E-07         1.020000e-07   \n",
       "3            Pr/PI              Pr/PI         1.72E-10         1.720000e-10   \n",
       "4            Pr/PI              Pr/PI         1.92E-09         1.920000e-09   \n",
       "...            ...                ...              ...                  ...   \n",
       "7080      TCR/pMHC  TCR/pMHC,1JCK_A_B          2.4E-04         2.400000e-04   \n",
       "7081      TCR/pMHC  TCR/pMHC,1JCK_A_B         >1.1E-03         1.100000e-03   \n",
       "7082      TCR/pMHC  TCR/pMHC,1JCK_A_B         3.38E-05         3.380000e-05   \n",
       "7083      TCR/pMHC  TCR/pMHC,1JCK_A_B         4.34E-05         4.340000e-05   \n",
       "7084      TCR/pMHC  TCR/pMHC,1JCK_A_B         4.29E-05         4.290000e-05   \n",
       "\n",
       "     Affinity_wt (M)  Affinity_wt_parsed  ... koff_mut_parsed  \\\n",
       "0           1.12E-12        1.120000e-12  ...             NaN   \n",
       "1           1.12E-12        1.120000e-12  ...             NaN   \n",
       "2           1.12E-12        1.120000e-12  ...             NaN   \n",
       "3           1.12E-12        1.120000e-12  ...             NaN   \n",
       "4           1.12E-12        1.120000e-12  ...             NaN   \n",
       "...              ...                 ...  ...             ...   \n",
       "7080         5.5E-06        5.500000e-06  ...           0.500   \n",
       "7081         5.5E-06        5.500000e-06  ...             NaN   \n",
       "7082         5.5E-06        5.500000e-06  ...           0.134   \n",
       "7083         5.5E-06        5.500000e-06  ...           0.175   \n",
       "7084         5.5E-06        5.500000e-06  ...           0.180   \n",
       "\n",
       "     koff_wt (s^(-1)) koff_wt_parsed dH_mut (kcal mol^(-1))  \\\n",
       "0                 NaN            NaN                    NaN   \n",
       "1                 NaN            NaN                    NaN   \n",
       "2                 NaN            NaN                    NaN   \n",
       "3                 NaN            NaN                    NaN   \n",
       "4                 NaN            NaN                    NaN   \n",
       "...               ...            ...                    ...   \n",
       "7080          2.2E-02          0.022                    NaN   \n",
       "7081              NaN            NaN                    NaN   \n",
       "7082          2.2E-02          0.022                    NaN   \n",
       "7083          2.2E-02          0.022                    NaN   \n",
       "7084          2.2E-02          0.022                    NaN   \n",
       "\n",
       "      dH_wt (kcal mol^(-1))  dS_mut (cal mol^(-1) K^(-1))  \\\n",
       "0                       NaN                           NaN   \n",
       "1                       NaN                           NaN   \n",
       "2                       NaN                           NaN   \n",
       "3                       NaN                           NaN   \n",
       "4                       NaN                           NaN   \n",
       "...                     ...                           ...   \n",
       "7080                    NaN                           NaN   \n",
       "7081                    NaN                           NaN   \n",
       "7082                    NaN                           NaN   \n",
       "7083                    NaN                           NaN   \n",
       "7084                    NaN                           NaN   \n",
       "\n",
       "      dS_wt (cal mol^(-1) K^(-1))  Notes Method  SKEMPI version  \n",
       "0                             NaN    NaN   IASP               1  \n",
       "1                             NaN    NaN   IASP               1  \n",
       "2                             NaN    NaN   IASP               1  \n",
       "3                             NaN    NaN   IASP               1  \n",
       "4                             NaN    NaN   IASP               1  \n",
       "...                           ...    ...    ...             ...  \n",
       "7080                          NaN    NaN    SPR               2  \n",
       "7081                          NaN    NaN    SPR               2  \n",
       "7082                          NaN    NaN    SPR               2  \n",
       "7083                          NaN    NaN    SPR               2  \n",
       "7084                          NaN    NaN    SPR               2  \n",
       "\n",
       "[6794 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3e6909-2cce-49ea-b9fe-7fbe619b77ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdb_class = ppi[\"#Pdb\"].value_counts()\n",
    "pdb_id = pdb_class.keys().tolist()\n",
    "counts = pdb_class.tolist()\n",
    "nt = 0\n",
    "lst_id = []\n",
    "for i in range(len(counts)):\n",
    "    if counts[i]>100:\n",
    "        #lst_id.append([pdb_id[i],counts[i]])\n",
    "        lst_id.append([pdb_id[i],counts[i]])\n",
    "        nt+= counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "207442a6-b3b3-4a05-912b-2a62bf04f160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21533d90-647b-4cdf-acc1-dd30a663d05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped = ppi.groupby(ppi.columns[0])\n",
    "\n",
    "# 将分组后的数据存储到一个字典中\n",
    "grouped_dict = {}\n",
    "for name, group in grouped:\n",
    "    grouped_dict[name] = group\n",
    "def split_string(s):\n",
    "    if \",\" in s:\n",
    "        return s.split(\",\")\n",
    "    else:\n",
    "        return [s]\n",
    "def get_mut_ddg(df):\n",
    "    mut_lst = df[\"Mutation(s)_cleaned\"].tolist()\n",
    "    for i in range(len(mut_lst)):\n",
    "        mut_lst[i] = split_string(mut_lst[i])\n",
    "    Affinity_mt = []\n",
    "    Affinity_wt = []\n",
    "    for i in df['Affinity_mut_parsed']:\n",
    "        Affinity_mt.append(float(i))\n",
    "    for i in df['Affinity_wt_parsed']:\n",
    "        Affinity_wt.append(float(i))\n",
    "        \n",
    "    ddg_list = [((8.314/4184)*(273.15 + 25.0)*np.log(j)) - ((8.314/4184)*(273.15 + 25.0)*np.log(i)) for i,j in zip(Affinity_wt, Affinity_mt)]\n",
    "    \n",
    "    return mut_lst,ddg_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd55ece-d213-4539-a2db-28d7e48ef4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_string(s):\n",
    "    if \",\" in s:\n",
    "        return s.split(\",\")\n",
    "    else:\n",
    "        return [s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8f9d8e-0ab3-4c71-ad86-d094ea6d5538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dssg/home/acct-clsyzs/clsyzs/.conda/envs/esm_dds/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, time, os, sys, glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split, Subset\n",
    "import torch.utils\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "def featurize(batch, device):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    B = len(batch)\n",
    "     \n",
    "    lengths = np.array([len(b['seq']) for b in batch], dtype=np.int32) #sum of chain seq lengths\n",
    "    L_max = max([len(b['seq']) for b in batch])\n",
    "    X = np.zeros([B, L_max, 4, 3])\n",
    "    residue_idx = -100*np.ones([B, L_max], dtype=np.int32) #residue idx with jumps across chains\n",
    "    chain_M = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted, 0.0 for the bits that are given\n",
    "    mask_self = np.ones([B, L_max, L_max], dtype=np.int32) #for interface loss calculation - 0.0 for self interaction, 1.0 for other\n",
    "    chain_encoding_all = np.zeros([B, L_max], dtype=np.int32) #integer encoding for chains 0, 0, 0,...0, 1, 1,..., 1, 2, 2, 2...\n",
    "    S = np.zeros([B, L_max], dtype=np.int32) #sequence AAs integers\n",
    "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
    "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
    "    chain_letters = init_alphabet + extra_alphabet\n",
    "    for i, b in enumerate(batch):\n",
    "        masked_chains = b['masked_list']\n",
    "        visible_chains = b['visible_list']\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        visible_temp_dict = {}\n",
    "        masked_temp_dict = {}\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            chain_seq = b[f'seq_chain_{letter}']\n",
    "            if letter in visible_chains:\n",
    "                visible_temp_dict[letter] = chain_seq\n",
    "            elif letter in masked_chains:\n",
    "                masked_temp_dict[letter] = chain_seq\n",
    "        for km, vm in masked_temp_dict.items():\n",
    "            for kv, vv in visible_temp_dict.items():\n",
    "                if vm == vv:\n",
    "                    if kv not in masked_chains:\n",
    "                        masked_chains.append(kv)\n",
    "                    if kv in visible_chains:\n",
    "                        visible_chains.remove(kv)\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        random.shuffle(all_chains) #randomly shuffle chain order\n",
    "        num_chains = b['num_of_chains']\n",
    "        mask_dict = {}\n",
    "        x_chain_list = []\n",
    "        chain_mask_list = []\n",
    "        chain_seq_list = []\n",
    "        chain_encoding_list = []\n",
    "        c = 1\n",
    "        l0 = 0\n",
    "        l1 = 0\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            if letter in visible_chains:\n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.zeros(chain_length) #0.0 for visible chains\n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_length,4,3]\n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "            elif letter in masked_chains: \n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.ones(chain_length) #0.0 for visible chains\n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]\n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "        x = np.concatenate(x_chain_list,0) #[L, 4, 3]\n",
    "        all_sequence = \"\".join(chain_seq_list)\n",
    "        m = np.concatenate(chain_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "        chain_encoding = np.concatenate(chain_encoding_list,0)\n",
    "\n",
    "        l = len(all_sequence)\n",
    "        x_pad = np.pad(x, [[0,L_max-l], [0,0], [0,0]], 'constant', constant_values=(np.nan, ))\n",
    "        X[i,:,:,:] = x_pad\n",
    "\n",
    "        m_pad = np.pad(m, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_M[i,:] = m_pad\n",
    "\n",
    "        chain_encoding_pad = np.pad(chain_encoding, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_encoding_all[i,:] = chain_encoding_pad\n",
    "\n",
    "        # Convert to labels\n",
    "        all_sequence = list(all_sequence)\n",
    "        for aas in range(len(all_sequence)):\n",
    "            if all_sequence[aas] not in alphabet:\n",
    "                all_sequence[aas] = \"X\"\n",
    "        all_sequence = \"\".join(all_sequence)\n",
    "        indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "        S[i, :l] = indices\n",
    "\n",
    "    isnan = np.isnan(X)\n",
    "    mask = np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n",
    "    X[isnan] = 0.\n",
    "\n",
    "    # Conversion\n",
    "    residue_idx = torch.from_numpy(residue_idx).to(dtype=torch.long,device=device)\n",
    "    S = torch.from_numpy(S).to(dtype=torch.long,device=device)\n",
    "    X = torch.from_numpy(X).to(dtype=torch.float32, device=device)\n",
    "    mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
    "    mask_self = torch.from_numpy(mask_self).to(dtype=torch.float32, device=device)\n",
    "    chain_M = torch.from_numpy(chain_M).to(dtype=torch.float32, device=device)\n",
    "    chain_encoding_all = torch.from_numpy(chain_encoding_all).to(dtype=torch.long, device=device)\n",
    "    return X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51be4a-5cad-481a-b3a7-cd3236c26641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "221ca984-88f8-436f-be5e-38951a552546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json, time, os, sys, glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split, Subset\n",
    "import torch.utils\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import itertools\n",
    "from transformer import TransformerBlock\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, padding_idx, learned=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.register_buffer(\"_float_tensor\", torch.FloatTensor(1))\n",
    "        self.weights = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz, seq_len = x.shape\n",
    "        max_pos = self.padding_idx + 1 + seq_len\n",
    "        if self.weights is None or max_pos > self.weights.size(0):\n",
    "            self.weights = self.get_embedding(max_pos)\n",
    "        self.weights = self.weights.type_as(self._float_tensor)\n",
    "\n",
    "        positions = self.make_positions(x)\n",
    "        return self.weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()\n",
    "\n",
    "    def make_positions(self, x):\n",
    "        mask = x.ne(self.padding_idx)\n",
    "        range_buf = torch.arange(x.size(1), device=x.device).expand_as(x) + self.padding_idx + 1\n",
    "        positions = range_buf.expand_as(x)\n",
    "        return positions * mask.long() + self.padding_idx * (1 - mask.long())\n",
    "\n",
    "    def get_embedding(self, num_embeddings):\n",
    "        half_dim = self.embed_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
    "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
    "        if self.embed_dim % 2 == 1:\n",
    "            # zero pad\n",
    "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
    "        if self.padding_idx is not None:\n",
    "            emb[self.padding_idx, :] = 0\n",
    "        return emb\n",
    "\n",
    "# Thanks for StructTrans\n",
    "# https://github.com/jingraham/neurips19-graph-protein-design\n",
    "def nan_to_num(tensor, nan=0.0):\n",
    "    idx = torch.isnan(tensor)\n",
    "    tensor[idx] = nan\n",
    "    return tensor\n",
    "\n",
    "def _normalize(tensor, dim=-1):\n",
    "    return nan_to_num(\n",
    "        torch.div(tensor, torch.norm(tensor, dim=dim, keepdim=True)))\n",
    "\n",
    "def cal_dihedral(X, eps=1e-7):\n",
    "    dX = X[:,1:,:] - X[:,:-1,:] # CA-N, C-CA, N-C, CA-N...\n",
    "    U = _normalize(dX, dim=-1)\n",
    "    u_0 = U[:,:-2,:] # CA-N, C-CA, N-C,...\n",
    "    u_1 = U[:,1:-1,:] # C-CA, N-C, CA-N, ... 0, psi_{i}, omega_{i}, phi_{i+1} or 0, tau_{i},...\n",
    "    u_2 = U[:,2:,:] # N-C, CA-N, C-CA, ...\n",
    "\n",
    "    n_0 = _normalize(torch.cross(u_0, u_1), dim=-1)\n",
    "    n_1 = _normalize(torch.cross(u_1, u_2), dim=-1)\n",
    "    \n",
    "    cosD = (n_0 * n_1).sum(-1)\n",
    "    cosD = torch.clamp(cosD, -1+eps, 1-eps)\n",
    "    \n",
    "    v = _normalize(torch.cross(n_0, n_1), dim=-1)\n",
    "    D = torch.sign((-v* u_1).sum(-1)) * torch.acos(cosD) # TODO: sign\n",
    "    \n",
    "    return D\n",
    "\n",
    "\n",
    "def _dihedrals(X, dihedral_type=0, eps=1e-7):\n",
    "    B, N, _, _ = X.shape\n",
    "    # psi, omega, phi\n",
    "    X = X[:,:,:3,:].reshape(X.shape[0], 3*X.shape[1], 3) # ['N', 'CA', 'C', 'O']\n",
    "    D = cal_dihedral(X)\n",
    "    D = F.pad(D, (1,2), 'constant', 0)\n",
    "    D = D.view((D.size(0), int(D.size(1)/3), 3)) \n",
    "    Dihedral_Angle_features = torch.cat((torch.cos(D), torch.sin(D)), 2)\n",
    "\n",
    "    # alpha, beta, gamma\n",
    "    dX = X[:,1:,:] - X[:,:-1,:] # CA-N, C-CA, N-C, CA-N...\n",
    "    U = _normalize(dX, dim=-1)\n",
    "    u_0 = U[:,:-2,:] # CA-N, C-CA, N-C,...\n",
    "    u_1 = U[:,1:-1,:] # C-CA, N-C, CA-N, ...\n",
    "    cosD = (u_0*u_1).sum(-1) # alpha_{i}, gamma_{i}, beta_{i+1}\n",
    "    cosD = torch.clamp(cosD, -1+eps, 1-eps)\n",
    "    D = torch.acos(cosD)\n",
    "    D = F.pad(D, (1,2), 'constant', 0)\n",
    "    D = D.view((D.size(0), int(D.size(1)/3), 3))\n",
    "    Angle_features = torch.cat((torch.cos(D), torch.sin(D)), 2)\n",
    "\n",
    "    D_features = torch.cat((Dihedral_Angle_features, Angle_features), 2)\n",
    "    return D_features\n",
    "\n",
    "def _hbonds(X, E_idx, mask_neighbors, eps=1E-3):\n",
    "    X_atoms = dict(zip(['N', 'CA', 'C', 'O'], torch.unbind(X, 2)))\n",
    "\n",
    "    X_atoms['C_prev'] = F.pad(X_atoms['C'][:,1:,:], (0,0,0,1), 'constant', 0)\n",
    "    X_atoms['H'] = X_atoms['N'] + _normalize(\n",
    "            _normalize(X_atoms['N'] - X_atoms['C_prev'], -1)\n",
    "        +  _normalize(X_atoms['N'] - X_atoms['CA'], -1)\n",
    "    , -1)\n",
    "\n",
    "    def _distance(X_a, X_b):\n",
    "        return torch.norm(X_a[:,None,:,:] - X_b[:,:,None,:], dim=-1)\n",
    "\n",
    "    def _inv_distance(X_a, X_b):\n",
    "        return 1. / (_distance(X_a, X_b) + eps)\n",
    "\n",
    "    U = (0.084 * 332) * (\n",
    "            _inv_distance(X_atoms['O'], X_atoms['N'])\n",
    "        + _inv_distance(X_atoms['C'], X_atoms['H'])\n",
    "        - _inv_distance(X_atoms['O'], X_atoms['H'])\n",
    "        - _inv_distance(X_atoms['C'], X_atoms['N'])\n",
    "    )\n",
    "\n",
    "    HB = (U < -0.5).type(torch.float32)\n",
    "    neighbor_HB = mask_neighbors * gather_edges_d(HB.unsqueeze(-1),  E_idx)\n",
    "    return neighbor_HB\n",
    "\n",
    "def _rbf(D, num_rbf):\n",
    "    D_min, D_max, D_count = 0., 20., num_rbf\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count).to(D.device)\n",
    "    D_mu = D_mu.view([1,1,1,-1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "    RBF = torch.exp(-((D_expand - D_mu) / D_sigma)**2)\n",
    "    return RBF\n",
    "\n",
    "def _get_rbf(A, B, E_idx=None, num_rbf=16):\n",
    "    if E_idx is not None:\n",
    "        D_A_B = torch.sqrt(torch.sum((A[:,:,None,:] - B[:,None,:,:])**2,-1) + 1e-6) #[B, L, L]\n",
    "        D_A_B_neighbors = gather_edges_d(D_A_B[:,:,:,None], E_idx)[:,:,:,0] #[B,L,K]\n",
    "        RBF_A_B = _rbf(D_A_B_neighbors, num_rbf)\n",
    "    else:\n",
    "        D_A_B = torch.sqrt(torch.sum((A[:,:,None,:] - B[:,:,None,:])**2,-1) + 1e-6) #[B, L, L]\n",
    "        RBF_A_B = _rbf(D_A_B, num_rbf)\n",
    "    return RBF_A_B\n",
    "\n",
    "def _orientations_coarse_gl(X, E_idx, eps=1e-6):\n",
    "    X = X[:,:,:3,:].reshape(X.shape[0], 3*X.shape[1], 3) \n",
    "    dX = X[:,1:,:] - X[:,:-1,:] # CA-N, C-CA, N-C, CA-N...\n",
    "    U = _normalize(dX, dim=-1)\n",
    "    u_0, u_1 = U[:,:-2,:], U[:,1:-1,:]\n",
    "    n_0 = _normalize(torch.cross(u_0, u_1), dim=-1)\n",
    "    b_1 = _normalize(u_0 - u_1, dim=-1)\n",
    "    \n",
    "    n_0 = n_0[:,::3,:]\n",
    "    b_1 = b_1[:,::3,:]\n",
    "    X = X[:,::3,:]\n",
    "\n",
    "    O = torch.stack((b_1, n_0, torch.cross(b_1, n_0)), 2)\n",
    "    O = O.view(list(O.shape[:2]) + [9])\n",
    "    O = F.pad(O, (0,0,0,1), 'constant', 0) # [16, 464, 9]\n",
    "\n",
    "    O_neighbors = gather_nodes_d(O, E_idx) # [16, 464, 30, 9]\n",
    "    X_neighbors = gather_nodes_d(X, E_idx) # [16, 464, 30, 3]\n",
    "\n",
    "    O = O.view(list(O.shape[:2]) + [3,3]).unsqueeze(2) # [16, 464, 1, 3, 3]\n",
    "    O_neighbors = O_neighbors.view(list(O_neighbors.shape[:3]) + [3,3]) # [16, 464, 30, 3, 3]\n",
    "\n",
    "    dX = X_neighbors - X.unsqueeze(-2) # [16, 464, 30, 3]\n",
    "    dU = torch.matmul(O, dX.unsqueeze(-1)).squeeze(-1) # [16, 464, 30, 3] 邻居的相对坐标\n",
    "    R = torch.matmul(O.transpose(-1,-2), O_neighbors)\n",
    "    feat = torch.cat((_normalize(dU, dim=-1), _quaternions(R)), dim=-1) # 相对方向向量+旋转四元数\n",
    "    return feat\n",
    "\n",
    "\n",
    "def _orientations_coarse_gl_tuple(X, E_idx, eps=1e-6):\n",
    "    #N CA C O\n",
    "    V = X.clone()\n",
    "    X = X[:,:,:3,:].reshape(X.shape[0], 3*X.shape[1], 3) #B 3L 3\n",
    "    dX = X[:,1:,:] - X[:,:-1,:] # CA-N, C-CA, N-C, CA-N...\n",
    "    U = _normalize(dX, dim=-1)\n",
    "    u_0, u_1 = U[:,:-2,:], U[:,1:-1,:]\n",
    "    n_0 = _normalize(torch.cross(u_0, u_1), dim=-1)\n",
    "    b_1 = _normalize(u_0 - u_1, dim=-1)\n",
    "    \n",
    "    n_0 = n_0[:,::3,:]\n",
    "    b_1 = b_1[:,::3,:]\n",
    "    X = X[:,::3,:]\n",
    "    Q = torch.stack((b_1, n_0, torch.cross(b_1, n_0)), 2)\n",
    "    Q = Q.view(list(Q.shape[:2]) + [9])\n",
    "    Q = F.pad(Q, (0,0,0,1), 'constant', 0) # [16, 464, 9]\n",
    "\n",
    "    Q_neighbors = gather_nodes_d(Q, E_idx) # [16, 464, 30, 9]\n",
    "    X_neighbors = gather_nodes_d(V[:,:,1,:], E_idx) # [16, 464, 30, 3]\n",
    "    N_neighbors = gather_nodes_d(V[:,:,0,:], E_idx)\n",
    "    C_neighbors = gather_nodes_d(V[:,:,2,:], E_idx)\n",
    "    O_neighbors = gather_nodes_d(V[:,:,3,:], E_idx)\n",
    "\n",
    "    Q = Q.view(list(Q.shape[:2]) + [3,3]).unsqueeze(2) # [16, 464, 1, 3, 3]\n",
    "    Q_neighbors = Q_neighbors.view(list(Q_neighbors.shape[:3]) + [3,3]) # [16, 464, 30, 3, 3]\n",
    "\n",
    "    dX = torch.stack([X_neighbors,N_neighbors,C_neighbors,O_neighbors], dim=3) - X[:,:,None,None,:] # [16, 464, 30, 3]\n",
    "    dU = torch.matmul(Q[:,:,:,None,:,:], dX[...,None]).squeeze(-1) # [16, 464, 30, 3] 邻居的相对坐标\n",
    "    B, N, K = dU.shape[:3]\n",
    "    E_direct = _normalize(dU, dim=-1)\n",
    "    E_direct = E_direct.reshape(B, N, K,-1)\n",
    "    R = torch.matmul(Q.transpose(-1,-2), Q_neighbors)\n",
    "    q = _quaternions(R)\n",
    "    # edge_feat = torch.cat((dU, q), dim=-1) # 相对方向向量+旋转四元数\n",
    "    \n",
    "    dX_inner = V[:,:,[0,2,3],:] - X.unsqueeze(-2)\n",
    "    dU_inner = torch.matmul(Q, dX_inner.unsqueeze(-1)).squeeze(-1)\n",
    "    dU_inner = _normalize(dU_inner, dim=-1)\n",
    "    V_direct = dU_inner.reshape(B,N,-1)\n",
    "    return V_direct, E_direct, q\n",
    "\n",
    "def gather_edges_d(edges, neighbor_idx):\n",
    "    neighbors = neighbor_idx.unsqueeze(-1).expand(-1, -1, -1, edges.size(-1))\n",
    "    return torch.gather(edges, 2, neighbors)\n",
    "\n",
    "def gather_nodes_d(nodes, neighbor_idx):\n",
    "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1)) # [4, 317, 30]-->[4, 9510]\n",
    "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2)) # [4, 9510, dim]\n",
    "    neighbor_features = torch.gather(nodes, 1, neighbors_flat) # [4, 9510, dim]\n",
    "    return neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1]) # [4, 317, 30, 128]\n",
    "\n",
    "\n",
    "def _quaternions(R):\n",
    "    diag = torch.diagonal(R, dim1=-2, dim2=-1)\n",
    "    Rxx, Ryy, Rzz = diag.unbind(-1)\n",
    "    magnitudes = 0.5 * torch.sqrt(torch.abs(1 + torch.stack([\n",
    "            Rxx - Ryy - Rzz, \n",
    "        - Rxx + Ryy - Rzz, \n",
    "        - Rxx - Ryy + Rzz\n",
    "    ], -1)))\n",
    "    _R = lambda i,j: R[:,:,:,i,j]\n",
    "    signs = torch.sign(torch.stack([\n",
    "        _R(2,1) - _R(1,2),\n",
    "        _R(0,2) - _R(2,0),\n",
    "        _R(1,0) - _R(0,1)\n",
    "    ], -1))\n",
    "    xyz = signs * magnitudes\n",
    "    w = torch.sqrt(F.relu(1 + diag.sum(-1, keepdim=True))) / 2.\n",
    "    Q = torch.cat((xyz, w), -1)\n",
    "    return _normalize(Q, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "def featurize(batch, device,is_train = True):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    Clabel = [0,1,3,3,0,1,2,0,2,0,0,1,0,1,2,1,1,0,0,1,0]\n",
    "    B = len(batch)\n",
    "     \n",
    "    lengths = np.array([len(b['seq']) for b in batch], dtype=np.int32) #sum of chain seq lengths\n",
    "    L_max = max([len(b['seq']) for b in batch])\n",
    "    X = np.zeros([B, L_max, 4, 3])\n",
    "    residue_idx = -100*np.ones([B, L_max], dtype=np.int32) #residue idx with jumps across chains\n",
    "    chain_M = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted, 0.0 for the bits that are given\n",
    "    mask_self = np.ones([B, L_max, L_max], dtype=np.int32) #for interface loss calculation - 0.0 for self interaction, 1.0 for other\n",
    "    chain_encoding_all = np.zeros([B, L_max], dtype=np.int32) #integer encoding for chains 0, 0, 0,...0, 1, 1,..., 1, 2, 2, 2...\n",
    "    S = np.zeros([B, L_max], dtype=np.int32) #sequence AAs integers\n",
    "    S_s = np.zeros([B, L_max], dtype=np.int32)\n",
    "    mask_rp = np.zeros([B, L_max], dtype=np.int32)\n",
    "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
    "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
    "    chain_letters = init_alphabet + extra_alphabet\n",
    "    \n",
    "    chain_fixed = batch[0]['masked_list'] + batch[0]['visible_list']\n",
    "    \n",
    "    for i, b in enumerate(batch):\n",
    "        masked_chains = b['masked_list']\n",
    "        visible_chains = b['visible_list']\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        visible_temp_dict = {}\n",
    "        masked_temp_dict = {}\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            chain_seq = b[f'seq_chain_{letter}']\n",
    "            if letter in visible_chains:\n",
    "                visible_temp_dict[letter] = chain_seq\n",
    "            elif letter in masked_chains:\n",
    "                masked_temp_dict[letter] = chain_seq\n",
    "        for km, vm in masked_temp_dict.items():\n",
    "            for kv, vv in visible_temp_dict.items():\n",
    "                if vm == vv:\n",
    "                    if kv not in masked_chains:\n",
    "                        masked_chains.append(kv)\n",
    "                    if kv in visible_chains:\n",
    "                        visible_chains.remove(kv)\n",
    "        all_chains = masked_chains + visible_chains\n",
    "        random.shuffle(all_chains) #randomly shuffle chain order\n",
    "        num_chains = b['num_of_chains']\n",
    "        mask_dict = {}\n",
    "        x_chain_list = []\n",
    "        chain_mask_list = []\n",
    "        chain_seq_list = []\n",
    "        chain_encoding_list = []\n",
    "        c = 1\n",
    "        l0 = 0\n",
    "        l1 = 0\n",
    "        for step, letter in enumerate(chain_fixed):\n",
    "            if letter in visible_chains:\n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.zeros(chain_length) #0.0 for visible chains\n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_length,4,3]\n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "            elif letter in masked_chains: \n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                chain_length = len(chain_seq)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.ones(chain_length) #0.0 for visible chains\n",
    "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]\n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                mask_self[i, l0:l1, l0:l1] = np.zeros([chain_length, chain_length])\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "        x = np.concatenate(x_chain_list,0) #[L, 4, 3]\n",
    "        all_sequence = \"\".join(chain_seq_list)\n",
    "        m = np.concatenate(chain_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "        chain_encoding = np.concatenate(chain_encoding_list,0)\n",
    "\n",
    "        l = len(all_sequence)\n",
    "        x_pad = np.pad(x, [[0,L_max-l], [0,0], [0,0]], 'constant', constant_values=(np.nan, ))\n",
    "        X[i,:,:,:] = x_pad\n",
    "\n",
    "        m_pad = np.pad(m, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_M[i,:] = m_pad\n",
    "\n",
    "        chain_encoding_pad = np.pad(chain_encoding, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_encoding_all[i,:] = chain_encoding_pad\n",
    "\n",
    "        # Convert to labels\n",
    "        all_sequence = list(all_sequence)\n",
    "        for aas in range(len(all_sequence)):\n",
    "            if all_sequence[aas] not in alphabet:\n",
    "                all_sequence[aas] = \"X\"\n",
    "        all_sequence = \"\".join(all_sequence)\n",
    "        indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "\n",
    "        S[i, :l] = indices\n",
    "        \n",
    "        \n",
    "        S_s_s = []\n",
    "        for ids_aa in indices:\n",
    "            S_s_s.append(Clabel[ids_aa])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        S_s[i, :l] = S_s_s\n",
    "        \n",
    "        mask_rp[i,:l] = np.ones([l], dtype=np.int32)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    isnan = np.isnan(X)\n",
    "    mask = np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n",
    "    X[isnan] = 0.\n",
    "    \n",
    "    \n",
    "    #if is_train:\n",
    "        #mask = torch.from_numpy(mask_rp).to(dtype=torch.float32, device=device)\n",
    "    #else:\n",
    "        #mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
    "        \n",
    "\n",
    "    # Conversion\n",
    "    mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
    "    mask_rp = torch.from_numpy(mask_rp).to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    residue_idx = torch.from_numpy(residue_idx).to(dtype=torch.long,device=device)\n",
    "    S = torch.from_numpy(S).to(dtype=torch.long,device=device)\n",
    "    S_s = torch.from_numpy(S_s).to(dtype=torch.long,device=device)\n",
    "    X = torch.from_numpy(X).to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    mask_self = torch.from_numpy(mask_self).to(dtype=torch.float32, device=device)\n",
    "    chain_M = torch.from_numpy(chain_M).to(dtype=torch.float32, device=device)\n",
    "    chain_encoding_all = torch.from_numpy(chain_encoding_all).to(dtype=torch.long, device=device)\n",
    "    return X, [S,S_s], mask,mask_rp, lengths, chain_M, residue_idx, mask_self, chain_encoding_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fddf24-f646-4464-8d35-b1033aaa4e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60a441c1-ab78-46ac-aa76-459bf0598e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_nll(S, log_probs, mask):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    criterion = torch.nn.NLLLoss(reduction='none')\n",
    "    loss = criterion(\n",
    "        log_probs.contiguous().view(-1, log_probs.size(-1)), S.contiguous().view(-1)\n",
    "    ).view(S.size())\n",
    "    S_argmaxed = torch.argmax(log_probs,-1) #[B, L]\n",
    "    true_false = (S == S_argmaxed).float()\n",
    "    loss_av = torch.sum(loss * mask) / torch.sum(mask)\n",
    "    return loss, loss_av, true_false\n",
    "\n",
    "\n",
    "def loss_smoothed(S, log_probs, mask, weight=0.1):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    S_onehot = torch.nn.functional.one_hot(S, 21).float()\n",
    "\n",
    "    # Label smoothing\n",
    "    S_onehot = S_onehot + weight / float(S_onehot.size(-1))\n",
    "    S_onehot = S_onehot / S_onehot.sum(-1, keepdim=True)\n",
    "\n",
    "    loss = -(S_onehot * log_probs).sum(-1)\n",
    "    loss_av = torch.sum(loss * mask) / 2000.0 #fixed \n",
    "    return loss, loss_av\n",
    "def loss_smoothed_4(S, log_probs, mask, weight=0.1):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    S_onehot = torch.nn.functional.one_hot(S, 4).float()\n",
    "\n",
    "    # Label smoothing\n",
    "    S_onehot = S_onehot + weight / float(S_onehot.size(-1))\n",
    "    S_onehot = S_onehot / S_onehot.sum(-1, keepdim=True)\n",
    "\n",
    "    loss = -(S_onehot * log_probs).sum(-1)\n",
    "    loss_av = torch.sum(loss * mask) / 2000.0 #fixed \n",
    "    return loss, loss_av\n",
    "\n",
    "# The following gather functions\n",
    "def gather_edges(edges, neighbor_idx):\n",
    "    # Features [B,N,N,C] at Neighbor indices [B,N,K] => Neighbor features [B,N,K,C]\n",
    "    neighbors = neighbor_idx.unsqueeze(-1).expand(-1, -1, -1, edges.size(-1))\n",
    "    edge_features = torch.gather(edges, 2, neighbors)\n",
    "    return edge_features\n",
    "\n",
    "def gather_nodes(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n",
    "    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n",
    "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n",
    "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    # Gather and re-pack\n",
    "    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n",
    "    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n",
    "    return neighbor_features\n",
    "\n",
    "def gather_nodes_t(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor index [B,K] => Neighbor features[B,K,C]\n",
    "    idx_flat = neighbor_idx.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    neighbor_features = torch.gather(nodes, 1, idx_flat)\n",
    "    return neighbor_features\n",
    "\n",
    "def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n",
    "    h_nodes = gather_nodes(h_nodes, E_idx)\n",
    "    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n",
    "    return h_nn\n",
    "\n",
    "\n",
    "class EncLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(EncLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.norm3 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W11 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W13 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.all_global = EncLayer_global(num_hidden)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, h_V, h_E, E_idx, mask_V=None, mask_attend=None, res_idx = None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "        E_num = E_idx.shape[-1]\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / E_num#self.scale\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "            \n",
    "        h_V = self.all_global(h_V,mask_V,res_idx)\n",
    "        \n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
    "        h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))\n",
    "        h_E = self.norm3(h_E + self.dropout3(h_message))\n",
    "        \n",
    "        \n",
    "        #c_V = h_V.mean(1)\n",
    "        #h_V = h_V * (self.V_MLP_g(c_V).unsqueeze(1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return h_V, h_E\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class EncLayer_global(nn.Module):\n",
    "    def __init__(self, num_hidden, dropout=0.2, num_heads=None):\n",
    "        super(EncLayer_global, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        #self.num_in = num_in\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        #self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        #self.norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.pos = SinusoidalPositionalEmbedding(num_hidden,0)\n",
    "\n",
    "\n",
    "        #self.act = torch.nn.GELU()\n",
    "        #self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "        \n",
    "\n",
    "        self.att = TransformerBlock(num_hidden,num_hidden*4)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, h_V,mask,res_idx):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "        \n",
    "        pos_hid = self.pos(res_idx.squeeze(-1))\n",
    "\n",
    "        h_V_agg = self.att(h_V+pos_hid,mask)\n",
    "        \n",
    "        #h_V_agg = self.act(h_V_agg)\n",
    "        \n",
    "        h_V = self.norm1(h_V + self.dropout1(h_V_agg))\n",
    "\n",
    "                \n",
    "        return h_V\n",
    "\n",
    "\n",
    "class DecLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(DecLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "        \n",
    "        E_num = h_E.shape[-2]\n",
    "        # Concatenate h_V_i to h_E_ij\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_E.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_E], -1)\n",
    "\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / E_num\n",
    "\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "\n",
    "        # Position-wise feedforward\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "        return h_V\n",
    "\n",
    "\n",
    "def mask_tensor(A, mask):\n",
    "    \n",
    "    mask = mask.unsqueeze(-1).expand_as(A)\n",
    "    \n",
    "    A = A * mask\n",
    "    \n",
    "    return A\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, num_hidden, num_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.W_in = nn.Linear(num_hidden, num_ff, bias=True)\n",
    "        self.W_out = nn.Linear(num_ff, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "    def forward(self, h_V):\n",
    "        h = self.act(self.W_in(h_V))\n",
    "        h = self.W_out(h)\n",
    "        return h\n",
    "\n",
    "class PositionalEncodings(nn.Module):\n",
    "    def __init__(self, num_embeddings, max_relative_feature=32):\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.max_relative_feature = max_relative_feature\n",
    "        self.linear = nn.Linear(2*max_relative_feature+1+1, num_embeddings)\n",
    "\n",
    "    def forward(self, offset, mask):\n",
    "        d = torch.clip(offset + self.max_relative_feature, 0, 2*self.max_relative_feature)*mask + (1-mask)*(2*self.max_relative_feature+1)\n",
    "        d_onehot = torch.nn.functional.one_hot(d, 2*self.max_relative_feature+1+1)\n",
    "        E = self.linear(d_onehot.float())\n",
    "        return E\n",
    "\n",
    "\n",
    "def replace_masked_elements(A, B, mask):\n",
    "    \n",
    "    assert A.shape == B.shape and A.shape[:2] == mask.shape, \"SSSSSSS\"\n",
    "    \n",
    "    \n",
    "    mask = mask.unsqueeze(-1).expand_as(A)\n",
    "    \n",
    "    \n",
    "    result = torch.where(mask == 0, B, A)\n",
    "    \n",
    "    return result\n",
    "\n",
    "class ProteinFeatures(nn.Module):\n",
    "    def __init__(self, edge_features, node_features, num_positional_embeddings=16,\n",
    "        num_rbf=16, top_k=30, augment_eps=0., num_chain_embeddings=16):\n",
    "        \"\"\" Extract protein features \"\"\"\n",
    "        super(ProteinFeatures, self).__init__()\n",
    "        self.edge_features = edge_features\n",
    "        self.node_features = node_features\n",
    "        self.top_k = top_k\n",
    "        self.augment_eps = augment_eps \n",
    "        self.num_rbf = num_rbf\n",
    "        self.num_positional_embeddings = num_positional_embeddings\n",
    "\n",
    "        self.embeddings = PositionalEncodings(num_positional_embeddings)\n",
    "        node_in, edge_in = 6, num_positional_embeddings + num_rbf*25\n",
    "        \n",
    "        edge_in = edge_in+ 18\n",
    "        \n",
    "        node_in = 22\n",
    "        self.edge_embedding = nn.Linear(edge_in, edge_features, bias=False)\n",
    "        self.norm_edges = nn.LayerNorm(edge_features)\n",
    "        \n",
    "        self.node_embedding = nn.Linear(node_in, edge_features, bias=False)\n",
    "        self.norm_node = nn.LayerNorm(edge_features)\n",
    "\n",
    "    def _dist(self, X, mask, eps=1E-6):\n",
    "        mask_2D = torch.unsqueeze(mask,1) * torch.unsqueeze(mask,2)\n",
    "        dX = torch.unsqueeze(X,1) - torch.unsqueeze(X,2)\n",
    "        D = mask_2D * torch.sqrt(torch.sum(dX**2, 3) + eps)\n",
    "        D_max, _ = torch.max(D, -1, keepdim=True)\n",
    "        D_adjust = D + (1. - mask_2D) * D_max\n",
    "        sampled_top_k = self.top_k\n",
    "        D_neighbors, E_idx = torch.topk(D_adjust, np.minimum(self.top_k, X.shape[1]), dim=-1, largest=False)\n",
    "        return D_neighbors, E_idx\n",
    "    \n",
    "    def _dist_seq(self, res_idx):\n",
    " \n",
    "        \n",
    "        D_adjust = res_idx[:,:,None] - res_idx[:,None,:]\n",
    "        D_adjust = abs(D_adjust)\n",
    "        sampled_top_k = self.top_k\n",
    "        _, E_idx = torch.topk(D_adjust, np.minimum(self.top_k, res_idx.shape[1]), dim=-1, largest=False)\n",
    "        return  E_idx\n",
    "\n",
    "    def _rbf(self, D):\n",
    "        device = D.device\n",
    "        D_min, D_max, D_count = 2., 22., self.num_rbf\n",
    "        D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "        D_mu = D_mu.view([1,1,1,-1])\n",
    "        D_sigma = (D_max - D_min) / D_count\n",
    "        D_expand = torch.unsqueeze(D, -1)\n",
    "        RBF = torch.exp(-((D_expand - D_mu) / D_sigma)**2)\n",
    "        return RBF\n",
    "\n",
    "    def _get_rbf(self, A, B, E_idx):\n",
    "        D_A_B = torch.sqrt(torch.sum((A[:,:,None,:] - B[:,None,:,:])**2,-1) + 1e-6) #[B, L, L]\n",
    "        D_A_B_neighbors = gather_edges(D_A_B[:,:,:,None], E_idx)[:,:,:,0] #[B,L,K]\n",
    "        RBF_A_B = self._rbf(D_A_B_neighbors)\n",
    "        return RBF_A_B\n",
    "\n",
    "    def forward(self, X, mask, residue_idx, chain_labels):\n",
    "        if self.training and self.augment_eps > 0:\n",
    "            X = X + self.augment_eps * torch.randn_like(X)\n",
    "        #N CA C O\n",
    "        N,B = X.shape[:2]\n",
    "        \n",
    "        \n",
    "        b = X[:,:,1,:] - X[:,:,0,:]\n",
    "        c = X[:,:,2,:] - X[:,:,1,:]\n",
    "        a = torch.cross(b, c, dim=-1)\n",
    "        Cb = -0.58273431*a + 0.56802827*b - 0.54067466*c + X[:,:,1,:]\n",
    "        Ca = X[:,:,1,:]\n",
    "        N = X[:,:,0,:]\n",
    "        C = X[:,:,2,:]\n",
    "        O = X[:,:,3,:]\n",
    " \n",
    "        D_neighbors, E_idx = self._dist(Ca, mask)\n",
    "        E_num = E_idx.shape[-1]\n",
    "        \n",
    "        E_idx_res = self._dist_seq(residue_idx)\n",
    "        E_idx = replace_masked_elements(E_idx,E_idx_res,mask)\n",
    "        \n",
    "        RBF_all = []\n",
    "        RBF_all.append(self._rbf(D_neighbors)) #Ca-Ca\n",
    "        RBF_all.append(self._get_rbf(N, N, E_idx)) #N-N\n",
    "        RBF_all.append(self._get_rbf(C, C, E_idx)) #C-C\n",
    "        RBF_all.append(self._get_rbf(O, O, E_idx)) #O-O\n",
    "        RBF_all.append(self._get_rbf(Cb, Cb, E_idx)) #Cb-Cb\n",
    "        RBF_all.append(self._get_rbf(Ca, N, E_idx)) #Ca-N\n",
    "        RBF_all.append(self._get_rbf(Ca, C, E_idx)) #Ca-C\n",
    "        RBF_all.append(self._get_rbf(Ca, O, E_idx)) #Ca-O\n",
    "        RBF_all.append(self._get_rbf(Ca, Cb, E_idx)) #Ca-Cb\n",
    "        RBF_all.append(self._get_rbf(N, C, E_idx)) #N-C\n",
    "        RBF_all.append(self._get_rbf(N, O, E_idx)) #N-O\n",
    "        RBF_all.append(self._get_rbf(N, Cb, E_idx)) #N-Cb\n",
    "        RBF_all.append(self._get_rbf(Cb, C, E_idx)) #Cb-C\n",
    "        RBF_all.append(self._get_rbf(Cb, O, E_idx)) #Cb-O\n",
    "        RBF_all.append(self._get_rbf(O, C, E_idx)) #O-C\n",
    "        RBF_all.append(self._get_rbf(N, Ca, E_idx)) #N-Ca\n",
    "        RBF_all.append(self._get_rbf(C, Ca, E_idx)) #C-Ca\n",
    "        RBF_all.append(self._get_rbf(O, Ca, E_idx)) #O-Ca\n",
    "        RBF_all.append(self._get_rbf(Cb, Ca, E_idx)) #Cb-Ca\n",
    "        RBF_all.append(self._get_rbf(C, N, E_idx)) #C-N\n",
    "        RBF_all.append(self._get_rbf(O, N, E_idx)) #O-N\n",
    "        RBF_all.append(self._get_rbf(Cb, N, E_idx)) #Cb-N\n",
    "        RBF_all.append(self._get_rbf(C, Cb, E_idx)) #C-Cb\n",
    "        RBF_all.append(self._get_rbf(O, Cb, E_idx)) #O-Cb\n",
    "        RBF_all.append(self._get_rbf(C, O, E_idx)) #C-O\n",
    "        RBF_all = torch.cat(tuple(RBF_all), dim=-1)\n",
    "        \n",
    "        \n",
    "        V_angles = _dihedrals(X, 0)  # B N 12\n",
    "        #V_angles = node_mask_select(V_angles)\n",
    "        \n",
    "        mask_V_angles = mask.unsqueeze(-1)\n",
    "        V_angles = V_angles * mask_V_angles\n",
    "    \n",
    "        \n",
    "\n",
    "        V_direct, E_direct, E_angles = _orientations_coarse_gl_tuple(X, E_idx)\n",
    "        \n",
    "        V_direct = V_direct * mask_V_angles #B N 9\n",
    "        \n",
    "        mask_E = torch.gather(mask.unsqueeze(1).repeat(1,B,1),-1,E_idx) #B N E\n",
    "        \n",
    "        mask_expend = mask.unsqueeze(-1).repeat(1,1,E_num)\n",
    "        \n",
    "        mask_expend_E = torch.cat([mask_E.unsqueeze(-1),mask_expend.unsqueeze(-1)],-1) #B N E 2\n",
    "        \n",
    "        E_direct = E_direct * (mask_E.unsqueeze(-1))#B N E 12\n",
    "        E_angles = E_angles * (mask_E.unsqueeze(-1))#B N E 4\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        offset = residue_idx[:,:,None]-residue_idx[:,None,:]\n",
    "        offset = gather_edges(offset[:,:,:,None], E_idx)[:,:,:,0] #[B, L, K]\n",
    "\n",
    "        d_chains = ((chain_labels[:, :, None] - chain_labels[:,None,:])==0).long() #find self vs non-self interaction\n",
    "        E_chains = gather_edges(d_chains[:,:,:,None], E_idx)[:,:,:,0]\n",
    "        E_positional = self.embeddings(offset.long(), E_chains)\n",
    "        E = torch.cat((E_positional, RBF_all), -1)\n",
    "        \n",
    "        E = torch.cat((E, E_direct), -1)\n",
    "        \n",
    "        E = torch.cat((E, E_angles), -1)\n",
    "        \n",
    "        E = torch.cat((E, mask_expend_E), -1)\n",
    "        \n",
    "        \n",
    "        E = self.edge_embedding(E)\n",
    "        E = self.norm_edges(E)\n",
    "        \n",
    "        V = torch.cat([V_angles,V_direct],-1)\n",
    "        \n",
    "        V = torch.cat([V,mask.unsqueeze(-1)],-1)\n",
    "        \n",
    "        V = self.node_embedding(V)\n",
    "        V = self.norm_node(V)\n",
    "        \n",
    "        return E, E_idx,V\n",
    "\n",
    "\n",
    "\n",
    "class ProDualNet_pretrain(nn.Module):\n",
    "    def __init__(self, num_letters=21, node_features=128, edge_features=128,\n",
    "        hidden_dim=128, num_encoder_layers=3, num_decoder_layers=3,\n",
    "        vocab=21, k_neighbors=32, augment_eps=0.1, dropout=0.1):\n",
    "        super(ProDualNet_pretrain, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.node_features = node_features\n",
    "        self.edge_features = edge_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.features = ProteinFeatures(node_features, edge_features, top_k=k_neighbors, augment_eps=augment_eps)\n",
    "\n",
    "        self.W_e = nn.Linear(edge_features, hidden_dim, bias=True)\n",
    "        self.W_v = nn.Linear(edge_features, hidden_dim, bias=True)\n",
    "        self.W_s = nn.Embedding(vocab, hidden_dim)\n",
    "        \n",
    "        self.W_ss = nn.Embedding(4, hidden_dim)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncLayer(hidden_dim, hidden_dim*2, dropout=dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecLayer(hidden_dim, hidden_dim*3, dropout=dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        #self.h1 = nn.Linear(hidden_dim*2, hidden_dim, bias=True)\n",
    "        \n",
    "        self.W_out = nn.Linear(hidden_dim, num_letters, bias=True)\n",
    "        \n",
    "        self.W_out1 = nn.Linear(hidden_dim, num_letters, bias=True)\n",
    "        self.W_out2 = nn.Linear(hidden_dim, num_letters, bias=True)\n",
    "        \n",
    "        self.W_out_s = nn.Linear(hidden_dim, 4, bias=True)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, X, S, mask_train, mask, chain_M, residue_idx, chain_encoding_all, is_eval = False ):\n",
    "        \"\"\" Graph-conditioned sequence model \"\"\"\n",
    "        device=X.device\n",
    "        # Prepare node and edge embeddings\n",
    "        #print(mask.shape)\n",
    "        #S,S1 = S[0],S[1]\n",
    "        E, E_idx,h_V = self.features(X, mask_train, residue_idx, chain_encoding_all)\n",
    "        #h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device)\n",
    "        h_E = self.W_e(E)\n",
    "        h_V = self.W_v(h_V)\n",
    "\n",
    "        # Encoder is unmasked self-attention\n",
    "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
    "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
    "        \n",
    "        #print(mask)\n",
    "        for layer in self.encoder_layers:\n",
    "            #print(mask.shape)\n",
    "            h_V, h_E = torch.utils.checkpoint.checkpoint(layer, h_V, h_E, E_idx, mask, mask_attend,residue_idx)\n",
    "            \n",
    "\n",
    "        # Concatenate sequence embeddings for autoregressive decoder\n",
    "        \n",
    "        \n",
    "        h_S = self.W_s(S[0])\n",
    "        \n",
    "        #h_S = self.h1(h_S)\n",
    "        \n",
    "        h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)\n",
    "\n",
    "        # Build encoder embeddings\n",
    "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
    "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
    "\n",
    "\n",
    "        chain_M = chain_M*mask #update chain_M to include missing regions\n",
    "        decoding_order = torch.argsort((chain_M+0.0001)*(torch.abs(torch.randn(chain_M.shape, device=device)))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
    "        mask_size = E_idx.shape[1]\n",
    "        permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
    "        order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
    "        if is_eval:\n",
    "            order_mask_backward = torch.ones_like(order_mask_backward)\n",
    "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
    "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
    "        mask_bw = mask_1D * mask_attend\n",
    "        mask_fw = mask_1D * (1. - mask_attend)\n",
    "\n",
    "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
    "        for layer in self.decoder_layers:\n",
    "            h_ESV = cat_neighbors_nodes(h_V, h_ES, E_idx)\n",
    "            h_ESV = mask_bw * h_ESV + h_EXV_encoder_fw\n",
    "            h_V = torch.utils.checkpoint.checkpoint(layer, h_V, h_ESV, mask_train)\n",
    "\n",
    "        logits = self.W_out(h_V)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "      \n",
    "        \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "440a2b26-df7f-4d4b-a1a3-d13de1b1b6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok----------\n"
     ]
    }
   ],
   "source": [
    "    import argparse\n",
    "    import os.path\n",
    "\n",
    "    import json, time, os, sys, glob\n",
    "    import shutil\n",
    "    import warnings\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch import optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    import queue\n",
    "    import copy\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import random\n",
    "    import os.path\n",
    "    import subprocess\n",
    "    from concurrent.futures import ProcessPoolExecutor    \n",
    "\n",
    "     \n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "    PATH =  checkpoint_path = \"produalnet_pretrain.pt\"\n",
    "    model = ProDualNet_pretrain(node_features=128, \n",
    "                        edge_features=128, \n",
    "                        hidden_dim=128, \n",
    "                        num_encoder_layers=4, \n",
    "                        num_decoder_layers=4, \n",
    "                        k_neighbors=48, \n",
    "                        dropout=0.1, \n",
    "                        augment_eps=0.2)\n",
    "    #model.to(device)\n",
    "\n",
    "\n",
    "    if PATH:\n",
    "        checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "        total_step = checkpoint['step'] #write total_step from the checkpoint\n",
    "        epoch = checkpoint['epoch'] #write epoch from the checkpoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        print(\"load ok----------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac2ea6-cd6e-474b-a70b-0dfe4df060d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2c3a7-4bd9-4cb4-812c-18511af69615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f79b6-c7cf-4ee2-a122-0a52d25215c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39bd31a3-7804-4da7-b0b6-0c0d201bbc04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _scores(S, log_probs, mask):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    criterion = torch.nn.NLLLoss(reduction='none')\n",
    "    loss = criterion(\n",
    "        log_probs.contiguous().view(-1,log_probs.size(-1)),\n",
    "        S.contiguous().view(-1)\n",
    "    ).view(S.size())\n",
    "    scores = torch.sum(loss * mask, dim=-1) / torch.sum(mask, dim=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c83534-c1e9-4f42-b742-589ce70ea6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "# 打开文件并逐行读取\n",
    "with open('./mut_complex_ddg/alls.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的JSON数据\n",
    "        data1 = json.loads(line)\n",
    "       \n",
    "        data1[\"masked_list\"] = []\n",
    "        data1[\"visible_list\"] = []\n",
    "        data.append( data1)\n",
    "        \n",
    "data_pdb = {}\n",
    "for i in data:\n",
    "    data_pdb[i[\"name\"]] = i\n",
    "    \n",
    "        \n",
    "            \n",
    "for i in lst_id:\n",
    "    pdb_name = i[0]\n",
    "    names = pdb_name[4:]\n",
    "    names = names.split(\"_\")\n",
    "    chain_lst = []\n",
    "    for i in names:\n",
    "        chain_lst = chain_lst + list(i)\n",
    "    data_pdb[pdb_name[:4]][\"visible_list\"] = chain_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d69d55-c248-4c8e-8043-8d582012055e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1867a86-38c1-4f2f-817e-88e4b2b8932e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e493b-bbc8-4123-9fdc-6a8e245a1a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47963b-b970-40a2-a8df-628be90aab59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c575aea6-37d4-4bc9-a940-19d3ab5f94e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1CHO_EFG_I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dssg/home/acct-clsyzs/clsyzs/.conda/envs/esm_dds/lib/python3.7/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3SGB_E_I\n",
      "1R0R_E_I\n",
      "1JTG_A_B\n",
      "1PPF_E_I\n",
      "1A22_A_B\n",
      "3BT1_A_U\n",
      "3S9D_A_B\n",
      "1AO7_ABC_DE\n",
      "1DAN_HL_UT\n",
      "1LFD_A_B\n",
      "3HFM_HL_Y\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "lst_result = []\n",
    "lst_label = []\n",
    "for i in lst_id[:]:\n",
    "    print(i[0])\n",
    "    names = i[0]\n",
    "    mut_lst,ddg_list = get_mut_ddg(grouped_dict[names])\n",
    "    pdb_id = names[:4]\n",
    "    pdb_wt = copy.deepcopy(data_pdb[pdb_id])\n",
    "    lst_pdb = []\n",
    "    lst_pdb.append(pdb_wt)\n",
    "    \n",
    "    for mut in mut_lst:\n",
    "        #if len(mut) != 1:\n",
    "            #continue\n",
    "        pdb_mt = copy.deepcopy(data_pdb[pdb_id])\n",
    "        for single_mut in mut:\n",
    "            chain_mut = single_mut[1]\n",
    "            wt_aa = single_mut[0]\n",
    "            mt_aa = single_mut[-1]\n",
    "            lens = len(single_mut)\n",
    "            loc = int(single_mut[2:-1])\n",
    "            \n",
    "            if wt_aa == pdb_mt[\"seq_chain_\"+chain_mut][loc-1]:\n",
    "                s_list = list(pdb_mt[\"seq_chain_\"+chain_mut])\n",
    "                s_list[loc-1] = mt_aa \n",
    "                new_s = ''.join(s_list)\n",
    "                pdb_mt[\"seq_chain_\"+chain_mut] = new_s\n",
    "            else:\n",
    "                print(\"error\",wt_aa,single_mut)\n",
    "                \n",
    "        lst_pdb.append(pdb_mt)\n",
    "    #model.cuda()    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X, S, mask,mask_train, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(lst_pdb, \"cpu\")\n",
    "        log_probs  = model(X, S, mask,mask_train, chain_M, residue_idx, chain_encoding_all,is_eval = True)\n",
    "        \n",
    "        global_scores = -_scores(S[0], log_probs, mask)\n",
    "        global_native_score = global_scores.cpu().data\n",
    "\n",
    "        lst_result.append(global_native_score[1:] / global_native_score[0])\n",
    "        lst_label.append(ddg_list)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef510716-372a-42fd-82a8-7e5bf2597aeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ESM-IF1 cant eval 1DAN_HL_UT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca48ceb-6220-49c5-bbe2-7365cd15652c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b862a-9e0e-4a18-bdec-842cf08fc0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esm_dds)",
   "language": "python",
   "name": "esm_dds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
