{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17282db4-e8e4-4933-8285-19997e030883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8929b406-0c77-4ca1-aaaa-59a97ae6aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, os, sys, glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split, Subset\n",
    "\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#A number of functions/classes are adopted from: https://github.com/jingraham/neurips19-graph-protein-design\n",
    "\n",
    "def parse_fasta(filename,limit=-1, omit=[]):\n",
    "    header = []\n",
    "    sequence = []\n",
    "    lines = open(filename, \"r\")\n",
    "    for line in lines:\n",
    "        line = line.rstrip()\n",
    "        if line[0] == \">\":\n",
    "            if len(header) == limit:\n",
    "                break\n",
    "            header.append(line[1:])\n",
    "            sequence.append([])\n",
    "        else:\n",
    "            if omit:\n",
    "                line = [item for item in line if item not in omit]\n",
    "                line = ''.join(line)\n",
    "            line = ''.join(line)\n",
    "            sequence[-1].append(line)\n",
    "    lines.close()\n",
    "    sequence = [''.join(seq) for seq in sequence]\n",
    "    return np.array(header), np.array(sequence)\n",
    "\n",
    "def _scores(S, log_probs, mask):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    criterion = torch.nn.NLLLoss(reduction='none')\n",
    "    loss = criterion(\n",
    "        log_probs.contiguous().view(-1,log_probs.size(-1)),\n",
    "        S.contiguous().view(-1)\n",
    "    ).view(S.size())\n",
    "    scores = torch.sum(loss * mask, dim=-1) / torch.sum(mask, dim=-1)\n",
    "    return scores\n",
    "\n",
    "def _S_to_seq(S, mask):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    seq = ''.join([alphabet[c] for c, m in zip(S.tolist(), mask.tolist()) if m > 0])\n",
    "    return seq\n",
    "\n",
    "def parse_PDB_biounits(x, atoms=['N','CA','C'], chain=None):\n",
    "  '''\n",
    "  input:  x = PDB filename\n",
    "          atoms = atoms to extract (optional)\n",
    "  output: (length, atoms, coords=(x,y,z)), sequence\n",
    "  '''\n",
    "\n",
    "  alpha_1 = list(\"ARNDCQEGHILKMFPSTWYV-\")\n",
    "  states = len(alpha_1)\n",
    "  alpha_3 = ['ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE',\n",
    "             'LEU','LYS','MET','PHE','PRO','SER','THR','TRP','TYR','VAL','GAP']\n",
    "  \n",
    "  aa_1_N = {a:n for n,a in enumerate(alpha_1)}\n",
    "  aa_3_N = {a:n for n,a in enumerate(alpha_3)}\n",
    "  aa_N_1 = {n:a for n,a in enumerate(alpha_1)}\n",
    "  aa_1_3 = {a:b for a,b in zip(alpha_1,alpha_3)}\n",
    "  aa_3_1 = {b:a for a,b in zip(alpha_1,alpha_3)}\n",
    "  \n",
    "  def AA_to_N(x):\n",
    "    # [\"ARND\"] -> [[0,1,2,3]]\n",
    "    x = np.array(x);\n",
    "    if x.ndim == 0: x = x[None]\n",
    "    return [[aa_1_N.get(a, states-1) for a in y] for y in x]\n",
    "  \n",
    "  def N_to_AA(x):\n",
    "    # [[0,1,2,3]] -> [\"ARND\"]\n",
    "    x = np.array(x);\n",
    "    if x.ndim == 1: x = x[None]\n",
    "    return [\"\".join([aa_N_1.get(a,\"-\") for a in y]) for y in x]\n",
    "\n",
    "  xyz,seq,min_resn,max_resn = {},{},1e6,-1e6\n",
    "  for line in open(x,\"rb\"):\n",
    "    line = line.decode(\"utf-8\",\"ignore\").rstrip()\n",
    "\n",
    "    if line[:6] == \"HETATM\" and line[17:17+3] == \"MSE\":\n",
    "      line = line.replace(\"HETATM\",\"ATOM  \")\n",
    "      line = line.replace(\"MSE\",\"MET\")\n",
    "\n",
    "    if line[:4] == \"ATOM\":\n",
    "      ch = line[21:22]\n",
    "      if ch == chain or chain is None:\n",
    "        atom = line[12:12+4].strip()\n",
    "        resi = line[17:17+3]\n",
    "        resn = line[22:22+5].strip()\n",
    "        x,y,z = [float(line[i:(i+8)]) for i in [30,38,46]]\n",
    "\n",
    "        if resn[-1].isalpha(): \n",
    "            resa,resn = resn[-1],int(resn[:-1])-1\n",
    "        else: \n",
    "            resa,resn = \"\",int(resn)-1\n",
    "#         resn = int(resn)\n",
    "        if resn < min_resn: \n",
    "            min_resn = resn\n",
    "        if resn > max_resn: \n",
    "            max_resn = resn\n",
    "        if resn not in xyz: \n",
    "            xyz[resn] = {}\n",
    "        if resa not in xyz[resn]: \n",
    "            xyz[resn][resa] = {}\n",
    "        if resn not in seq: \n",
    "            seq[resn] = {}\n",
    "        if resa not in seq[resn]: \n",
    "            seq[resn][resa] = resi\n",
    "\n",
    "        if atom not in xyz[resn][resa]:\n",
    "          xyz[resn][resa][atom] = np.array([x,y,z])\n",
    "\n",
    "  # convert to numpy arrays, fill in missing values\n",
    "  seq_,xyz_ = [],[]\n",
    "  try:\n",
    "      for resn in range(min_resn,max_resn+1):\n",
    "        if resn in seq:\n",
    "          for k in sorted(seq[resn]): seq_.append(aa_3_N.get(seq[resn][k],20))\n",
    "        else: seq_.append(20)\n",
    "        if resn in xyz:\n",
    "          for k in sorted(xyz[resn]):\n",
    "            for atom in atoms:\n",
    "              if atom in xyz[resn][k]: xyz_.append(xyz[resn][k][atom])\n",
    "              else: xyz_.append(np.full(3,np.nan))\n",
    "        else:\n",
    "          for atom in atoms: xyz_.append(np.full(3,np.nan))\n",
    "      return np.array(xyz_).reshape(-1,len(atoms),3), N_to_AA(np.array(seq_))\n",
    "  except TypeError:\n",
    "      return 'no_chain', 'no_chain'\n",
    "\n",
    "def parse_PDB(path_to_pdb, input_chain_list=None, ca_only=False):\n",
    "    c=0\n",
    "    pdb_dict_list = []\n",
    "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
    "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
    "    chain_alphabet = init_alphabet + extra_alphabet\n",
    "     \n",
    "    if input_chain_list:\n",
    "        chain_alphabet = input_chain_list  \n",
    " \n",
    "\n",
    "    biounit_names = [path_to_pdb]\n",
    "    for biounit in biounit_names:\n",
    "        my_dict = {}\n",
    "        s = 0\n",
    "        concat_seq = ''\n",
    "        concat_N = []\n",
    "        concat_CA = []\n",
    "        concat_C = []\n",
    "        concat_O = []\n",
    "        concat_mask = []\n",
    "        coords_dict = {}\n",
    "        for letter in chain_alphabet:\n",
    "            if ca_only:\n",
    "                sidechain_atoms = ['CA']\n",
    "            else:\n",
    "                sidechain_atoms = ['N', 'CA', 'C', 'O']\n",
    "            xyz, seq = parse_PDB_biounits(biounit, atoms=sidechain_atoms, chain=letter)\n",
    "            if type(xyz) != str:\n",
    "                concat_seq += seq[0]\n",
    "                my_dict['seq_chain_'+letter]=seq[0]\n",
    "                coords_dict_chain = {}\n",
    "                if ca_only:\n",
    "                    coords_dict_chain['CA_chain_'+letter]=xyz.tolist()\n",
    "                else:\n",
    "                    coords_dict_chain['N_chain_' + letter] = xyz[:, 0, :].tolist()\n",
    "                    coords_dict_chain['CA_chain_' + letter] = xyz[:, 1, :].tolist()\n",
    "                    coords_dict_chain['C_chain_' + letter] = xyz[:, 2, :].tolist()\n",
    "                    coords_dict_chain['O_chain_' + letter] = xyz[:, 3, :].tolist()\n",
    "                my_dict['coords_chain_'+letter]=coords_dict_chain\n",
    "                s += 1\n",
    "        fi = biounit.rfind(\"/\")\n",
    "        my_dict['name']=biounit[(fi+1):-4]\n",
    "        my_dict['num_of_chains'] = s\n",
    "        my_dict['seq'] = concat_seq\n",
    "        if s <= len(chain_alphabet):\n",
    "            pdb_dict_list.append(my_dict)\n",
    "            c+=1\n",
    "    return pdb_dict_list\n",
    "\n",
    "\n",
    "\n",
    "def tied_featurize(batch, device, chain_dict, fixed_position_dict=None, omit_AA_dict=None, tied_positions_dict=None, pssm_dict=None, bias_by_res_dict=None, ca_only=False):\n",
    "    \"\"\" Pack and pad batch into torch tensors \"\"\"\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    B = len(batch)\n",
    "    lengths = np.array([len(b['seq']) for b in batch], dtype=np.int32) #sum of chain seq lengths\n",
    "    L_max = max([len(b['seq']) for b in batch])\n",
    "    if ca_only:\n",
    "        X = np.zeros([B, L_max, 1, 3])\n",
    "    else:\n",
    "        X = np.zeros([B, L_max, 4, 3])\n",
    "    residue_idx = -100*np.ones([B, L_max], dtype=np.int32)\n",
    "    chain_M = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted\n",
    "    pssm_coef_all = np.zeros([B, L_max], dtype=np.float32) #1.0 for the bits that need to be predicted\n",
    "    pssm_bias_all = np.zeros([B, L_max, 21], dtype=np.float32) #1.0 for the bits that need to be predicted\n",
    "    pssm_log_odds_all = 10000.0*np.ones([B, L_max, 21], dtype=np.float32) #1.0 for the bits that need to be predicted\n",
    "    chain_M_pos = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted\n",
    "    bias_by_res_all = np.zeros([B, L_max, 21], dtype=np.float32)\n",
    "    chain_encoding_all = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted\n",
    "    S = np.zeros([B, L_max], dtype=np.int32)\n",
    "    omit_AA_mask = np.zeros([B, L_max, len(alphabet)], dtype=np.int32)\n",
    "    # Build the batch\n",
    "    letter_list_list = []\n",
    "    visible_list_list = []\n",
    "    masked_list_list = []\n",
    "    masked_chain_length_list_list = []\n",
    "    tied_pos_list_of_lists_list = []\n",
    "    for i, b in enumerate(batch):\n",
    "        #print(\"------------------\",b['name'])\n",
    "        if chain_dict != None:\n",
    "            masked_chains, visible_chains = chain_dict[b['name']] #masked_chains a list of chain letters to predict [A, D, F]\n",
    "        else:\n",
    "            masked_chains = [item[-1:] for item in list(b) if item[:10]=='seq_chain_']\n",
    "            visible_chains = []\n",
    "        masked_chains.sort() #sort masked_chains \n",
    "        visible_chains.sort() #sort visible_chains \n",
    "        #print(\"visible_chains\",visible_chains)\n",
    "        #print(\"masked_chains\",masked_chains)\n",
    "        all_chains = masked_chains + visible_chains\n",
    "    print(all_chains)\n",
    "    for i, b in enumerate(batch):\n",
    "        mask_dict = {}\n",
    "        a = 0\n",
    "        x_chain_list = []\n",
    "        chain_mask_list = []\n",
    "        chain_seq_list = []\n",
    "        chain_encoding_list = []\n",
    "        c = 1\n",
    "        letter_list = []\n",
    "        global_idx_start_list = [0]\n",
    "        visible_list = []\n",
    "        masked_list = []\n",
    "        masked_chain_length_list = []\n",
    "        fixed_position_mask_list = []\n",
    "        omit_AA_mask_list = []\n",
    "        pssm_coef_list = []\n",
    "        pssm_bias_list = []\n",
    "        pssm_log_odds_list = []\n",
    "        bias_by_res_list = []\n",
    "        l0 = 0\n",
    "        l1 = 0\n",
    "        for step, letter in enumerate(all_chains):\n",
    "            if letter in visible_chains:\n",
    "                letter_list.append(letter)\n",
    "                visible_list.append(letter)\n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                chain_seq = ''.join([a if a!='-' else 'X' for a in chain_seq])\n",
    "                chain_length = len(chain_seq)\n",
    "                global_idx_start_list.append(global_idx_start_list[-1]+chain_length)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.zeros(chain_length) #0.0 for visible chains\n",
    "                if ca_only:\n",
    "                    x_chain = np.array(chain_coords[f'CA_chain_{letter}']) #[chain_lenght,1,3] #CA_diff\n",
    "                    if len(x_chain.shape) == 2:\n",
    "                        x_chain = x_chain[:,None,:]\n",
    "                else:\n",
    "                    x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]\n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "                fixed_position_mask = np.ones(chain_length)\n",
    "                fixed_position_mask_list.append(fixed_position_mask)\n",
    "                omit_AA_mask_temp = np.zeros([chain_length, len(alphabet)], np.int32)\n",
    "                #print(\"-------------------1\",omit_AA_mask_temp.shape)\n",
    "                omit_AA_mask_list.append(omit_AA_mask_temp)\n",
    "                pssm_coef = np.zeros(chain_length)\n",
    "                pssm_bias = np.zeros([chain_length, 21])\n",
    "                pssm_log_odds = 10000.0*np.ones([chain_length, 21])\n",
    "                pssm_coef_list.append(pssm_coef)\n",
    "                pssm_bias_list.append(pssm_bias)\n",
    "                pssm_log_odds_list.append(pssm_log_odds)\n",
    "                bias_by_res_list.append(np.zeros([chain_length, 21]))\n",
    "            if letter in masked_chains:\n",
    "                masked_list.append(letter)\n",
    "                letter_list.append(letter)\n",
    "                #print(b)\n",
    "                chain_seq = b[f'seq_chain_{letter}']\n",
    "                chain_seq = ''.join([a if a!='-' else 'X' for a in chain_seq])\n",
    "                chain_length = len(chain_seq)\n",
    "                global_idx_start_list.append(global_idx_start_list[-1]+chain_length)\n",
    "                masked_chain_length_list.append(chain_length)\n",
    "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
    "                chain_mask = np.ones(chain_length) #1.0 for masked\n",
    "                if ca_only:\n",
    "                    x_chain = np.array(chain_coords[f'CA_chain_{letter}']) #[chain_lenght,1,3] #CA_diff\n",
    "                    if len(x_chain.shape) == 2:\n",
    "                        x_chain = x_chain[:,None,:]\n",
    "                else:\n",
    "                    x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]               \n",
    "                x_chain_list.append(x_chain)\n",
    "                chain_mask_list.append(chain_mask)\n",
    "                chain_seq_list.append(chain_seq)\n",
    "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
    "                l1 += chain_length\n",
    "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
    "                l0 += chain_length\n",
    "                c+=1\n",
    "                fixed_position_mask = np.ones(chain_length)\n",
    "                if fixed_position_dict!=None:\n",
    "                    fixed_pos_list = fixed_position_dict[b['name']][letter]\n",
    "                    if fixed_pos_list:\n",
    "                        fixed_position_mask[np.array(fixed_pos_list)-1] = 0.0\n",
    "                fixed_position_mask_list.append(fixed_position_mask)\n",
    "                omit_AA_mask_temp = np.zeros([chain_length, len(alphabet)], np.int32)\n",
    "                #print(\"--------------------\",omit_AA_mask_temp.shape)\n",
    "                if omit_AA_dict!=None:\n",
    "                    for item in omit_AA_dict[b['name']][letter]:\n",
    "                        idx_AA = np.array(item[0])-1\n",
    "                        AA_idx = np.array([np.argwhere(np.array(list(alphabet))== AA)[0][0] for AA in item[1]]).repeat(idx_AA.shape[0])\n",
    "                        idx_ = np.array([[a, b] for a in idx_AA for b in AA_idx])\n",
    "                        omit_AA_mask_temp[idx_[:,0], idx_[:,1]] = 1\n",
    "                omit_AA_mask_list.append(omit_AA_mask_temp)\n",
    "                pssm_coef = np.zeros(chain_length)\n",
    "                pssm_bias = np.zeros([chain_length, 21])\n",
    "                pssm_log_odds = 10000.0*np.ones([chain_length, 21])\n",
    "                if pssm_dict:\n",
    "                    if pssm_dict[b['name']][letter]:\n",
    "                        pssm_coef = pssm_dict[b['name']][letter]['pssm_coef']\n",
    "                        pssm_bias = pssm_dict[b['name']][letter]['pssm_bias']\n",
    "                        pssm_log_odds = pssm_dict[b['name']][letter]['pssm_log_odds']\n",
    "                pssm_coef_list.append(pssm_coef)\n",
    "                pssm_bias_list.append(pssm_bias)\n",
    "                pssm_log_odds_list.append(pssm_log_odds)\n",
    "                if bias_by_res_dict:\n",
    "                    bias_by_res_list.append(bias_by_res_dict[b['name']][letter])\n",
    "                else:\n",
    "                    bias_by_res_list.append(np.zeros([chain_length, 21]))\n",
    "\n",
    "       \n",
    "        letter_list_np = np.array(letter_list)\n",
    "        tied_pos_list_of_lists = []\n",
    "        tied_beta = np.ones(L_max)\n",
    "        if tied_positions_dict!=None:\n",
    "            tied_pos_list = tied_positions_dict[b['name']]\n",
    "            if tied_pos_list:\n",
    "                set_chains_tied = set(list(itertools.chain(*[list(item) for item in tied_pos_list])))\n",
    "                for tied_item in tied_pos_list:\n",
    "                    one_list = []\n",
    "                    for k, v in tied_item.items():\n",
    "                        start_idx = global_idx_start_list[np.argwhere(letter_list_np == k)[0][0]]\n",
    "                        if isinstance(v[0], list):\n",
    "                            for v_count in range(len(v[0])):\n",
    "                                one_list.append(start_idx+v[0][v_count]-1)#make 0 to be the first\n",
    "                                tied_beta[start_idx+v[0][v_count]-1] = v[1][v_count]\n",
    "                        else:\n",
    "                            for v_ in v:\n",
    "                                one_list.append(start_idx+v_-1)#make 0 to be the first\n",
    "                    tied_pos_list_of_lists.append(one_list)\n",
    "        tied_pos_list_of_lists_list.append(tied_pos_list_of_lists)\n",
    "\n",
    "\n",
    " \n",
    "        x = np.concatenate(x_chain_list,0) #[L, 4, 3]\n",
    "        all_sequence = \"\".join(chain_seq_list)\n",
    "        m = np.concatenate(chain_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "        chain_encoding = np.concatenate(chain_encoding_list,0)\n",
    "        m_pos = np.concatenate(fixed_position_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "\n",
    "        pssm_coef_ = np.concatenate(pssm_coef_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "        pssm_bias_ = np.concatenate(pssm_bias_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "        pssm_log_odds_ = np.concatenate(pssm_log_odds_list,0) #[L,], 1.0 for places that need to be predicted\n",
    "\n",
    "        bias_by_res_ = np.concatenate(bias_by_res_list, 0)  #[L,21], 0.0 for places where AA frequencies don't need to be tweaked\n",
    "\n",
    "        l = len(all_sequence)\n",
    "        x_pad = np.pad(x, [[0,L_max-l], [0,0], [0,0]], 'constant', constant_values=(np.nan, ))\n",
    "        X[i,:,:,:] = x_pad\n",
    "\n",
    "        m_pad = np.pad(m, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        m_pos_pad = np.pad(m_pos, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        \n",
    "        #print(np.concatenate(omit_AA_mask_list,0).shape)\n",
    "        \n",
    "        omit_AA_mask_pad = np.concatenate(omit_AA_mask_list,0)#np.pad(np.concatenate(omit_AA_mask_list,0), [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_M[i,:] = m_pad\n",
    "        chain_M_pos[i,:] = m_pos_pad\n",
    "        #print(omit_AA_mask.shape,omit_AA_mask_pad.shape)\n",
    "        omit_AA_mask[i,:l] = omit_AA_mask_pad\n",
    "\n",
    "        chain_encoding_pad = np.pad(chain_encoding, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        chain_encoding_all[i,:] = chain_encoding_pad\n",
    "\n",
    "        pssm_coef_pad = np.pad(pssm_coef_, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
    "        pssm_bias_pad = np.pad(pssm_bias_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))\n",
    "        pssm_log_odds_pad = np.pad(pssm_log_odds_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))\n",
    "\n",
    "        pssm_coef_all[i,:] = pssm_coef_pad\n",
    "        pssm_bias_all[i,:] = pssm_bias_pad\n",
    "        pssm_log_odds_all[i,:] = pssm_log_odds_pad\n",
    "\n",
    "        bias_by_res_pad = np.pad(bias_by_res_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))\n",
    "        bias_by_res_all[i,:] = bias_by_res_pad\n",
    "\n",
    "        # Convert to labels\n",
    "        indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "        S[i, :l] = indices\n",
    "        letter_list_list.append(letter_list)\n",
    "        visible_list_list.append(visible_list)\n",
    "        masked_list_list.append(masked_list)\n",
    "        masked_chain_length_list_list.append(masked_chain_length_list)\n",
    "\n",
    "\n",
    "    isnan = np.isnan(X)\n",
    "    mask = np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n",
    "    X[isnan] = 0.\n",
    "\n",
    "    # Conversion\n",
    "    pssm_coef_all = torch.from_numpy(pssm_coef_all).to(dtype=torch.float32, device=device)\n",
    "    pssm_bias_all = torch.from_numpy(pssm_bias_all).to(dtype=torch.float32, device=device)\n",
    "    pssm_log_odds_all = torch.from_numpy(pssm_log_odds_all).to(dtype=torch.float32, device=device)\n",
    "\n",
    "    tied_beta = torch.from_numpy(tied_beta).to(dtype=torch.float32, device=device)\n",
    "\n",
    "    jumps = ((residue_idx[:,1:]-residue_idx[:,:-1])==1).astype(np.float32)\n",
    "    bias_by_res_all = torch.from_numpy(bias_by_res_all).to(dtype=torch.float32, device=device)\n",
    "    phi_mask = np.pad(jumps, [[0,0],[1,0]])\n",
    "    psi_mask = np.pad(jumps, [[0,0],[0,1]])\n",
    "    omega_mask = np.pad(jumps, [[0,0],[0,1]])\n",
    "    dihedral_mask = np.concatenate([phi_mask[:,:,None], psi_mask[:,:,None], omega_mask[:,:,None]], -1) #[B,L,3]\n",
    "    dihedral_mask = torch.from_numpy(dihedral_mask).to(dtype=torch.float32, device=device)\n",
    "    residue_idx = torch.from_numpy(residue_idx).to(dtype=torch.long,device=device)\n",
    "    S = torch.from_numpy(S).to(dtype=torch.long,device=device)\n",
    "    X = torch.from_numpy(X).to(dtype=torch.float32, device=device)\n",
    "    mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
    "    chain_M = torch.from_numpy(chain_M).to(dtype=torch.float32, device=device)\n",
    "    chain_M_pos = torch.from_numpy(chain_M_pos).to(dtype=torch.float32, device=device)\n",
    "    omit_AA_mask = torch.from_numpy(omit_AA_mask).to(dtype=torch.float32, device=device)\n",
    "    chain_encoding_all = torch.from_numpy(chain_encoding_all).to(dtype=torch.long, device=device)\n",
    "    if ca_only:\n",
    "        X_out = X[:,:,0]\n",
    "    else:\n",
    "        X_out = X\n",
    "    return X_out, S, mask, lengths, chain_M, chain_encoding_all, letter_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef_all, pssm_bias_all, pssm_log_odds_all, bias_by_res_all, tied_beta\n",
    "\n",
    "\n",
    "\n",
    "def loss_nll(S, log_probs, mask):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    criterion = torch.nn.NLLLoss(reduction='none')\n",
    "    loss = criterion(\n",
    "        log_probs.contiguous().view(-1, log_probs.size(-1)), S.contiguous().view(-1)\n",
    "    ).view(S.size())\n",
    "    loss_av = torch.sum(loss * mask) / torch.sum(mask)\n",
    "    return loss, loss_av\n",
    "\n",
    "\n",
    "def loss_smoothed(S, log_probs, mask, weight=0.1):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    S_onehot = torch.nn.functional.one_hot(S, 21).float()\n",
    "\n",
    "    # Label smoothing\n",
    "    S_onehot = S_onehot + weight / float(S_onehot.size(-1))\n",
    "    S_onehot = S_onehot / S_onehot.sum(-1, keepdim=True)\n",
    "\n",
    "    loss = -(S_onehot * log_probs).sum(-1)\n",
    "    loss_av = torch.sum(loss * mask) / torch.sum(mask)\n",
    "    return loss, loss_av\n",
    "\n",
    "class StructureDataset():\n",
    "    def __init__(self, jsonl_file, verbose=True, truncate=None, max_length=100,\n",
    "        alphabet='ACDEFGHIKLMNPQRSTVWYX-'):\n",
    "        alphabet_set = set([a for a in alphabet])\n",
    "        discard_count = {\n",
    "            'bad_chars': 0,\n",
    "            'too_long': 0,\n",
    "            'bad_seq_length': 0\n",
    "        }\n",
    "\n",
    "        with open(jsonl_file) as f:\n",
    "            self.data = []\n",
    "\n",
    "            lines = f.readlines()\n",
    "            start = time.time()\n",
    "            for i, line in enumerate(lines):\n",
    "                entry = json.loads(line)\n",
    "                seq = entry['seq'] \n",
    "                name = entry['name']\n",
    "\n",
    "                # Convert raw coords to np arrays\n",
    "                #for key, val in entry['coords'].items():\n",
    "                #    entry['coords'][key] = np.asarray(val)\n",
    "\n",
    "                # Check if in alphabet\n",
    "                bad_chars = set([s for s in seq]).difference(alphabet_set)\n",
    "                if len(bad_chars) == 0:\n",
    "                    if len(entry['seq']) <= max_length:\n",
    "                        if True:\n",
    "                            self.data.append(entry)\n",
    "                        else:\n",
    "                            discard_count['bad_seq_length'] += 1\n",
    "                    else:\n",
    "                        discard_count['too_long'] += 1\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(name, bad_chars, entry['seq'])\n",
    "                    discard_count['bad_chars'] += 1\n",
    "\n",
    "                # Truncate early\n",
    "                if truncate is not None and len(self.data) == truncate:\n",
    "                    return\n",
    "\n",
    "                if verbose and (i + 1) % 1000 == 0:\n",
    "                    elapsed = time.time() - start\n",
    "                    print('{} entries ({} loaded) in {:.1f} s'.format(len(self.data), i+1, elapsed))\n",
    "            if verbose:\n",
    "                print('discarded', discard_count)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\n",
    "class StructureDatasetPDB():\n",
    "    def __init__(self, pdb_dict_list, verbose=True, truncate=None, max_length=100,\n",
    "        alphabet='ACDEFGHIKLMNPQRSTVWYX-'):\n",
    "        alphabet_set = set([a for a in alphabet])\n",
    "        discard_count = {\n",
    "            'bad_chars': 0,\n",
    "            'too_long': 0,\n",
    "            'bad_seq_length': 0\n",
    "        }\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        start = time.time()\n",
    "        for i, entry in enumerate(pdb_dict_list):\n",
    "            seq = entry['seq']\n",
    "            name = entry['name']\n",
    "\n",
    "            bad_chars = set([s for s in seq]).difference(alphabet_set)\n",
    "            if len(bad_chars) == 0:\n",
    "                if len(entry['seq']) <= max_length:\n",
    "                    self.data.append(entry)\n",
    "                else:\n",
    "                    discard_count['too_long'] += 1\n",
    "            else:\n",
    "                discard_count['bad_chars'] += 1\n",
    "\n",
    "            # Truncate early\n",
    "            if truncate is not None and len(self.data) == truncate:\n",
    "                return\n",
    "\n",
    "            if verbose and (i + 1) % 1000 == 0:\n",
    "                elapsed = time.time() - start\n",
    "\n",
    "            #print('Discarded', discard_count)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "    \n",
    "class StructureLoader():\n",
    "    def __init__(self, dataset, batch_size=100, shuffle=True,\n",
    "        collate_fn=lambda x:x, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.size = len(dataset)\n",
    "        self.lengths = [len(dataset[i]['seq']) for i in range(self.size)]\n",
    "        self.batch_size = batch_size\n",
    "        sorted_ix = np.argsort(self.lengths)\n",
    "\n",
    "        # Cluster into batches of similar sizes\n",
    "        clusters, batch = [], []\n",
    "        batch_max = 0\n",
    "        for ix in sorted_ix:\n",
    "            size = self.lengths[ix]\n",
    "            if size * (len(batch) + 1) <= self.batch_size:\n",
    "                batch.append(ix)\n",
    "                batch_max = size\n",
    "            else:\n",
    "                clusters.append(batch)\n",
    "                batch, batch_max = [], 0\n",
    "        if len(batch) > 0:\n",
    "            clusters.append(batch)\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clusters)\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.clusters)\n",
    "        for b_idx in self.clusters:\n",
    "            batch = [self.dataset[i] for i in b_idx]\n",
    "            yield batch\n",
    "            \n",
    "            \n",
    "            \n",
    "# The following gather functions\n",
    "def gather_edges(edges, neighbor_idx):\n",
    "    # Features [B,N,N,C] at Neighbor indices [B,N,K] => Neighbor features [B,N,K,C]\n",
    "    neighbors = neighbor_idx.unsqueeze(-1).expand(-1, -1, -1, edges.size(-1))\n",
    "    edge_features = torch.gather(edges, 2, neighbors)\n",
    "    return edge_features\n",
    "\n",
    "def gather_nodes(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n",
    "    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n",
    "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n",
    "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    # Gather and re-pack\n",
    "    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n",
    "    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n",
    "    return neighbor_features\n",
    "\n",
    "def gather_nodes_t(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor index [B,K] => Neighbor features[B,K,C]\n",
    "    idx_flat = neighbor_idx.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    neighbor_features = torch.gather(nodes, 1, idx_flat)\n",
    "    return neighbor_features\n",
    "\n",
    "def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n",
    "    h_nodes = gather_nodes(h_nodes, E_idx)\n",
    "    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n",
    "    return h_nn\n",
    "\n",
    "\n",
    "class EncLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(EncLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.norm3 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W11 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W13 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, E_idx, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / self.scale\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
    "        h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))\n",
    "        h_E = self.norm3(h_E + self.dropout3(h_message))\n",
    "        return h_V, h_E\n",
    "\n",
    "\n",
    "class DecLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(DecLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "\n",
    "        # Concatenate h_V_i to h_E_ij\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_E.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_E], -1)\n",
    "\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / self.scale\n",
    "\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "\n",
    "        # Position-wise feedforward\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "        return h_V \n",
    "\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, num_hidden, num_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.W_in = nn.Linear(num_hidden, num_ff, bias=True)\n",
    "        self.W_out = nn.Linear(num_ff, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "    def forward(self, h_V):\n",
    "        h = self.act(self.W_in(h_V))\n",
    "        h = self.W_out(h)\n",
    "        return h\n",
    "\n",
    "class PositionalEncodings(nn.Module):\n",
    "    def __init__(self, num_embeddings, max_relative_feature=32):\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.max_relative_feature = max_relative_feature\n",
    "        self.linear = nn.Linear(2*max_relative_feature+1+1, num_embeddings)\n",
    "\n",
    "    def forward(self, offset, mask):\n",
    "        d = torch.clip(offset + self.max_relative_feature, 0, 2*self.max_relative_feature)*mask + (1-mask)*(2*self.max_relative_feature+1)\n",
    "        d_onehot = torch.nn.functional.one_hot(d, 2*self.max_relative_feature+1+1)\n",
    "        E = self.linear(d_onehot.float())\n",
    "        return E\n",
    "\n",
    "\n",
    "\n",
    "class CA_ProteinFeatures(nn.Module):\n",
    "    def __init__(self, edge_features, node_features, num_positional_embeddings=16,\n",
    "        num_rbf=16, top_k=30, augment_eps=0., num_chain_embeddings=16):\n",
    "        \"\"\" Extract protein features \"\"\"\n",
    "        super(CA_ProteinFeatures, self).__init__()\n",
    "        self.edge_features = edge_features\n",
    "        self.node_features = node_features\n",
    "        self.top_k = top_k\n",
    "        self.augment_eps = augment_eps \n",
    "        self.num_rbf = num_rbf\n",
    "        self.num_positional_embeddings = num_positional_embeddings\n",
    "\n",
    "        # Positional encoding\n",
    "        self.embeddings = PositionalEncodings(num_positional_embeddings)\n",
    "        # Normalization and embedding\n",
    "        node_in, edge_in = 3, num_positional_embeddings + num_rbf*9 + 7\n",
    "        self.node_embedding = nn.Linear(node_in,  node_features, bias=False) #NOT USED\n",
    "        self.edge_embedding = nn.Linear(edge_in, edge_features, bias=False)\n",
    "        self.norm_nodes = nn.LayerNorm(node_features)\n",
    "        self.norm_edges = nn.LayerNorm(edge_features)\n",
    "\n",
    "\n",
    "    def _quaternions(self, R):\n",
    "        \"\"\" Convert a batch of 3D rotations [R] to quaternions [Q]\n",
    "            R [...,3,3]\n",
    "            Q [...,4]\n",
    "        \"\"\"\n",
    "        # Simple Wikipedia version\n",
    "        # en.wikipedia.org/wiki/Rotation_matrix#Quaternion\n",
    "        # For other options see math.stackexchange.com/questions/2074316/calculating-rotation-axis-from-rotation-matrix\n",
    "        diag = torch.diagonal(R, dim1=-2, dim2=-1)\n",
    "        Rxx, Ryy, Rzz = diag.unbind(-1)\n",
    "        magnitudes = 0.5 * torch.sqrt(torch.abs(1 + torch.stack([\n",
    "              Rxx - Ryy - Rzz, \n",
    "            - Rxx + Ryy - Rzz, \n",
    "            - Rxx - Ryy + Rzz\n",
    "        ], -1)))\n",
    "        _R = lambda i,j: R[:,:,:,i,j]\n",
    "        signs = torch.sign(torch.stack([\n",
    "            _R(2,1) - _R(1,2),\n",
    "            _R(0,2) - _R(2,0),\n",
    "            _R(1,0) - _R(0,1)\n",
    "        ], -1))\n",
    "        xyz = signs * magnitudes\n",
    "        # The relu enforces a non-negative trace\n",
    "        w = torch.sqrt(F.relu(1 + diag.sum(-1, keepdim=True))) / 2.\n",
    "        Q = torch.cat((xyz, w), -1)\n",
    "        Q = F.normalize(Q, dim=-1)\n",
    "        return Q\n",
    "\n",
    "    def _orientations_coarse(self, X, E_idx, eps=1e-6):\n",
    "        dX = X[:,1:,:] - X[:,:-1,:]\n",
    "        dX_norm = torch.norm(dX,dim=-1)\n",
    "        dX_mask = (3.6<dX_norm) & (dX_norm<4.0) #exclude CA-CA jumps\n",
    "        dX = dX*dX_mask[:,:,None]\n",
    "        U = F.normalize(dX, dim=-1)\n",
    "        u_2 = U[:,:-2,:]\n",
    "        u_1 = U[:,1:-1,:]\n",
    "        u_0 = U[:,2:,:]\n",
    "        # Backbone normals\n",
    "        n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n",
    "        n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n",
    "\n",
    "        # Bond angle calculation\n",
    "        cosA = -(u_1 * u_0).sum(-1)\n",
    "        cosA = torch.clamp(cosA, -1+eps, 1-eps)\n",
    "        A = torch.acos(cosA)\n",
    "        # Angle between normals\n",
    "        cosD = (n_2 * n_1).sum(-1)\n",
    "        cosD = torch.clamp(cosD, -1+eps, 1-eps)\n",
    "        D = torch.sign((u_2 * n_1).sum(-1)) * torch.acos(cosD)\n",
    "        # Backbone features\n",
    "        AD_features = torch.stack((torch.cos(A), torch.sin(A) * torch.cos(D), torch.sin(A) * torch.sin(D)), 2)\n",
    "        AD_features = F.pad(AD_features, (0,0,1,2), 'constant', 0)\n",
    "\n",
    "        # Build relative orientations\n",
    "        o_1 = F.normalize(u_2 - u_1, dim=-1)\n",
    "        O = torch.stack((o_1, n_2, torch.cross(o_1, n_2)), 2)\n",
    "        O = O.view(list(O.shape[:2]) + [9])\n",
    "        O = F.pad(O, (0,0,1,2), 'constant', 0)\n",
    "        O_neighbors = gather_nodes(O, E_idx)\n",
    "        X_neighbors = gather_nodes(X, E_idx)\n",
    "        \n",
    "        # Re-view as rotation matrices\n",
    "        O = O.view(list(O.shape[:2]) + [3,3])\n",
    "        O_neighbors = O_neighbors.view(list(O_neighbors.shape[:3]) + [3,3])\n",
    "\n",
    "        # Rotate into local reference frames\n",
    "        dX = X_neighbors - X.unsqueeze(-2)\n",
    "        dU = torch.matmul(O.unsqueeze(2), dX.unsqueeze(-1)).squeeze(-1)\n",
    "        dU = F.normalize(dU, dim=-1)\n",
    "        R = torch.matmul(O.unsqueeze(2).transpose(-1,-2), O_neighbors)\n",
    "        Q = self._quaternions(R)\n",
    "\n",
    "        # Orientation features\n",
    "        O_features = torch.cat((dU,Q), dim=-1)\n",
    "        return AD_features, O_features\n",
    "\n",
    "\n",
    "\n",
    "    def _dist(self, X, mask, eps=1E-6):\n",
    "        \"\"\" Pairwise euclidean distances \"\"\"\n",
    "        # Convolutional network on NCHW\n",
    "        mask_2D = torch.unsqueeze(mask,1) * torch.unsqueeze(mask,2)\n",
    "        dX = torch.unsqueeze(X,1) - torch.unsqueeze(X,2)\n",
    "        D = mask_2D * torch.sqrt(torch.sum(dX**2, 3) + eps)\n",
    "\n",
    "        # Identify k nearest neighbors (including self)\n",
    "        D_max, _ = torch.max(D, -1, keepdim=True)\n",
    "        D_adjust = D + (1. - mask_2D) * D_max\n",
    "        D_neighbors, E_idx = torch.topk(D_adjust, np.minimum(self.top_k, X.shape[1]), dim=-1, largest=False)\n",
    "        mask_neighbors = gather_edges(mask_2D.unsqueeze(-1), E_idx)\n",
    "        return D_neighbors, E_idx, mask_neighbors\n",
    "\n",
    "    def _rbf(self, D):\n",
    "        # Distance radial basis function\n",
    "        device = D.device\n",
    "        D_min, D_max, D_count = 2., 22., self.num_rbf\n",
    "        D_mu = torch.linspace(D_min, D_max, D_count).to(device)\n",
    "        D_mu = D_mu.view([1,1,1,-1])\n",
    "        D_sigma = (D_max - D_min) / D_count\n",
    "        D_expand = torch.unsqueeze(D, -1)\n",
    "        RBF = torch.exp(-((D_expand - D_mu) / D_sigma)**2)\n",
    "        return RBF\n",
    "\n",
    "    def _get_rbf(self, A, B, E_idx):\n",
    "        D_A_B = torch.sqrt(torch.sum((A[:,:,None,:] - B[:,None,:,:])**2,-1) + 1e-6) #[B, L, L]\n",
    "        D_A_B_neighbors = gather_edges(D_A_B[:,:,:,None], E_idx)[:,:,:,0] #[B,L,K]\n",
    "        RBF_A_B = self._rbf(D_A_B_neighbors)\n",
    "        return RBF_A_B\n",
    "\n",
    "    def forward(self, Ca, mask, residue_idx, chain_labels):\n",
    "        \"\"\" Featurize coordinates as an attributed graph \"\"\"\n",
    "        if self.augment_eps > 0:\n",
    "            Ca = Ca + self.augment_eps * torch.randn_like(Ca)\n",
    "\n",
    "        D_neighbors, E_idx, mask_neighbors = self._dist(Ca, mask)\n",
    "\n",
    "        Ca_0 = torch.zeros(Ca.shape, device=Ca.device)\n",
    "        Ca_2 = torch.zeros(Ca.shape, device=Ca.device)\n",
    "        Ca_0[:,1:,:] = Ca[:,:-1,:]\n",
    "        Ca_1 = Ca\n",
    "        Ca_2[:,:-1,:] = Ca[:,1:,:]\n",
    "\n",
    "        V, O_features = self._orientations_coarse(Ca, E_idx)\n",
    "        \n",
    "        RBF_all = []\n",
    "        RBF_all.append(self._rbf(D_neighbors)) #Ca_1-Ca_1\n",
    "        RBF_all.append(self._get_rbf(Ca_0, Ca_0, E_idx)) \n",
    "        RBF_all.append(self._get_rbf(Ca_2, Ca_2, E_idx))\n",
    "\n",
    "        RBF_all.append(self._get_rbf(Ca_0, Ca_1, E_idx))\n",
    "        RBF_all.append(self._get_rbf(Ca_0, Ca_2, E_idx))\n",
    "\n",
    "        RBF_all.append(self._get_rbf(Ca_1, Ca_0, E_idx))\n",
    "        RBF_all.append(self._get_rbf(Ca_1, Ca_2, E_idx))\n",
    "\n",
    "        RBF_all.append(self._get_rbf(Ca_2, Ca_0, E_idx))\n",
    "        RBF_all.append(self._get_rbf(Ca_2, Ca_1, E_idx))\n",
    "\n",
    "\n",
    "        RBF_all = torch.cat(tuple(RBF_all), dim=-1)\n",
    "\n",
    "\n",
    "        offset = residue_idx[:,:,None]-residue_idx[:,None,:]\n",
    "        offset = gather_edges(offset[:,:,:,None], E_idx)[:,:,:,0] #[B, L, K]\n",
    "\n",
    "        d_chains = ((chain_labels[:, :, None] - chain_labels[:,None,:])==0).long()\n",
    "        E_chains = gather_edges(d_chains[:,:,:,None], E_idx)[:,:,:,0]\n",
    "        E_positional = self.embeddings(offset.long(), E_chains)\n",
    "        E = torch.cat((E_positional, RBF_all, O_features), -1)\n",
    "        \n",
    "\n",
    "        E = self.edge_embedding(E)\n",
    "        E = self.norm_edges(E)\n",
    "        \n",
    "        return E, E_idx \n",
    "\n",
    "\n",
    "class ProteinFeatures(nn.Module):\n",
    "    def __init__(self, edge_features, node_features, num_positional_embeddings=16,\n",
    "        num_rbf=16, top_k=30, augment_eps=0., num_chain_embeddings=16):\n",
    "        \"\"\" Extract protein features \"\"\"\n",
    "        super(ProteinFeatures, self).__init__()\n",
    "        self.edge_features = edge_features\n",
    "        self.node_features = node_features\n",
    "        self.top_k = top_k\n",
    "        self.augment_eps = augment_eps \n",
    "        self.num_rbf = num_rbf\n",
    "        self.num_positional_embeddings = num_positional_embeddings\n",
    "\n",
    "        self.embeddings = PositionalEncodings(num_positional_embeddings)\n",
    "        node_in, edge_in = 6, num_positional_embeddings + num_rbf*25\n",
    "        self.edge_embedding = nn.Linear(edge_in, edge_features, bias=False)\n",
    "        self.norm_edges = nn.LayerNorm(edge_features)\n",
    "\n",
    "    def _dist(self, X, mask, eps=1E-6):\n",
    "        mask_2D = torch.unsqueeze(mask,1) * torch.unsqueeze(mask,2)\n",
    "        dX = torch.unsqueeze(X,1) - torch.unsqueeze(X,2)\n",
    "        D = mask_2D * torch.sqrt(torch.sum(dX**2, 3) + eps)\n",
    "        D_max, _ = torch.max(D, -1, keepdim=True)\n",
    "        D_adjust = D + (1. - mask_2D) * D_max\n",
    "        sampled_top_k = self.top_k\n",
    "        D_neighbors, E_idx = torch.topk(D_adjust, np.minimum(self.top_k, X.shape[1]), dim=-1, largest=False)\n",
    "        return D_neighbors, E_idx\n",
    "\n",
    "    def _rbf(self, D):\n",
    "        device = D.device\n",
    "        D_min, D_max, D_count = 2., 22., self.num_rbf\n",
    "        D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "        D_mu = D_mu.view([1,1,1,-1])\n",
    "        D_sigma = (D_max - D_min) / D_count\n",
    "        D_expand = torch.unsqueeze(D, -1)\n",
    "        RBF = torch.exp(-((D_expand - D_mu) / D_sigma)**2)\n",
    "        return RBF\n",
    "\n",
    "    def _get_rbf(self, A, B, E_idx):\n",
    "        D_A_B = torch.sqrt(torch.sum((A[:,:,None,:] - B[:,None,:,:])**2,-1) + 1e-6) #[B, L, L]\n",
    "        D_A_B_neighbors = gather_edges(D_A_B[:,:,:,None], E_idx)[:,:,:,0] #[B,L,K]\n",
    "        RBF_A_B = self._rbf(D_A_B_neighbors)\n",
    "        return RBF_A_B\n",
    "\n",
    "    def forward(self, X, mask, residue_idx, chain_labels):\n",
    "        if self.augment_eps > 0:\n",
    "            X = X + self.augment_eps * torch.randn_like(X)\n",
    "        \n",
    "        b = X[:,:,1,:] - X[:,:,0,:]\n",
    "        c = X[:,:,2,:] - X[:,:,1,:]\n",
    "        a = torch.cross(b, c, dim=-1)\n",
    "        Cb = -0.58273431*a + 0.56802827*b - 0.54067466*c + X[:,:,1,:]\n",
    "        Ca = X[:,:,1,:]\n",
    "        N = X[:,:,0,:]\n",
    "        C = X[:,:,2,:]\n",
    "        O = X[:,:,3,:]\n",
    " \n",
    "        D_neighbors, E_idx = self._dist(Ca, mask)\n",
    "\n",
    "        RBF_all = []\n",
    "        RBF_all.append(self._rbf(D_neighbors)) #Ca-Ca\n",
    "        RBF_all.append(self._get_rbf(N, N, E_idx)) #N-N\n",
    "        RBF_all.append(self._get_rbf(C, C, E_idx)) #C-C\n",
    "        RBF_all.append(self._get_rbf(O, O, E_idx)) #O-O\n",
    "        RBF_all.append(self._get_rbf(Cb, Cb, E_idx)) #Cb-Cb\n",
    "        RBF_all.append(self._get_rbf(Ca, N, E_idx)) #Ca-N\n",
    "        RBF_all.append(self._get_rbf(Ca, C, E_idx)) #Ca-C\n",
    "        RBF_all.append(self._get_rbf(Ca, O, E_idx)) #Ca-O\n",
    "        RBF_all.append(self._get_rbf(Ca, Cb, E_idx)) #Ca-Cb\n",
    "        RBF_all.append(self._get_rbf(N, C, E_idx)) #N-C\n",
    "        RBF_all.append(self._get_rbf(N, O, E_idx)) #N-O\n",
    "        RBF_all.append(self._get_rbf(N, Cb, E_idx)) #N-Cb\n",
    "        RBF_all.append(self._get_rbf(Cb, C, E_idx)) #Cb-C\n",
    "        RBF_all.append(self._get_rbf(Cb, O, E_idx)) #Cb-O\n",
    "        RBF_all.append(self._get_rbf(O, C, E_idx)) #O-C\n",
    "        RBF_all.append(self._get_rbf(N, Ca, E_idx)) #N-Ca\n",
    "        RBF_all.append(self._get_rbf(C, Ca, E_idx)) #C-Ca\n",
    "        RBF_all.append(self._get_rbf(O, Ca, E_idx)) #O-Ca\n",
    "        RBF_all.append(self._get_rbf(Cb, Ca, E_idx)) #Cb-Ca\n",
    "        RBF_all.append(self._get_rbf(C, N, E_idx)) #C-N\n",
    "        RBF_all.append(self._get_rbf(O, N, E_idx)) #O-N\n",
    "        RBF_all.append(self._get_rbf(Cb, N, E_idx)) #Cb-N\n",
    "        RBF_all.append(self._get_rbf(C, Cb, E_idx)) #C-Cb\n",
    "        RBF_all.append(self._get_rbf(O, Cb, E_idx)) #O-Cb\n",
    "        RBF_all.append(self._get_rbf(C, O, E_idx)) #C-O\n",
    "        RBF_all = torch.cat(tuple(RBF_all), dim=-1)\n",
    "\n",
    "        offset = residue_idx[:,:,None]-residue_idx[:,None,:]\n",
    "        offset = gather_edges(offset[:,:,:,None], E_idx)[:,:,:,0] #[B, L, K]\n",
    "\n",
    "        d_chains = ((chain_labels[:, :, None] - chain_labels[:,None,:])==0).long() #find self vs non-self interaction\n",
    "        E_chains = gather_edges(d_chains[:,:,:,None], E_idx)[:,:,:,0]\n",
    "        E_positional = self.embeddings(offset.long(), E_chains)\n",
    "        E = torch.cat((E_positional, RBF_all), -1)\n",
    "        E = self.edge_embedding(E)\n",
    "        E = self.norm_edges(E)\n",
    "        return E, E_idx \n",
    "\n",
    "class ProteinMPNN(nn.Module):\n",
    "    def __init__(self, num_letters, node_features, edge_features,\n",
    "        hidden_dim, num_encoder_layers=3, num_decoder_layers=3,\n",
    "        vocab=21, k_neighbors=64, augment_eps=0.05, dropout=0.1, ca_only=False):\n",
    "        super(ProteinMPNN, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.node_features = node_features\n",
    "        self.edge_features = edge_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Featurization layers\n",
    "        if ca_only:\n",
    "            self.features = CA_ProteinFeatures(node_features, edge_features, top_k=k_neighbors, augment_eps=augment_eps)\n",
    "            self.W_v = nn.Linear(node_features, hidden_dim, bias=True)\n",
    "        else:\n",
    "            self.features = ProteinFeatures(node_features, edge_features, top_k=k_neighbors, augment_eps=augment_eps)\n",
    "\n",
    "        self.W_e = nn.Linear(edge_features, hidden_dim, bias=True)\n",
    "        self.W_s = nn.Embedding(vocab, hidden_dim)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncLayer(hidden_dim, hidden_dim*2, dropout=dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecLayer(hidden_dim, hidden_dim*3, dropout=dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.W_out = nn.Linear(hidden_dim, num_letters, bias=True)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, X, S, mask, chain_M, residue_idx, chain_encoding_all, randn, \\\n",
    "                use_input_decoding_order=False, decoding_order=None,num_d = 2):\n",
    "        \"\"\" Graph-conditioned sequence model \"\"\"\n",
    "        device=X.device\n",
    "         \n",
    "        # Prepare node and edge embeddings\n",
    "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
    "        #print(\"forward,E,E_idx\",E.shape, E_idx.shape)\n",
    "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device)\n",
    "        h_E = self.W_e(E)\n",
    "\n",
    "        # Encoder is unmasked self-attention\n",
    "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
    "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
    "        for layer in self.encoder_layers:\n",
    "            h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)\n",
    "\n",
    "        # Concatenate sequence embeddings for autoregressive decoder\n",
    "        h_S = self.W_s(S)\n",
    "        #print(\"forward h_S\",h_S.shape)\n",
    "        h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)\n",
    "        #print(\"forward h_ES\",h_ES.shape)\n",
    "        # Build encoder embeddings\n",
    "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
    "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
    "\n",
    "\n",
    "        chain_M = chain_M*mask #update chain_M to include missing regions\n",
    "        if not use_input_decoding_order:\n",
    "            decoding_order = torch.argsort((chain_M+0.0001)*(torch.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
    "        mask_size = E_idx.shape[1]\n",
    "        permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
    "        order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
    "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
    "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
    "        mask_bw = mask_1D * mask_attend\n",
    "        mask_fw = mask_1D * (1. - mask_attend)\n",
    "\n",
    "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
    "        for layer in self.decoder_layers:\n",
    "            # Masked positions attend to encoder information, unmasked see. \n",
    "            h_ESV = cat_neighbors_nodes(h_V, h_ES, E_idx)\n",
    "            h_ESV = mask_bw * h_ESV + h_EXV_encoder_fw\n",
    "            h_V = layer(h_V, h_ESV, mask)\n",
    "\n",
    "        logits = self.W_out(h_V)\n",
    "        #print(\"logits.shape is: \",logits.shape)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "\n",
    "    def sample(self, X, randn, S_true, chain_mask, chain_encoding_all, residue_idx, mask=None, \n",
    "               temperature=1.0, omit_AAs_np=None, bias_AAs_np=None, chain_M_pos=None, omit_AA_mask=None, \n",
    "               pssm_coef=None, pssm_bias=None, pssm_multi=None, pssm_log_odds_flag=None, \n",
    "               pssm_log_odds_mask=None, pssm_bias_flag=None, bias_by_res=None,num_d = 2):\n",
    "        device = X.device\n",
    "        # Prepare node and edge embeddings\n",
    "        #print(X.shape)\n",
    "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
    "        #print(E.shape, E_idx.shape)\n",
    "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=device)\n",
    "        #print(\"h_V\",h_V.shape)\n",
    "        h_E = self.W_e(E)\n",
    "        #print(\"h_E\",h_E.shape)\n",
    "        # Encoder is unmasked self-attention\n",
    "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
    "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
    "        for layer in self.encoder_layers:\n",
    "            h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)\n",
    "        #print(\"h_V\",h_V.shape,\"h_E\",h_E.shape)\n",
    "\n",
    "        # Decoder uses masked self-attention\n",
    "        chain_mask = chain_mask*chain_M_pos*mask #update chain_M to include missing regions\n",
    "        #for ns in range(len(chain_mask)-1):\n",
    "        chain_mask = torch.prod(chain_mask, dim=0)#chain_mask[0]*chain_mask[1]*chain_mask[2]\n",
    "        #print(chain_mask.sum())\n",
    "        chain_mask = chain_mask[None,:].repeat(num_d,1)\n",
    "        \n",
    "        \n",
    "        decoding_order = torch.argsort((chain_mask+0.0001)*(torch.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
    "         \n",
    "        decoding_order = decoding_order[0][None,:].repeat(num_d,1)\n",
    "        #print((chain_mask+0.0001)*(torch.abs(randn)))\n",
    "        mask_size = E_idx.shape[1]\n",
    "        permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
    "        order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
    "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
    "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
    "        mask_bw = mask_1D * mask_attend\n",
    "        mask_fw = mask_1D * (1. - mask_attend)\n",
    "\n",
    "        N_batch, N_nodes = X.size(0), X.size(1)\n",
    "        log_probs = torch.zeros((N_batch, N_nodes, 21), device=device)\n",
    "        all_probs = torch.zeros((N_batch, N_nodes, 21), device=device, dtype=torch.float32)\n",
    "        h_S = torch.zeros_like(h_V, device=device)\n",
    "        S = torch.zeros((N_batch, N_nodes), dtype=torch.int64, device=device)\n",
    "        h_V_stack = [h_V] + [torch.zeros_like(h_V, device=device) for _ in range(len(self.decoder_layers))]\n",
    "        constant = torch.tensor(omit_AAs_np, device=device)\n",
    "        constant_bias = torch.tensor(bias_AAs_np, device=device)\n",
    "        #chain_mask_combined = chain_mask*chain_M_pos \n",
    "        omit_AA_mask_flag = omit_AA_mask != None\n",
    "\n",
    "\n",
    "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
    "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
    "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder#b n k c\n",
    "        #print(chain_mask.shape,N_nodes)\n",
    "        for t_ in range(N_nodes):\n",
    "            t = decoding_order[:,t_] #[B]\n",
    "            #print(t)\n",
    "            chain_mask_gathered = torch.gather(chain_mask, 1, t[:,None]) #[B]\n",
    "            mask_gathered = torch.gather(mask, 1, t[:,None]) #[B]\n",
    "            bias_by_res_gathered = torch.gather(bias_by_res, 1, t[:,None,None].repeat(1,1,21))[:,0,:] #[B, 21]\n",
    "            if (mask_gathered==0).all(): #for padded or missing regions only\n",
    "                S_t = torch.gather(S_true, 1, t[:,None])\n",
    "            else:\n",
    "                # Hidden layers\n",
    "                E_idx_t = torch.gather(E_idx, 1, t[:,None,None].repeat(1,1,E_idx.shape[-1]))\n",
    "                h_E_t = torch.gather(h_E, 1, t[:,None,None,None].repeat(1,1,h_E.shape[-2], h_E.shape[-1]))\n",
    "                h_ES_t = cat_neighbors_nodes(h_S, h_E_t, E_idx_t)\n",
    "                h_EXV_encoder_t = torch.gather(h_EXV_encoder_fw, 1, t[:,None,None,None].repeat(1,1,h_EXV_encoder_fw.shape[-2], h_EXV_encoder_fw.shape[-1]))\n",
    "                mask_t = torch.gather(mask, 1, t[:,None])\n",
    "                for l, layer in enumerate(self.decoder_layers):\n",
    "                    # Updated relational features for future states\n",
    "                    h_ESV_decoder_t = cat_neighbors_nodes(h_V_stack[l], h_ES_t, E_idx_t)\n",
    "                    h_V_t = torch.gather(h_V_stack[l], 1, t[:,None,None].repeat(1,1,h_V_stack[l].shape[-1]))\n",
    "                    h_ESV_t = torch.gather(mask_bw, 1, t[:,None,None,None].repeat(1,1,mask_bw.shape[-2], mask_bw.shape[-1])) * h_ESV_decoder_t + h_EXV_encoder_t\n",
    "                    h_V_stack[l+1].scatter_(1, t[:,None,None].repeat(1,1,h_V.shape[-1]), layer(h_V_t, h_ESV_t, mask_V=mask_t))\n",
    "                # Sampling step\n",
    "                h_V_t = torch.gather(h_V_stack[-1], 1, t[:,None,None].repeat(1,1,h_V_stack[-1].shape[-1]))[:,0]\n",
    "                if (chain_M_pos*chain_M)[0,t[0]] == 1:\n",
    "                    h_V_t = h_V_t.mean(0)\n",
    "                    #print(h_V_t.shape)\n",
    "                    h_V_t = h_V_t[None,:].repeat(num_d,1)\n",
    "                    logits = self.W_out(h_V_t) / temperature\n",
    "                \n",
    "                else:\n",
    "                    logits = self.W_out(h_V_t) / temperature\n",
    "                probs = F.softmax(logits-constant[None,:]*1e8+constant_bias[None,:]/temperature+bias_by_res_gathered/temperature, dim=-1)\n",
    "                #if (chain_M_pos*chain_M)[0,t[0]] == 1:\n",
    "                    \n",
    "                    #probs = probs.mean(0)[None,:].repeat(3,1)\n",
    "                    #print(probs[0])\n",
    "                    #probs = F.softmax(probs,-1)\n",
    "                \n",
    "                \n",
    "                if pssm_bias_flag:\n",
    "                    pssm_coef_gathered = torch.gather(pssm_coef, 1, t[:,None])[:,0]\n",
    "                    pssm_bias_gathered = torch.gather(pssm_bias, 1, t[:,None,None].repeat(1,1,pssm_bias.shape[-1]))[:,0]\n",
    "                    probs = (1-pssm_multi*pssm_coef_gathered[:,None])*probs + pssm_multi*pssm_coef_gathered[:,None]*pssm_bias_gathered\n",
    "                if pssm_log_odds_flag:\n",
    "                    pssm_log_odds_mask_gathered = torch.gather(pssm_log_odds_mask, 1, t[:,None, None].repeat(1,1,pssm_log_odds_mask.shape[-1]))[:,0] #[B, 21]\n",
    "                    probs_masked = probs*pssm_log_odds_mask_gathered\n",
    "                    probs_masked += probs * 0.001\n",
    "                    probs = probs_masked/torch.sum(probs_masked, dim=-1, keepdim=True) #[B, 21]\n",
    "                if omit_AA_mask_flag:\n",
    "                    omit_AA_mask_gathered = torch.gather(omit_AA_mask, 1, t[:,None, None].repeat(1,1,omit_AA_mask.shape[-1]))[:,0] #[B, 21]\n",
    "                    probs_masked = probs*(1.0-omit_AA_mask_gathered)\n",
    "                    probs = probs_masked/torch.sum(probs_masked, dim=-1, keepdim=True) #[B, 21]\n",
    "                S_t = torch.multinomial(probs, 1)\n",
    "                #print(S_t.shape)\n",
    "                if (chain_M_pos*chain_M)[0,t[0]] == 1:\n",
    "                     \n",
    "                    S_t = torch.multinomial(probs, 1)[0][None,:].repeat(num_d,1) \n",
    "                     \n",
    "                \n",
    "                \n",
    "                all_probs.scatter_(1, t[:,None,None].repeat(1,1,21), (chain_mask_gathered[:,:,None,]*probs[:,None,:]).float())\n",
    "            S_true_gathered = torch.gather(S_true, 1, t[:,None])\n",
    "            S_t = (S_t*chain_mask_gathered+S_true_gathered*(1.0-chain_mask_gathered)).long()\n",
    "            temp1 = self.W_s(S_t)\n",
    "            h_S.scatter_(1, t[:,None,None].repeat(1,1,temp1.shape[-1]), temp1)\n",
    "            S.scatter_(1, t[:,None], S_t)\n",
    "        output_dict = {\"S\": S, \"probs\": all_probs, \"decoding_order\": decoding_order}\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07abbd1e-8d6a-45b3-85d4-f3a9563dc5df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_path = \"./test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484169be-8332-442d-95d1-46526777de2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python /dssg/home/acct-clsyzs/clsyzs/C1/ProteinMPNN_multi_re/helper_scripts/parse_multiple_chains.py --input_path=/dssg/home/acct-clsyzs/clsyzs/C1/ProteinMPNN_multi_re/test_data --output_path=./test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43615576-1fbd-476e-b0b6-ce9ea3132901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ddbbe70-3775-46fc-b8b7-2bbfa1fdab0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2 = {}\n",
    "data2[\"seq_chain_B\"] = data1['seq_chain_A']\n",
    "data2[\"seq_chain_A\"] = data1['seq_chain_B']\n",
    "dit1 = {}\n",
    "dit1[\"N_chain_A\"] = data1['coords_chain_B']['N_chain_B']\n",
    "dit1[\"CA_chain_A\"] = data1['coords_chain_B']['CA_chain_B']\n",
    "dit1[\"C_chain_A\"] = data1['coords_chain_B']['C_chain_B']\n",
    "dit1[\"O_chain_A\"] = data1['coords_chain_B']['O_chain_B']\n",
    "data2[\"coords_chain_A\"] = dit1\n",
    "dit1 = {}\n",
    "dit1[\"N_chain_B\"] = data1['coords_chain_A']['N_chain_A']\n",
    "dit1[\"CA_chain_B\"] = data1['coords_chain_A']['CA_chain_A']\n",
    "dit1[\"C_chain_B\"] = data1['coords_chain_A']['C_chain_A']\n",
    "dit1[\"O_chain_B\"] = data1['coords_chain_A']['O_chain_A']\n",
    "\n",
    "data2[\"coords_chain_B\"] = dit1\n",
    "\n",
    "data2[\"name\"] = data1[\"name\"]\n",
    "data2[\"num_of_chains\"] = data1[\"num_of_chains\"]\n",
    "data2[\"seq\"] = data1[\"seq\"]\n",
    "data2[\"masked_list\"] = [\"B\"]\n",
    "data2[\"visible_list\"] = [\"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3a38140-65b3-49f3-951d-31ba57abdb52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(data,\"3met_5tdr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6c46a6-2918-41bf-9045-9d7c7af66d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "# 打开文件并逐行读取\n",
    "with open(test_data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的JSON数据\n",
    "        data1 = json.loads(line)\n",
    "        data1[\"masked_list\"] = [\"B\"]\n",
    "        data1[\"visible_list\"] = [\"A\"]\n",
    "        data.append( data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59b9e53c-d81d-47f3-9a91-d17834e78152",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[1][\"seq_chain_B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c24595db-cfce-436e-9e22-542156b0cd39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0][\"seq_chain_B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b225d48-7987-4331-87a2-f6092ee47b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['seq_chain_A', 'coords_chain_A', 'seq_chain_B', 'coords_chain_B', 'seq_chain_G', 'coords_chain_G', 'seq_chain_N', 'coords_chain_N', 'seq_chain_R', 'coords_chain_R', 'name', 'num_of_chains', 'seq', 'masked_list', 'visible_list'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][\"seq_chain_A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c28a9ef-e808-4890-b059-e5266b2bfa5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [data[0],data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c415cbfc-a7a9-49cf-ac05-c765951ccfef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python /dssg/home/acct-clsyzs/clsyzs/C1/ProteinMPNN_multi_re/helper_scripts/assign_fixed_chains.py --input_path=./test.jsonl --output_path=./assigned_pdbs.jsonl --chain_list \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336ccafa-51be-4a01-83ed-820ac410421a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python /dssg/home/acct-clsyzs/clsyzs/C1/ProteinMPNN_multi_re/helper_scripts/make_fixed_positions_dict.py --input_path=./test.jsonl --output_path=./fixed_pdbs.jsonl --chain_list \"A B\" --position_list \"26 27 28 30 31 32 33 52 53 54 55 57 58 98 99 100 101 103 104, 31 32 33 34 50 51 54 55 56 57 58 92 93 94 95 96 97 98\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a74486-694c-4c6b-8217-405ff67361e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "pssm_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "omit_AA_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "bias_AA_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "tied_positions_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "bias by residue dictionary is not loaded, or not provided\n",
      "----------------------------------------\n",
      "discarded {'bad_chars': 0, 'too_long': 0, 'bad_seq_length': 0}\n",
      "2\n",
      "{'fold_bimagrumab_fab_actriia_20250119_model_0': {'A': [26, 27, 28, 30, 31, 32, 33, 52, 53, 54, 55, 57, 58, 98, 99, 100, 101, 103, 104], 'B': [31, 32, 33, 34, 50, 51, 54, 55, 56, 57, 58, 92, 93, 94, 95, 96, 97, 98], 'C': []}, 'fold_bimagrumab_fab_actriib_20250119_model_0': {'A': [26, 27, 28, 30, 31, 32, 33, 52, 53, 54, 55, 57, 58, 98, 99, 100, 101, 103, 104], 'B': [31, 32, 33, 34, 50, 51, 54, 55, 56, 57, 58, 92, 93, 94, 95, 96, 97, 98], 'C': []}}\n",
      "------------chain_order: ['A', 'B', 'C']\n",
      "------------chain_order: ['A', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "!python /dssg/home/acct-clsyzs/clsyzs/C1/ProteinMPNN_multi_re/protein_mpnn_data_p.py \\\n",
    "        --jsonl_path ./test.jsonl \\\n",
    "        --chain_id_jsonl ./assigned_pdbs.jsonl \\\n",
    "        --fixed_positions_jsonl ./fixed_pdbs.jsonl \\\n",
    "        --out_folder ./1/ \\\n",
    "        --num_seq_per_target 100 \\\n",
    "        --sampling_temp \"0.1\" \\\n",
    "        --seed 37 \\\n",
    "        --batch_size 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db641701-b6f1-41ef-a686-cdf36bb11b25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3494913/2003686275.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_pre = torch.load(\"data_pre.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data_pre = torch.load(\"data_pre.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cac8177-2857-4bf4-8cdf-636b3b5bcaf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = data_pre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ed2c0fd-ee40-4fc7-b504-832e185e3bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1301, 21])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_by_res_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90eee5c2-1a00-4cff-ad15-7bed9b6f8704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2491476/2103471514.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "    #checkpoint_path = \"/dssg/home/acct-clsyzs/clsyzs/C1/ProteinMPNN_multi/vanilla_model_weights/v_48_020.pt\"\n",
    "    checkpoint_path = \"abmpnn.pt\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    noise_level_print = checkpoint['noise_level']\n",
    "    model = ProteinMPNN(num_letters=21, node_features=128, edge_features=128, hidden_dim=128, num_encoder_layers=3, num_decoder_layers=3, augment_eps=0., k_neighbors=checkpoint['num_edges'])\n",
    "    model.to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a98c3aa9-6731-4ce7-8375-ea91ae9c1a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "randn = torch.randn(chain_M.shape).to(device)\n",
    "X = X.to(device)\n",
    "S = S.to(device)\n",
    "mask = mask.to(device)\n",
    "#lengths = lengths.to(device)\n",
    "chain_M = chain_M.to(device)\n",
    "chain_encoding_all = chain_encoding_all.to(device)\n",
    "#chain_list_list = chain_list_list.to(device)\n",
    "#visible_list_list = visible_list_list.to(device)\n",
    "#masked_list_list = masked_list_list.to(device)\n",
    "#masked_chain_length_list_list = masked_chain_length_list_list.to(device)\n",
    "chain_M_pos = chain_M_pos.to(device)\n",
    "omit_AA_mask = omit_AA_mask.to(device)\n",
    "residue_idx = residue_idx.to(device)\n",
    "dihedral_mask = dihedral_mask.to(device)\n",
    "#tied_pos_list_of_lists_list = tied_pos_list_of_lists_list.to(device)\n",
    "pssm_coef = pssm_coef.to(device)\n",
    "pssm_bias = pssm_bias.to(device)\n",
    "pssm_log_odds_all = pssm_log_odds_all.to(device)\n",
    "bias_by_res_all = bias_by_res_all.to(device)\n",
    "tied_beta = tied_beta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a925ec8-14a2-4a2a-bf45-c47eb42fa3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pssm_log_odds_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m omit_AAs_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([AA \u001b[38;5;129;01min\u001b[39;00m omit_AAs_list \u001b[38;5;28;01mfor\u001b[39;00m AA \u001b[38;5;129;01min\u001b[39;00m alphabet])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      5\u001b[0m bias_AAs_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(alphabet))\n\u001b[0;32m----> 6\u001b[0m pssm_log_odds_mask \u001b[38;5;241m=\u001b[39m (\u001b[43mpssm_log_odds_all\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pssm_log_odds_all' is not defined"
     ]
    }
   ],
   "source": [
    "    omit_AAs_list = \"X\"\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    alphabet_dict = dict(zip(alphabet, range(21)))   \n",
    "    omit_AAs_np = np.array([AA in omit_AAs_list for AA in alphabet]).astype(np.float32)\n",
    "    bias_AAs_np = np.zeros(len(alphabet))\n",
    "    pssm_log_odds_mask = (pssm_log_odds_all > 0.).float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d38419d-1b87-4eda-9b7a-1e78d9009af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_M_pos[0,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb6b11-e3bb-4b0b-8ded-4cbacd55a607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "lst_sequence = []\n",
    "T = int(chain_M[0].sum()) #设计序列长度\n",
    "total_design = 2000#设计数量\n",
    "for i in range(total_design):\n",
    "    if i %100 ==0:\n",
    "        print(i)\n",
    "    with torch.no_grad(): \n",
    "        randn = torch.randn(chain_M.shape).to(device)\n",
    "        S_pre = model.sample(X, randn, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=0.1, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=0., pssm_log_odds_flag=bool(0), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(0), bias_by_res=bias_by_res_all)\n",
    "        lst_sequence.append(S_pre[\"S\"][0,:T].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa75fe-8dc6-400d-8ffe-f5c87abf5376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def indices_to_chars(indices):\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "    return ''.join([alphabet[i] for i in indices])\n",
    "seq_design = []\n",
    "#outputs = torch.argmax(outputs,-1)\n",
    "for i in lst_sequence:\n",
    "    seq_design.append(indices_to_chars(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44776ef4-ea1f-4406-b47c-91095369d541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#检查设计\n",
    "for i in seq_design:\n",
    "    if \"X\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e137f54-ecb2-42cb-977e-09c82a566b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def split_chains(lst, sequence):\n",
    "    # 创建一个字典，key 是链的编号，value 是对应的链的序列\n",
    "    chains = defaultdict(str)\n",
    "    \n",
    "    # 遍历列表 lst 和字符串 sequence，根据 lst 的值将字符分配到对应链\n",
    "    for i, chain_id in enumerate(lst):\n",
    "        chains[chain_id] += sequence[i]\n",
    "    \n",
    "    return dict(chains)  # 返回字典，键是链的编号，值是对应链的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac047bf-2c4a-407b-a986-1b8b4eadd7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2914132-16d0-4ef9-a274-4cd230a41a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 示例用法\n",
    "lst = [int(i) for i in chain_encoding_all[0,:T]]\n",
    "\n",
    "lst_design_seq = []\n",
    "for i in seq_design:\n",
    "    sequence = i\n",
    "    result = split_chains(lst, sequence)\n",
    "    if len(result) == 1:\n",
    "        lst_design_seq.append(list(result.values())[0])\n",
    "    else:\n",
    "        lst_design_seq.append(\":\".join(list(result.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24477446-561e-4bb6-8213-4fcb71d935eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.value_counts(lst_design_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c391d3-3df7-4ba8-a39f-20adc343da1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(lst_design_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b1179-0f97-4d73-829e-2760a260dc7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#去除重复\n",
    "seq_design = pd.value_counts(lst_design_seq).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9d250-0230-4f9c-94eb-e33d6fd7c0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/dssg/home/acct-clsyzs/clsyzs/C1/gxy_119.fasta\", \"w\") as f:\n",
    "    for i, seq in enumerate(lst_design_seq):\n",
    "        f.write(f\"> {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(seq)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ba2cb-f89b-4800-8455-a6ac409bfc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3e8dd-e198-4222-a7ad-b645680499cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9607893-379f-45a5-b828-12c9b6841512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9390543f-47e6-4cfe-848f-bd42178a5d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c255d5-8b70-4e5e-a211-919e181d1462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se3Environment",
   "language": "python",
   "name": "se3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
